{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2282fd",
   "metadata": {},
   "source": [
    "## Langchain Demo: AI Agents on AWS Bedrock\n",
    "\n",
    "- thinking about an agent AI that integrates with the toolset\n",
    "- see the reference documentation here: https://python.langchain.com/docs/tutorials/agents/\n",
    "- in this case, using AWS Bedrock for DeepSeek model access using my named profile in `~/.aws/credentials` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd30d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your AWS credentials are configured\n",
    "import langchain\n",
    "from langchain.chat_models import init_chat_model\n",
    "import pandas as pd\n",
    "import getout_of_text_3 as got3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc07369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id='us.deepseek.r1-v1:0' # does not work with tools implementation\n",
    "#model_id='us.meta.llama4-maverick-17b-instruct-v1:0' # 1M\n",
    "#model_id='global.anthropic.claude-sonnet-4-5-20250929-v1:0' # 200K\n",
    "#max_tokens=64000 # for anthropic.claude-sonnet-4-5-20250929-v1:0\n",
    "#model_id='amazon.nova-pro-v1:0'\n",
    "#max_tokens=10000\n",
    "#model_id='us.amazon.nova-premier-v1:0'\n",
    "#max_tokens=32000\n",
    "model_id='openai.gpt-oss-120b-1:0' # 128,000\n",
    "max_tokens=128000 # for openai.gpt-oss-120b-1:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88d5ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(model_id, \n",
    "                        model_provider=\"bedrock_converse\",\n",
    "                        credentials_profile_name='atn-developer',\n",
    "                        max_tokens=max_tokens # maximum for bedrock converse\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2d95f8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Portugal is **Lisbon**.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi! What is the capital of Portugal?\"\n",
    "response = model.invoke([{\"role\": \"user\", \"content\": query}])\n",
    "response.text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f7429",
   "metadata": {},
   "source": [
    "## DIY SCOTUS Corpus Query Tool & Example\n",
    "\n",
    "- Using `got3` to filter on keywords in a DIY SCOTUS Corpus, via extracting text in PDFs downloaded from **Library of Congress** collection on the [United States Reports](https://www.loc.gov/collections/united-states-reports/).\n",
    "- Corpus is stored as a **dictionary** of **dataframes**, where each key is a **volume number** and each dataframe contains the cases in that volume.\n",
    "    - the `case_id` is a concatenated volume and page number (i.e. `329001` is volume 329, page 1). This is also a schema for saving PDF downloads locally. See details at [the U.S. Report page](https://www.supremecourt.gov/opinions/USReports.aspx).\n",
    "\n",
    "        ```json\n",
    "        {\"329\": {DataFrame}, \n",
    "        \"330\": {DataFrame}, ..., \n",
    "        \"570\": {DataFrame}\n",
    "        }\n",
    "        ```\n",
    "        ```text\n",
    "        case_id\ttext\n",
    "        0\t570729\tOCTOBER \\nTERM, 2012 \\n729 \\nSyllabus \\nSEKHAR...\n",
    "        1\t570338\t338 \\nOCTOBER \\nTERM, 2012 \\nSyllabus \\nUNIVER...\n",
    "        2\t570099\tOCTOBER \\nTERM, 2012 \\n99 \\nSyllabus \\nALLEYNE...\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "36df068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pdf scotus files\n",
    "df = pd.read_json(\"loc_gov.json\", lines=True)\n",
    "\n",
    "df['key'] = df['filename'].apply(lambda x: x.split('usrep')[1][:3])\n",
    "df['subkey'] = df['filename'].apply(lambda x: x.split('usrep')[1].split('.pdf')[0])\n",
    "\n",
    "# Create a dictionary to hold the DataFrame contents\n",
    "df_dict = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if row['key'] not in df_dict:\n",
    "        df_dict[row['key']] = {}\n",
    "    df_dict[row['key']][row['subkey']] = row['content']\n",
    "\n",
    "# format scotus data for getout_of_text_3, similar to COCA keyword results\n",
    "db_dict_formatted = {}\n",
    "for volume, cases in df_dict.items():\n",
    "    # Create a DataFrame for each volume with case text\n",
    "    case_data = []\n",
    "    for case_id, case_text in cases.items():\n",
    "        case_data.append({'case_id': case_id, 'text': case_text})\n",
    "    db_dict_formatted[volume] = pd.DataFrame(case_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bbae1f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570729</td>\n",
       "      <td>OCTOBER \\nTERM, 2012 \\n729 \\nSyllabus \\nSEKHAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570338</td>\n",
       "      <td>338 \\nOCTOBER \\nTERM, 2012 \\nSyllabus \\nUNIVER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570099</td>\n",
       "      <td>OCTOBER \\nTERM, 2012 \\n99 \\nSyllabus \\nALLEYNE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570529</td>\n",
       "      <td>OCTOBER \\nTERM, 2012 \\n529 \\nSyllabus \\nSHELBY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570048</td>\n",
       "      <td>48 \\nOCTOBER \\nTERM, 2012 \\nSyllabus \\nMARACIC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_id                                               text\n",
       "0  570729  OCTOBER \\nTERM, 2012 \\n729 \\nSyllabus \\nSEKHAR...\n",
       "1  570338  338 \\nOCTOBER \\nTERM, 2012 \\nSyllabus \\nUNIVER...\n",
       "2  570099  OCTOBER \\nTERM, 2012 \\n99 \\nSyllabus \\nALLEYNE...\n",
       "3  570529  OCTOBER \\nTERM, 2012 \\n529 \\nSyllabus \\nSHELBY...\n",
       "4  570048  48 \\nOCTOBER \\nTERM, 2012 \\nSyllabus \\nMARACIC..."
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_keys = sorted(db_dict_formatted.keys(), key=lambda x: int(x), reverse=False)\n",
    "print(sorted_keys)\n",
    "db_dict_formatted['570'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1087ed2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4a5e48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional, Type, Dict, Any, Union, List\n",
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# ORIGINAL live-search tool (now simplified for notebook-friendly execution)\n",
    "# ============================================================================\n",
    "class ScotusAnalysisInput(BaseModel):\n",
    "    \"\"\"Input for SCOTUS case analysis tool (performs a fresh keyword search).\"\"\"\n",
    "    keyword: str = Field(description=\"The keyword to search for and analyze in SCOTUS cases\")\n",
    "    analysis_focus: Optional[str] = Field(\n",
    "    default=\"general\", \n",
    "    description=\"Focus of analysis: 'general', 'evolution', 'judicial_philosophy', or 'custom'\"\n",
    "    )\n",
    "\n",
    "class ScotusAnalysisTool(BaseTool):\n",
    "    \"\"\"Tool that SEARCHES the SCOTUS corpus then analyzes.\n",
    "    NOTE: For pre-filtered JSON results, use ScotusFilteredAnalysisTool instead.\n",
    "\n",
    "    Implementation note (merged sync/async):\n",
    "    - We removed the prior _sync_run/_arun split + nest_asyncio hack.\n",
    "    - _run always works in notebooks (Jupyter event loop) using the sync model.invoke.\n",
    "    - A lightweight _arun delegator is kept for LangChain compatibility but just calls the sync core.\n",
    "    \"\"\"\n",
    "    name: str = \"scotus_analysis\"\n",
    "    description: str = (\n",
    "        \"Analyzes SCOTUS cases for a given keyword after performing an internal search. \"\n",
    "        \"Do NOT provide pre-filtered results to this tool.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = ScotusAnalysisInput\n",
    "    model: Any = Field(exclude=True)\n",
    "    db_dict_formatted: Any = Field(exclude=True)\n",
    "\n",
    "    def __init__(self, model, db_dict_formatted, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.db_dict_formatted = db_dict_formatted\n",
    "\n",
    "    # Public entry point (sync) ------------------------------------------------\n",
    "    def _run(self, keyword: str, analysis_focus: str = \"general\") -> str:  # noqa: D401\n",
    "        try:\n",
    "            return self._execute(keyword, analysis_focus)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error analyzing SCOTUS results: {e}\"\n",
    "            print(f\"❌ TOOL(search): {error_msg}\")\n",
    "            return error_msg\n",
    "\n",
    "    # Async compatibility (simply defers to sync to avoid loop issues in notebooks)\n",
    "    async def _arun(self, keyword: str, analysis_focus: str = \"general\") -> str:  # noqa: D401\n",
    "        return self._run(keyword, analysis_focus)\n",
    "\n",
    "    # Internal core (shared) --------------------------------------------------\n",
    "    def _execute(self, keyword: str, analysis_focus: str) -> str:\n",
    "        print(f\"🔍 TOOL(search): Searching SCOTUS database for keyword: '{keyword}'\")\n",
    "        import getout_of_text_3 as got3\n",
    "        # Dynamic context window that auto-shrinks if prompt would exceed max_tokens\n",
    "        base_context_words = 20\n",
    "        context_words = base_context_words\n",
    "        attempts = 0\n",
    "        final_prompt = None\n",
    "        results_dict = None\n",
    "        volumes = []\n",
    "        total_cases = 0\n",
    "        while True:\n",
    "            search_results = got3.search_keyword_corpus(\n",
    "                keyword=keyword,\n",
    "                db_dict=self.db_dict_formatted,\n",
    "                case_sensitive=False,\n",
    "                show_context=True,\n",
    "                context_words=context_words,\n",
    "                output=\"json\"\n",
    "            )\n",
    "            results_dict = {k: v for k, v in sorted(search_results.items(), key=lambda item: int(item[0])) if v}\n",
    "            if not results_dict:\n",
    "                return f\"No results found for keyword '{keyword}' in the SCOTUS database.\"\n",
    "            total_cases = sum(len(cases) for cases in results_dict.values())\n",
    "            volumes = list(results_dict.keys())\n",
    "            prompt = self._build_prompt(results_dict, keyword, analysis_focus, volumes, total_cases)\n",
    "            if len(prompt) <= max_tokens or context_words <= 5:\n",
    "                final_prompt = prompt\n",
    "                break\n",
    "            # Need to shrink\n",
    "            attempts += 1\n",
    "            ratio = max_tokens / len(prompt)\n",
    "            new_context_words = max(5, int(context_words * ratio * 0.9))  # 0.9 safety margin\n",
    "            if new_context_words >= context_words:\n",
    "                # No further shrink possible; accept risk of truncation\n",
    "                print(f\"⚠️ AUTO-SHRINK: Unable to further reduce context (floor reached or no progress). Using context_words={context_words}.\")\n",
    "                final_prompt = prompt\n",
    "                break\n",
    "            print(f\"🪄 AUTO-SHRINK: prompt {len(prompt)} > max_tokens {max_tokens}. context_words {context_words} -> {new_context_words} (ratio {ratio:.3f}). Retrying...\")\n",
    "            context_words = new_context_words\n",
    "        print(f\"📊 TOOL(search): Found {total_cases} cases across {len(volumes)} volumes | context_words_used={context_words} | shrink_attempts={attempts}\")\n",
    "        print(f\"🤖 TOOL(search): Sending {len(final_prompt)} characters to AI model for analysis\")\n",
    "        try:\n",
    "            if len(final_prompt) > max_tokens:\n",
    "                print(f\"⚠️ TOOL(search): Prompt length {len(final_prompt)} still > max_tokens {max_tokens}; model/provider may truncate.\")\n",
    "            response = self.model.invoke([{\"role\": \"user\", \"content\": final_prompt}])\n",
    "            content = getattr(response, 'content', str(response))\n",
    "            print(f\"✅ TOOL(search): Analysis complete, returning {len(content)} characters\")\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Model invocation failed: {e}\")\n",
    "\n",
    "    def _build_prompt(self, results_dict, keyword, analysis_focus, volumes, total_cases) -> str:\n",
    "        analysis_prompts = {\n",
    "            \"general\": f\"\"\"\n",
    "            Instructions:\n",
    "            You are an AI Agent inside the open-source forensic linguistic tool `getout_of_text_3`.\n",
    "            Analyze these SCOTUS case search results for the keyword \\\"{keyword}\\\" ONLY using the provided data.\n",
    "            Data summary:\n",
    "            - Volumes: {', '.join(sorted(volumes, key=int))}\n",
    "            - Total case occurrences: {total_cases}\n",
    "            Provide insights on:\n",
    "            1. Temporal evolution\n",
    "            2. Contextual variation\n",
    "            3. Notable intra-dataset patterns (do NOT import outside knowledge)\n",
    "            4. Interpretive themes relevant to ordinary meaning\n",
    "            Results (truncated JSON): {json.dumps(results_dict, indent=2)}...\n",
    "            \"\"\",\n",
    "            \"evolution\": f\"Focus on change over volumes for '{keyword}'.\\nData: {json.dumps(results_dict, indent=2)}...\",\n",
    "            \"judicial_philosophy\": f\"Assess patterns of usage that may hint at differing interpretive approaches for '{keyword}'. Use ONLY provided texts. Data: {json.dumps(results_dict, indent=2)}...\",\n",
    "            \"custom\": f\"Comprehensive analysis for '{keyword}'. Use ONLY provided dataset. Data: {json.dumps(results_dict, indent=2)}...\"\n",
    "        }\n",
    "        return analysis_prompts.get(analysis_focus, analysis_prompts[\"general\"]).strip()\n",
    "\n",
    "# ============================================================================\n",
    "# Pre-filtered JSON analysis tool (enhanced: extraction strategies + debug metrics + token preflight)\n",
    "# ============================================================================\n",
    "class ScotusFilteredAnalysisInput(BaseModel):\n",
    "    \"\"\"Input for analyzing an already-filtered SCOTUS keyword JSON result set.\"\"\"\n",
    "    keyword: str = Field(description=\"Keyword being analyzed (for labeling only, not for searching).\")\n",
    "    results_json: Union[str, Dict[str, Any]] = Field(\n",
    "        description=\"Pre-filtered JSON (or dict) output from got3.search_keyword_corpus AFTER user filtering.\"\n",
    "    )\n",
    "    analysis_focus: Optional[str] = Field(\n",
    "        default=\"general\", description=\"'general', 'evolution', 'judicial_philosophy', or 'custom'\"\n",
    "    )\n",
    "    max_contexts: Optional[int] = Field(\n",
    "        default=None, description=\"OPTIONAL cap on number of context snippets. If None/0 => include ALL.\"\n",
    "    )\n",
    "    return_json: bool = Field(\n",
    "        default=False, description=\"If True, attempt to return structured JSON with reasoning_content, summary, etc.\"\n",
    "    )\n",
    "    extraction_strategy: str = Field(\n",
    "        default=\"first\",\n",
    "        description=\"How to extract text from each occurrence: 'first' (first matching field), 'all' (all matching fields), 'raw_json' (embed entire JSON).\"\n",
    "    )\n",
    "    debug: bool = Field(\n",
    "        default=False, description=\"If True, prints and embeds debug metrics about extraction & token estimates.\"\n",
    "    )\n",
    "\n",
    "class ScotusFilteredAnalysisTool(BaseTool):\n",
    "    \"\"\"Analyze ONLY the supplied pre-filtered SCOTUS keyword result JSON.\n",
    "\n",
    "    Enhanced with:\n",
    "    - extraction_strategy (first|all|raw_json)\n",
    "    - debug metrics (raw vs extracted char counts, estimated tokens)\n",
    "    - token preflight rejection (early fail if would exceed model max_tokens)\n",
    "    Simplified for notebooks (single sync execution core). Async call just delegates.\n",
    "    \"\"\"\n",
    "    name: str = \"scotus_filtered_analysis\"\n",
    "    description: str = (\n",
    "        \"Analyzes pre-filtered SCOTUS keyword search JSON (from got3) without performing any new retrieval.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = ScotusFilteredAnalysisInput\n",
    "    model: Any = Field(exclude=True)\n",
    "\n",
    "    def __init__(self, model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "\n",
    "    # Normalization helper ----------------------------------------------------\n",
    "    def _normalize_model_content(self, raw: Any) -> str:\n",
    "        if isinstance(raw, str):\n",
    "            return raw\n",
    "        if isinstance(raw, list):\n",
    "            parts = []\n",
    "            for block in raw:\n",
    "                if isinstance(block, str):\n",
    "                    parts.append(block)\n",
    "                elif isinstance(block, dict):\n",
    "                    for key in (\"text\", \"content\", \"value\", \"message\"):\n",
    "                        val = block.get(key)\n",
    "                        if isinstance(val, str):\n",
    "                            parts.append(val)\n",
    "                            break\n",
    "                    else:\n",
    "                        parts.append(str(block))\n",
    "                else:\n",
    "                    parts.append(str(block))\n",
    "            return \"\\n\".join(parts)\n",
    "        if isinstance(raw, dict):\n",
    "            for key in (\"text\", \"content\", \"value\"):\n",
    "                if key in raw and isinstance(raw[key], str):\n",
    "                    return raw[key]\n",
    "            return json.dumps(raw)\n",
    "        return str(raw)\n",
    "\n",
    "    # Public sync entry -------------------------------------------------------\n",
    "    def _run(\n",
    "        self,\n",
    "        keyword: str,\n",
    "        results_json: Union[str, Dict[str, Any]],\n",
    "        analysis_focus: str = \"general\",\n",
    "        max_contexts: Optional[int] = None,\n",
    "        return_json: bool = False,\n",
    "        extraction_strategy: str = \"first\",\n",
    "        debug: bool = False,\n",
    "    ) -> Union[str, Dict[str, Any]]:\n",
    "        try:\n",
    "            return self._execute(keyword, results_json, analysis_focus, max_contexts, return_json, extraction_strategy, debug)\n",
    "        except Exception as e:\n",
    "            msg = f\"Error (filtered analysis): {e}\"\n",
    "            print(msg)\n",
    "            return {\"error\": msg} if return_json else msg\n",
    "\n",
    "    # Async delegator ---------------------------------------------------------\n",
    "    async def _arun(\n",
    "        self,\n",
    "        keyword: str,\n",
    "        results_json: Union[str, Dict[str, Any]],\n",
    "        analysis_focus: str = \"general\",\n",
    "        max_contexts: Optional[int] = None,\n",
    "        return_json: bool = False,\n",
    "        extraction_strategy: str = \"first\",\n",
    "        debug: bool = False,\n",
    "    ) -> Union[str, Dict[str, Any]]:\n",
    "        return self._run(keyword, results_json, analysis_focus, max_contexts, return_json, extraction_strategy, debug)\n",
    "\n",
    "    # Core logic --------------------------------------------------------------\n",
    "    def _execute(self, keyword, results_json, analysis_focus, max_contexts, return_json, extraction_strategy, debug):\n",
    "        if extraction_strategy not in {\"first\", \"all\", \"raw_json\"}:\n",
    "            raise ValueError(\"extraction_strategy must be one of: 'first','all','raw_json'\")\n",
    "        results_dict = self._coerce_results(results_json)\n",
    "        stats = self._compute_stats(results_dict, keyword, extraction_strategy)\n",
    "\n",
    "        # Debug & metrics ------------------------------------------------------\n",
    "        raw_json_str = json.dumps(results_dict, sort_keys=True)\n",
    "        raw_chars = len(raw_json_str)\n",
    "        # For metrics we still collect contexts unless raw_json mode\n",
    "        if extraction_strategy == 'raw_json':\n",
    "            extracted_contexts_for_metrics = self._sample_contexts(results_dict, max_contexts, 'all')\n",
    "        else:\n",
    "            extracted_contexts_for_metrics = self._sample_contexts(results_dict, max_contexts, extraction_strategy)\n",
    "        extracted_chars = sum(len(c) for c in extracted_contexts_for_metrics)\n",
    "        approx_tokens_raw = raw_chars / 4\n",
    "        approx_tokens_extracted = extracted_chars / 4\n",
    "        reduction_ratio = (extracted_chars / raw_chars) if raw_chars else 0\n",
    "        if debug:\n",
    "            print(f\"🧪 DEBUG(filtered): raw_chars={raw_chars} extracted_chars={extracted_chars} reduction_ratio={reduction_ratio:.3f} raw≈{approx_tokens_raw:.0f}tok extracted≈{approx_tokens_extracted:.0f}tok strategy={extraction_strategy} limit={max_contexts}\")\n",
    "\n",
    "        # Build prompt ----------------------------------------------------------\n",
    "        prompt = self._build_prompt(keyword, results_dict, stats, analysis_focus, max_contexts, return_json, extraction_strategy, debug, {\n",
    "            'raw_chars': raw_chars,\n",
    "            'extracted_chars': extracted_chars,\n",
    "            'approx_tokens_raw': approx_tokens_raw,\n",
    "            'approx_tokens_extracted': approx_tokens_extracted,\n",
    "            'reduction_ratio': reduction_ratio,\n",
    "            'extraction_strategy': extraction_strategy,\n",
    "        })\n",
    "        # Token preflight (char/4 heuristic) -----------------------------------\n",
    "        approx_prompt_tokens = len(prompt) / 4\n",
    "        if approx_prompt_tokens > max_tokens:\n",
    "            msg = (f\"Preflight rejection: prompt would exceed model max_tokens. approx_prompt_tokens={approx_prompt_tokens:.0f} > max_tokens={max_tokens}. \"\n",
    "                   f\"Strategy='{extraction_strategy}' raw_tokens≈{approx_tokens_raw:.0f} extracted_tokens≈{approx_tokens_extracted:.0f}. Consider: lower max_contexts, switch to 'first', or filter upstream.\")\n",
    "            print(f\"⚠️ {msg}\")\n",
    "            if return_json:\n",
    "                return {\n",
    "                    \"error\": \"prompt_too_large\",\n",
    "                    \"message\": msg,\n",
    "                    \"keyword\": keyword,\n",
    "                    \"total_contexts\": stats['total_contexts'],\n",
    "                    \"extraction_strategy\": extraction_strategy,\n",
    "                    \"raw_chars\": raw_chars,\n",
    "                    \"extracted_chars\": extracted_chars,\n",
    "                    \"approx_tokens_prompt\": approx_prompt_tokens,\n",
    "                }\n",
    "            return msg\n",
    "\n",
    "        print(f\"🤖 TOOL(filtered): Sending {len(prompt)} chars (≈{approx_prompt_tokens:.0f} tok) to model (contexts: {stats['total_contexts']}) | strategy={extraction_strategy} return_json={return_json}\")\n",
    "        response = self.model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "        raw = getattr(response, 'content', None)\n",
    "        if raw is None and hasattr(response, 'text'):\n",
    "            try:\n",
    "                raw = response.text()\n",
    "            except Exception:\n",
    "                raw = response.text\n",
    "        content = self._normalize_model_content(raw)\n",
    "        if return_json:\n",
    "            return self._postprocess_json(content, results_dict, stats)\n",
    "        return content\n",
    "\n",
    "    # ---------------- Internal helpers ----------------\n",
    "    def _coerce_results(self, results_json: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        if isinstance(results_json, str):\n",
    "            results_dict = json.loads(results_json)\n",
    "        else:\n",
    "            results_dict = results_json\n",
    "        if not isinstance(results_dict, dict) or not results_dict:\n",
    "            raise ValueError(\"results_json must be a non-empty dict or JSON string\")\n",
    "        return results_dict\n",
    "\n",
    "    def _extract_contexts_from_case(self, occs, extraction_strategy: str) -> List[str]:\n",
    "        contexts: List[str] = []\n",
    "        if isinstance(occs, str):\n",
    "            contexts.append(occs)\n",
    "        elif isinstance(occs, dict):\n",
    "            keys_to_check = (\"context\", \"text\", \"snippet\", \"kwic\", \"content\", \"full_text\", \"body\")\n",
    "            if extraction_strategy == 'first':\n",
    "                for k in keys_to_check:\n",
    "                    if k in occs and isinstance(occs[k], str):\n",
    "                        contexts.append(occs[k])\n",
    "                        break\n",
    "            else:  # 'all' or 'raw_json' (raw_json uses entire JSON elsewhere but for metrics we gather all)\n",
    "                for k in keys_to_check:\n",
    "                    v = occs.get(k)\n",
    "                    if isinstance(v, str):\n",
    "                        contexts.append(v)\n",
    "        elif isinstance(occs, list):\n",
    "            for o in occs:\n",
    "                contexts.extend(self._extract_contexts_from_case(o, extraction_strategy))\n",
    "        return contexts\n",
    "\n",
    "    def _compute_stats(self, results_dict: Dict[str, Any], keyword: str, extraction_strategy: str) -> Dict[str, Any]:\n",
    "        volumes = sorted(results_dict.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x))\n",
    "        case_counts = {}\n",
    "        total_contexts = 0\n",
    "        occurrences_per_case = []\n",
    "        for vol, cases in results_dict.items():\n",
    "            if not isinstance(cases, dict):\n",
    "                continue\n",
    "            case_counts[vol] = len(cases)\n",
    "            for case_id, occs in cases.items():\n",
    "                contexts = self._extract_contexts_from_case(occs, extraction_strategy)\n",
    "                occ_count = len(contexts)\n",
    "                total_contexts += occ_count\n",
    "                occurrences_per_case.append({\"volume\": vol, \"case_id\": case_id, \"occurrences\": occ_count})\n",
    "        return {\n",
    "            \"volumes\": volumes,\n",
    "            \"case_counts\": case_counts,\n",
    "            \"total_cases\": sum(case_counts.values()),\n",
    "            \"total_contexts\": total_contexts,\n",
    "            \"occurrences_per_case\": occurrences_per_case,\n",
    "            \"keyword\": keyword,\n",
    "        }\n",
    "\n",
    "    def _sample_contexts(self, results_dict: Dict[str, Any], max_contexts: Optional[int], extraction_strategy: str) -> List[str]:\n",
    "        # If max_contexts is None/0/negative: include ALL contexts (no truncation)\n",
    "        limit = max_contexts if isinstance(max_contexts, int) and max_contexts > 0 else None\n",
    "        samples: List[str] = []\n",
    "        if extraction_strategy == 'raw_json':\n",
    "            return samples  # handled separately (we embed full JSON)\n",
    "        for vol in sorted(results_dict.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x)):\n",
    "            cases = results_dict[vol]\n",
    "            if not isinstance(cases, dict):\n",
    "                continue\n",
    "            for case_id, occs in cases.items():\n",
    "                contexts = self._extract_contexts_from_case(occs, extraction_strategy)\n",
    "                for ctx in contexts:\n",
    "                    cleaned = ' '.join(ctx.split())  # no clipping\n",
    "                    samples.append(f'[{vol}:{case_id}] {cleaned}')\n",
    "                    if limit and len(samples) >= limit:\n",
    "                        return samples\n",
    "        return samples\n",
    "\n",
    "    def _build_prompt(\n",
    "        self,\n",
    "        keyword: str,\n",
    "        results_dict: Dict[str, Any],\n",
    "        stats: Dict[str, Any],\n",
    "        analysis_focus: str,\n",
    "        max_contexts: Optional[int],\n",
    "        return_json: bool,\n",
    "        extraction_strategy: str,\n",
    "        debug: bool,\n",
    "        metrics: Dict[str, Any],\n",
    "    ) -> str:\n",
    "        if extraction_strategy == 'raw_json':\n",
    "            raw_json_block = json.dumps(results_dict, indent=2)\n",
    "            # Provide a short note, then raw JSON. Model must rely ONLY on raw JSON.\n",
    "            contexts_section = (\n",
    "                f\"RAW_JSON_MODE: Entire filtered JSON provided below. Size chars={metrics['raw_chars']} approx_tokens={metrics['approx_tokens_raw']:.0f}.\\n\"\n",
    "                \"Do NOT hallucinate beyond this data.\\n---\\n\" + raw_json_block + \"\\n---\\n\"\n",
    "            )\n",
    "            sample_contexts = []\n",
    "        else:\n",
    "            sample_contexts = self._sample_contexts(results_dict, max_contexts, extraction_strategy)\n",
    "            if not sample_contexts:\n",
    "                sample_contexts = [\"(No context strings extracted — verify input JSON structure)\"]\n",
    "            contexts_section = (\n",
    "                f\"Sample Contexts ({len(sample_contexts)}) strategy={extraction_strategy} (max_contexts={max_contexts}):\\n---\\n\"\n",
    "                + \"\\n\".join(sample_contexts) + \"\\n---\\n\"\n",
    "            )\n",
    "\n",
    "        focus_instructions = {\n",
    "            \"general\": \"Provide an overview of usage patterns, semantic ranges, and any interpretive variability.\",\n",
    "            \"evolution\": \"Describe shifts across volumes (treat volume ordering as temporal proxy if applicable).\",\n",
    "            \"judicial_philosophy\": \"Identify internal patterns that might hint at differing interpretive strategies (ONLY within provided data).\",\n",
    "            \"custom\": \"Provide a comprehensive structured analysis (frequency, contextual clusters, potential senses).\",\n",
    "        }\n",
    "        occ_lines = sorted(\n",
    "            [f\"{o['volume']}:{o['case_id']}={o['occurrences']}\" for o in stats[\"occurrences_per_case\"]],\n",
    "            key=lambda x: x\n",
    "        )[:80]\n",
    "\n",
    "        debug_block = \"\"\n",
    "        if debug:\n",
    "            debug_block = (\n",
    "                \"DEBUG METRICS (for transparency, do NOT just repeat):\\n\"\n",
    "                f\"raw_chars={metrics['raw_chars']} extracted_chars={metrics['extracted_chars']} reduction_ratio={metrics['reduction_ratio']:.3f}\\n\"\n",
    "                f\"approx_tokens_raw={metrics['approx_tokens_raw']:.0f} approx_tokens_extracted={metrics['approx_tokens_extracted']:.0f} strategy={metrics['extraction_strategy']}\\n\"\n",
    "            )\n",
    "\n",
    "        base = f\"\"\"\n",
    "            You are an AI analysis component of `getout_of_text_3`.\n",
    "            STRICT RULE: Use ONLY the provided contexts / JSON. DO NOT introduce external cases, doctrines, or speculative references.\n",
    "            Keyword: \"{keyword}\"\n",
    "            Volumes: {', '.join(stats['volumes'])}\n",
    "            Total Cases: {stats['total_cases']} | Total Context Snippets (computed with strategy='{extraction_strategy}'): {stats['total_contexts']}\n",
    "            Occurrences Per Case (sample): {'; '.join(occ_lines)}\n",
    "            Analysis Focus: {analysis_focus} → {focus_instructions.get(analysis_focus, focus_instructions['general'])}\n",
    "            {debug_block}\n",
    "            {contexts_section}\n",
    "        \"\"\"\n",
    "        if return_json:\n",
    "            base += (\n",
    "                \"Return ONLY valid JSON with this exact top-level structure (no extra prose):\\n\"\n",
    "                \"{\\n\"\n",
    "                \"  \\\"keyword\\\": string,\\n\"\n",
    "                \"  \\\"total_contexts\\\": number,\\n\"\n",
    "                \"  \\\"occurrences_summary\\\": string,\\n\"\n",
    "                \"  \\\"reasoning_content\\\": [string, ...],\\n\"\n",
    "                \"  \\\"summary\\\": string,\\n\"\n",
    "                \"  \\\"limitations\\\": string\\n\"\n",
    "                \"}\\n\"\n",
    "                \"Populate reasoning_content with a short step-by-step (3-6 bullets).\\n\"\n",
    "                \"If only one occurrence, reasoning_content should note insufficient data for variation.\"\n",
    "            )\n",
    "        else:\n",
    "            base += (\n",
    "                \"Required Output Sections:\\n1. Usage Summary\\n2. Contextual Patterns / Proto-senses\\n3. Frequency & Distribution Observations\\n4. Interpretability Notes (ordinary meaning indicators)\\n5. Open Questions / Ambiguities\\nGround all claims ONLY in the contexts above.\"\n",
    "            )\n",
    "        if stats['total_contexts'] == 1:\n",
    "            base += \"\\nNOTE: Only one occurrence detected.\"\n",
    "        return base.strip()\n",
    "\n",
    "    def _postprocess_json(self, content: str, results_dict: Dict[str, Any], stats: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        parsed = None\n",
    "        try:\n",
    "            parsed = json.loads(content)\n",
    "        except Exception:\n",
    "            if isinstance(content, str):\n",
    "                match = re.search(r'{[\\s\\S]*}', content)\n",
    "                if match:\n",
    "                    try:\n",
    "                        parsed = json.loads(match.group(0))\n",
    "                    except Exception:\n",
    "                        parsed = None\n",
    "        if not isinstance(parsed, dict):\n",
    "            parsed = {\n",
    "                \"keyword\": stats['keyword'],\n",
    "                \"total_contexts\": stats['total_contexts'],\n",
    "                \"occurrences_summary\": f\"{stats['total_contexts']} context snippet(s) across {stats['total_cases']} case(s)\",\n",
    "                \"reasoning_content\": [\n",
    "                    \"Model did not return valid JSON; wrapped raw text.\",\n",
    "                    \"Single occurrence limits distributional inference.\" if stats['total_contexts']==1 else \"Multiple contexts allow limited comparative analysis.\"\n",
    "                ],\n",
    "                \"summary\": content[:4000] if isinstance(content, str) else str(content)[:4000],\n",
    "                \"limitations\": \"Auto-wrapped due to invalid JSON from model.\"\n",
    "            }\n",
    "        for k, default in [\n",
    "            (\"reasoning_content\", []),\n",
    "            (\"summary\", \"\"),\n",
    "            (\"occurrences_summary\", f\"{stats['total_contexts']} snippet(s) across {stats['total_cases']} case(s)\"),\n",
    "            (\"limitations\", \"\")\n",
    "        ]:\n",
    "            if k not in parsed:\n",
    "                parsed[k] = default\n",
    "        return parsed\n",
    "\n",
    "# Helper: format result so reasoning_content appears LAST under a heading\n",
    "def format_result_output(result):\n",
    "    \"\"\"Return a single string with main answer first and reasoning_content appended at end.\n",
    "    Format:\n",
    "    <answer text>\n",
    "\n",
    "    ## reasoning content\n",
    "    ```text\n",
    "    <reasoning>\n",
    "    ```\n",
    "    Handles several possible shapes returned by Bedrock / LangChain tool binding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(result, str):\n",
    "            return result\n",
    "        if isinstance(result, dict):\n",
    "            reasoning = []\n",
    "            rc = result.get(\"reasoning_content\")\n",
    "            if isinstance(rc, list):\n",
    "                reasoning.append(\"\\n\".join(str(x) for x in rc))\n",
    "            elif isinstance(rc, str):\n",
    "                reasoning.append(rc)\n",
    "            elif isinstance(rc, dict) and \"text\" in rc:\n",
    "                reasoning.append(rc[\"text\"])\n",
    "            main_parts = []\n",
    "            for key in (\"summary\", \"text\", \"content\"):\n",
    "                if key in result and isinstance(result[key], str):\n",
    "                    main_parts.append(result[key])\n",
    "            main_text = \"\\n\\n\".join(p for p in main_parts if p and p.strip())\n",
    "            reasoning_text = \"\\n\\n\".join(r for r in reasoning if r and r.strip())\n",
    "            if reasoning_text:\n",
    "                return f\"{main_text}\\n\\n## reasoning content\\n```text\\n{reasoning_text}\\n```\" if main_text else f\"## reasoning content\\n```text\\n{reasoning_text}\\n```\"\n",
    "            return main_text or str(result)\n",
    "        if isinstance(result, list):\n",
    "            reasoning_segments = []\n",
    "            answer_segments = []\n",
    "            for block in result:\n",
    "                if not isinstance(block, dict):\n",
    "                    continue\n",
    "                rc = block.get(\"reasoning_content\")\n",
    "                if isinstance(rc, dict) and \"text\" in rc and rc[\"text\"].strip():\n",
    "                    reasoning_segments.append(rc[\"text\"].strip())\n",
    "                elif isinstance(rc, str) and rc.strip():\n",
    "                    reasoning_segments.append(rc.strip())\n",
    "                if \"text\" in block and isinstance(block[\"text\"], str) and block[\"text\"].strip():\n",
    "                    answer_segments.append(block[\"text\"].strip())\n",
    "            main_answer = \"\\n\\n\".join(answer_segments) if answer_segments else \"\"\n",
    "            reasoning_text = \"\\n\\n\".join(reasoning_segments)\n",
    "            if reasoning_text:\n",
    "                return f\"{main_answer}\\n\\n## reasoning content\\n```text\\n{reasoning_text}\\n```\" if main_answer else f\"## reasoning content\\n```text\\n{reasoning_text}\\n```\"\n",
    "            return main_answer or str(result)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"(Formatting error: {e})\\n{result}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "40118887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def export_markdown(result_text, keyword):\n",
    "    \"\"\"\n",
    "    Export unified analysis (answer + reasoning content at end) to ONE markdown file.\n",
    "    \"\"\"\n",
    "    safe_keyword = \"\".join(c if c.isalnum() or c in (\"-\", \"_\") else \"_\" for c in keyword.strip()) or \"analysis_output\"\n",
    "    formatted_output = format_result_output(result_text)\n",
    "    outfile = f\"{safe_keyword}.md\"\n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(formatted_output)\n",
    "    print(f\"Wrote combined analysis + reasoning to {outfile} (length={len(formatted_output)} chars)\")\n",
    "    return outfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cfdfd",
   "metadata": {},
   "source": [
    "## Create the agent by binging the tools and the model together\n",
    "\n",
    "Updated: We now have two tools bound:\n",
    "1. `scotus_analysis` performs an internal search.\n",
    "2. `scotus_filtered_analysis` ONLY analyzes pre-filtered JSON you already produced (no new search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "22099eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined: ScotusAnalysisTool (searching) & ScotusFilteredAnalysisTool (pre-filtered with flexible parsing + optional JSON mode, normalized content)\n",
      " ✅ Tools bound to model: ['scotus_analysis', 'scotus_filtered_analysis']\n",
      " 👨‍⚖️ SCOTUS database has 242 volumes\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tools\n",
    "scotus_tool = ScotusAnalysisTool(model=model, db_dict_formatted=db_dict_formatted)\n",
    "filtered_scotus_tool = ScotusFilteredAnalysisTool(model=model)\n",
    "\n",
    "tools = [scotus_tool, filtered_scotus_tool]\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "print(\"Defined: ScotusAnalysisTool (searching) & ScotusFilteredAnalysisTool (pre-filtered with flexible parsing + optional JSON mode, normalized content)\")\n",
    "print(f\" ✅ Tools bound to model: {[tool.name for tool in tools]}\")\n",
    "print(f\" 👨‍⚖️ SCOTUS database has {len(db_dict_formatted)} volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1a0ae",
   "metadata": {},
   "source": [
    "### Compare extraction strategies (`first`, `all`, `raw_json`) with debug metrics\n",
    "The next cell will:\n",
    "1. Run a search for a moderately frequent keyword.\n",
    "2. Invoke `scotus_filtered_analysis` three times with different `extraction_strategy` values.\n",
    "3. Show debug metrics (raw vs extracted char counts, token estimates, reduction ratio).\n",
    "4. Demonstrate preflight rejection if the raw JSON would exceed token limit (you can artificially force this by choosing a very frequent term or increasing context window upstream).\n",
    "\n",
    "NOTE: `raw_json` embeds the entire JSON (can be huge) — use sparingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d12964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: extraction strategies\n",
    "import getout_of_text_3 as got3\n",
    "\n",
    "_demo_keyword = 'vehicle'  # adjust to test token scaling\n",
    "print(f\"[DEMO] Building pre-filtered results for '{_demo_keyword}' (context_words=30)\")\n",
    "_demo_results = got3.search_keyword_corpus(\n",
    "    keyword=_demo_keyword,\n",
    "    db_dict=db_dict_formatted,\n",
    "    case_sensitive=False,\n",
    "    show_context=True,\n",
    "    context_words=30,\n",
    "    output='json'\n",
    ")\n",
    "# prune empties & sort\n",
    "_demo_results = {k: v for k, v in sorted(_demo_results.items(), key=lambda item: int(item[0])) if v}\n",
    "_demo_json = json.dumps(_demo_results)\n",
    "print(f\"Volumes with hits: {list(_demo_results.keys())[:8]} ... total_vols={len(_demo_results)}\")\n",
    "\n",
    "for strategy in ['first','all','raw_json']:\n",
    "    print(\"\\n============================\")\n",
    "    print(f\"[DEMO] Strategy='{strategy}' (max_contexts=None, debug=True)\")\n",
    "    try:\n",
    "        out = filtered_scotus_tool._run(\n",
    "            keyword=_demo_keyword,\n",
    "            results_json=_demo_json,\n",
    "            analysis_focus='general',\n",
    "            max_contexts=None,  # unlimited\n",
    "            return_json=False,\n",
    "            extraction_strategy=strategy,\n",
    "            debug=True\n",
    "        )\n",
    "        # Only print a small slice to avoid flooding\n",
    "        print(out[:600] + ('...' if len(out)>600 else ''))\n",
    "    except Exception as e:\n",
    "        print(f\"(Error strategy={strategy}: {e})\")\n",
    "print(\"\\n[DEMO] Extraction strategy comparison complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc087c",
   "metadata": {},
   "source": [
    "## 1st tool: AI will search the corpus for you and analyze results\n",
    "\n",
    "- for terms in the corpora that are very frequent, you'll almost certainly hit the `max_token` restriction (it's also frustrating that AWS Bedrock has limitations on the actual max tokens you can invoke versus what they publish is the maximum). To solve this, some recommendations:\n",
    "    - batch the requests into multiple parts\n",
    "    - lower the KWIC context window so you are sending less tokens\n",
    "    - randomly drop some values\n",
    "    - filter out stop words to cut down on char count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ca2f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword=\"bovine\"\n",
    "#keyword=\"dictionary\"\n",
    "#keyword=\"etienne\"\n",
    "#keyword=\"ordinary meaning\"\n",
    "#keyword=\"bank\"\n",
    "keyword=\"vehicle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b99f71f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEMO] Running scotus_analysis (live search) for keyword='vehicle' focus='general'\n",
      "🔍 TOOL(search): Searching SCOTUS database for keyword: 'vehicle'\n",
      "🪄 AUTO-SHRINK: prompt 246322 > max_tokens 128000. context_words 20 -> 9 (ratio 0.520). Retrying...\n",
      "📊 TOOL(search): Found 903 cases across 231 volumes | context_words_used=9 | shrink_attempts=1\n",
      "🤖 TOOL(search): Sending 125359 characters to AI model for analysis\n",
      "✅ TOOL(search): Analysis complete, returning 2 characters\n",
      "\n",
      "[DEMO] Runtime: 37.27s | Raw result type: list\n",
      "Wrote combined analysis + reasoning to scotus_analysis_vehicle.md (length=13522 chars)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'scotus_analysis_vehicle.md'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "search_keyword = keyword  # reuse previous, or set explicitly like: search_keyword = \"dictionary\"\n",
    "analysis_focus = \"general\"  # options: general | evolution | judicial_philosophy | custom\n",
    "\n",
    "print(f\"[DEMO] Running scotus_analysis (live search) for keyword='{search_keyword}' focus='{analysis_focus}'\")\n",
    "start_time = time()\n",
    "result_text = scotus_tool._run(keyword=search_keyword, analysis_focus=analysis_focus)\n",
    "elapsed = time() - start_time\n",
    "\n",
    "print(f\"\\n[DEMO] Runtime: {elapsed:.2f}s | Raw result type: {type(result_text).__name__}\")\n",
    "export_markdown(result_text, f\"scotus_analysis_{search_keyword}\")\n",
    "# ============================================================================\n",
    "# Unified formatted output (reasoning last)\n",
    "#formatted = format_result_output(result_text)\n",
    "#print(\"\\n=== FORMATTED OUTPUT (reasoning at end) ===\\n\")\n",
    "#print(formatted[:120])  # safety slice to avoid flooding notebook; adjust if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7cc781",
   "metadata": {},
   "source": [
    "### Preview Markdown Report of AI summary\n",
    "\n",
    "saved as `{keyword}.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851cce1",
   "metadata": {},
   "source": [
    "## 2nd tool: Passing pre-filtered JSON results to the filtered analysis tool\n",
    "\n",
    "- for greater control on the keyword samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9b916",
   "metadata": {},
   "source": [
    "#### New parameters for `scotus_filtered_analysis`\n",
    "- `extraction_strategy`: `first` (default) chooses first matching field among context/text/snippet/kwic/content/full_text/body; `all` concatenates all; `raw_json` embeds entire JSON (largest).\n",
    "- `debug=True` adds transparency metrics (raw/extracted char counts, token estimates, reduction ratio) to stdout and prompt.\n",
    "- Early preflight rejection triggers if the prompt's char/4 heuristic exceeds `max_tokens`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6027102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 TOOL(filtered): Sending 303280 chars (≈75820 tok) to model (contexts: 1964) | strategy=first return_json=False\n"
     ]
    }
   ],
   "source": [
    "keyword='bank'\n",
    "loc_results = got3.search_keyword_corpus(\n",
    "    keyword=keyword,\n",
    "    db_dict=db_dict_formatted,\n",
    "    case_sensitive=False,\n",
    "    show_context=True,\n",
    "    context_words=12,\n",
    "    output=\"json\"\n",
    ")\n",
    "# Drop keys with empty dicts and sort by keys (as integers)\n",
    "filtered_sorted_results = {k: v for k, v in sorted(loc_results.items(), key=lambda item: int(item[0])) if v}\n",
    "filtered_json_str = json.dumps(filtered_sorted_results)\n",
    "# Plain text (narrative) mode\n",
    "filtered_result_text = filtered_scotus_tool._run(\n",
    "    keyword=keyword,\n",
    "    results_json=filtered_json_str,\n",
    "    analysis_focus='general',\n",
    "    #max_contexts=40,\n",
    "    return_json=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5fe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'reasoning_content', 'reasoning_content': {'text': 'We need to produce analysis based on provided contexts snippets. Summarize usage patterns, semantic ranges, interpretive variability for the keyword \"bank\". Use only given contexts, not external. Provide sections as requested.\\n\\nWe saw many contexts: \"bank\" appears in many legal contexts: National Bank, Federal Reserve Bank, bank of United States, bank robbery, bankruptcy (\"bank-ruptcy\" hyphenated), bank accounts, bank deposits, bank of river (geographical bank), bank as a verb (bank on, bank the river), bank as a noun for financial institution, bank as a verb meaning to tilt aircraft (\"bank\"), also \"bank of the river\", \"bank of the night\", \"bank of data\". Also \"bank\" appears in phrases like \"bank of the west\", \"bank of the bank\" etc. Many contexts involve Supreme Court cases, statutes, and legal commentary.\\n\\nWe should discuss patterns: common collocations: \"bank of the United States\", \"First National Bank\", \"Chase National Bank\", \"bankruptcy\", \"bank-ruptcy\" (hyphen), \"bank robbery\", \"bank of the river\", \"bank of the river\" (physical bank). Also \"bank\" used as part of compound nouns: \"bank of the river\", \"banker\\'s\", \"bank accounts\", \"bank deposits\", \"bank notes\", \"bank holdings\", \"bank merging\". Additionally \"bank\" as verb with \"to bank\", like \"bank to the left\", \"bank rolls\".\\n\\nFrequency: appears in almost every case, many citations. Distribution across volumes: broad across 329 to 570 showing wide temporal span.\\n\\nInterpretability notes: In most legal context, \"bank\" refers to financial institution, often specified as \"National Bank\" or a particular named bank. In bankruptcy context, \"bank-ruptcy\" hyphenated indicates legal process. In river context, it\\'s not financial. Also \"bank robbery\" refers to criminal offense. The use of \"bank\" in \"bank of the river\" is clearly physical. There\\'s also \"bank of a river\" used in geographical context. Airplane \"bank\" is used metaphorically.\\n\\nOpen questions: distinction between \"bank\" meaning riverbank vs financial institution in some contexts; cases with \"bank\" as part of phrase \"bank of the night\" ???; need to interpret hyphenated \"bank-ruptcy\" and \"bank-rupt\" usage.\\n\\nWill produce sections accordingly.', 'signature': ''}}\n",
      "**1. Usage Summary**  \n",
      "The term **bank** appears overwhelmingly in its **financial‑institution sense** – a corporation authorized to take deposits, make loans, issue notes, etc.  The majority of citations are to Supreme‑Court opinions, statutes, and secondary legal commentary that discuss:  \n",
      "\n",
      "* the jurisdiction of **national‑, state‑, or federal banks** (e.g., *First National Bank*, *Chase National Bank*, *Continental Bank*);  \n",
      "* **bank‑ruptcy** (the bankruptcy process for banks, often hyphenated as *bank‑ruptcy* or *bank‑rupt*);  \n",
      "* **bank robbery** and related criminal statutes (e.g., 18 U.S.C. § 2113);  \n",
      "* **bank accounts, deposits, notes, and securities** (e.g., “bank account,” “bank deposits,” “bank notes,” “bank loan”).  \n",
      "\n",
      "A secondary, but frequent, sense is the **geographical “bank”** – the land alongside a river or other body of water (e.g., “west bank of the Mississippi,” “south bank of the Ohio”).  \n",
      "\n",
      "A third, occasional use is the **verb “to bank”** meaning to tilt an aircraft or to rely on something (“bank on”).  This occurs in a very small number of snippets (e.g., “bank of the aircraft,” “bank on the public interest”) and is always distinguishable from the financial sense by surrounding context.\n",
      "\n",
      "---\n",
      "\n",
      "**2. Contextual Patterns / Proto‑senses**  \n",
      "\n",
      "| Proto‑sense | Typical collocates & constructions | Illustrative excerpts |\n",
      "|------------|-----------------------------------|----------------------|\n",
      "| **Financial institution (noun)** | *National Bank, First National Bank, State Bank, Federal Reserve Bank, deposit, loan, note, securities, charter, merger, holding company* | “*Continental Bank v. Chicago* …”; “*Chase National Bank* cannot now be heard to complain …”; “*Federal Home Loan Bank Commissioner* …” |\n",
      "| **Bank‑ruptcy (legal process)** | *bank‑ruptcy, bank‑rupt, Chapter X, Chapter XI, reorganization, trustee, debtor* | “*Bank‑rupt‑cy court should not adjudge the bankrupt in contempt*”; “*Bank‑rupt‑cy Act* …”; “*bank‑rupt*  ·  *bank‑ruptcy*” |\n",
      "| **Bank robbery (criminal offense)** | *robbery, armed, firearm, dangerous weapon, § 2113, conviction* | “*armed bank robbery*”; “*bank robbery trial*”; “*bank‑robbery counts*” |\n",
      "| **Geographical “bank” (noun)** | *river, west bank, east bank, low‑tide elevation, tributary* | “*west bank of the Mississippi*”; “*low‑tide elevation* … ‘a bank or rock exposed only at low tide’ ” |\n",
      "| **Verb “to bank” (tilt / rely)** | *bank on, bank the aircraft, bank the river* | “*bank on the public interest*”; “*the aircraft damaged the bank of the river*” |\n",
      "| **Metaphoric / Institutional “bank”** | *bank of data, data‑bank, “bank of the night”, “bank of the river” used figuratively* | “*data bank of the computer*”; “*the bank of the river was “bank‑full”*” |\n",
      "\n",
      "The **financial‑institution proto‑sense** splits further into sub‑domains:\n",
      "\n",
      "* **Litigation & jurisdiction** – “*national bank can sue in state court*,” “*bank of the United States v. Deveaux*,” etc.  \n",
      "* **Regulation & statutory scheme** – references to the *Banking Act, Bank Secrecy Act, National Bank Act*; often in the context of pre‑emption or constitutional analysis.  \n",
      "* **Corporate transactions** – mergers, acquisitions, holding‑company issues, tax treatment of bank shares.  \n",
      "\n",
      "The **bank‑ruptcy proto‑sense** is tightly linked to the **bank** noun but is marked by the hyphenated form (*bank‑ruptcy*), signalling the **bankruptcy of a banking entity** and the special procedural rules that apply.\n",
      "\n",
      "The **geographical proto‑sense** is isolated to contexts discussing river boundaries, water rights, or navigation (e.g., “*the bank of the river*” and “*low‑tide elevation*”).\n",
      "\n",
      "---\n",
      "\n",
      "**3. Frequency & Distribution Observations**  \n",
      "\n",
      "* *Breadth*: “bank” appears in **all 242** case‑snippets shown (volumes 329 – 570).  \n",
      "* *Temporal spread*: citations span from early 19th‑century decisions (e.g., *Bank of the United States v. Deveaux*, 1809) through contemporary Supreme Court opinions (e.g., *College Savings Bank v. Florida Prepaid* (1999) and *RadLAX Gateway Hotel v. Amalgamated Bank* (2012)).  \n",
      "* *Dominant categories*:  \n",
      "  * **Banking‑institution citations** – > 70 % of the snippets (e.g., *First National Bank* cases, *Chase National Bank* mentions).  \n",
      "  * **Bank‑ruptcy citations** – ~ 12 % (identified by the hyphenated form).  \n",
      "  * **Bank‑robbery citations** – ~ 8 % (criminal‐law contexts).  \n",
      "  * **Geographical “bank”** – < 5 % (primarily river‑boundary cases).  \n",
      "* *Jurisdictional focus*: Many entries involve **U.S. Supreme Court** decisions, indicating that “bank” is a frequent term in high‑level constitutional and statutory interpretation.  \n",
      "\n",
      "---\n",
      "\n",
      "**4. Interpretability Notes (ordinary‑meaning indicators)**  \n",
      "\n",
      "| Indicator | Interpretation | Example |\n",
      "|----------|----------------|---------|\n",
      "| **Capitalized proper name + “Bank”** | Refers to a specific financial institution (corporate entity) | “*First National Bank of Boston*” |\n",
      "| **Modifiers “national”, “state”, “federal reserve”** | Institutional sense, often in jurisdictional analysis | “*National Bank Act*”, “*Federal Reserve Bank*” |\n",
      "| **Hyphenated “bank‑ruptcy” / “bank‑rupt”** | Legal process concerning a bank’s insolvency | “*bank‑ruptcy court*” |\n",
      "| **Criminal‑law language (“armed”, “firearm”, “§ 2113”)** | Criminal “bank robbery” sense | “*armed bank robbery*” |\n",
      "| **Geographical nouns (“river”, “tributary”, “low‑tide”)** | Physical landform sense | “*west bank of the Mississippi*” |\n",
      "| **Verbal constructions (“bank on”, “bank the aircraft”)** | Verb sense (rely, tilt) – rare, clearly signaled by surrounding verb phrase | “*bank on the public interest*” |\n",
      "| **“data bank”, “bank of the night”** | Metaphoric extension, still derives from the storage‑or‑containing notion of a bank | “*data bank of the computer*” |\n",
      "\n",
      "When the surrounding text mentions **deposits, loans, securities, charter, merger**, the interpretation as a **financial institution** is virtually certain.  When the context involves **river boundaries, navigation, low‑tide elevations**, the **geographical sense** is the only plausible reading.  The **criminal‑law context** is signaled by the presence of **“robbery”, “weapon”, “convicted”** etc.  The **bank‑ruptcy** sense is uniquely identified by the hyphen and by references to **Chapter X, Chapter XI, trustee, debtor**.\n",
      "\n",
      "---\n",
      "\n",
      "**5. Open Questions / Ambiguities**  \n",
      "\n",
      "1. **“Bank” as a verb vs. noun in mixed constructions** – a few snippets (e.g., “*bank on the public interest*”) could be read as either a figurative noun (“bank” meaning “institution”) or a verb (“to bank”).  The surrounding clause usually clarifies, but isolated excerpts might be ambiguous.  \n",
      "\n",
      "2. **Hyphenated forms** – the corpus contains both “bank‑ruptcy” and “bank‑rupt” (noun) and occasionally “bank‑rupt” used adjectivally (e.g., “*bank‑rupt estate*”).  Whether “bank‑rupt” in a given sentence functions as a noun or an adjective can depend on syntax that is not fully visible in the snippet.  \n",
      "\n",
      "3. **“Bank” in titles of non‑financial entities** – a few cases (e.g., “*Bank of Nova Scotia*” which is a financial institution, but “*Bank of the United States*” which historically functioned as a government‑owned entity) raise the question of whether the term always retains the *institutional* sense or can shift toward a *sovereign* sense.  The distinction is subtle but can affect constitutional analysis (e.g., sovereign immunity).  \n",
      "\n",
      "4. **“Bank of the river” vs. “Bank of the night”** – the phrase “*bank of the night*” appears once, with unclear meaning; it may be a metaphorical usage or a transcription error.  Without broader context, its interpretation remains open.  \n",
      "\n",
      "5. **Overlap between “bank” (financial) and “bank” (geographical) in the same opinion** – some opinions discuss both a bank’s liability for a “river‑bank” (e.g., water‑rights cases) and banking regulations, which could lead to momentary syntactic ambiguity if pronouns refer back to “bank”.  Readers must rely on the immediate lexical cues to disambiguate.  \n",
      "\n",
      "---  \n",
      "\n",
      "**Overall**, the corpus shows **bank** overwhelmingly operating as a **legal‑entity term** (financial institution), with well‑defined sub‑senses (bank‑ruptcy, bank robbery, geographical bank) that are signaled by characteristic collocations and syntactic markers.  The contexts provide ample internal evidence to resolve most ambiguities, leaving only a handful of isolated, metaphorical, or syntactically truncated instances as truly uncertain.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_result_text)\n",
    "export_markdown(filtered_result_text, f\"scotus_filtered_analysis_{keyword}_text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc19b4e",
   "metadata": {},
   "source": [
    "______________________\n",
    "\n",
    "## Conclusion 🐄 🐍 🧑‍⚖️\n",
    "\n",
    "### AWS Bedrock + LangChain + `got3` + SCOTUS Corpus Analysis\n",
    "\n",
    "This demo illustrated how to build custom AI‑powered analysis tools over a specialized legal text corpus using LangChain and AWS Bedrock. By combining dynamic keyword search with focused AI synthesis, we can rapidly explore judicial reasoning patterns in Supreme Court opinions.\n",
    "\n",
    "### AWS Cost Explorer -- Monitoring Bedrock Costs\n",
    "\n",
    "Useful `jq` filters for your AWS Cost Explorer output:\n",
    "\n",
    "> notably costs won't show up immediately, so you may need to wait a day or two after incurring costs to see them reflected in Cost Explorer.\n",
    "\n",
    "```bash\n",
    "# Filter for Bedrock services specifically from all services with costs\n",
    "named_profile=atn-developer\n",
    "region=us-east-1\n",
    "start_date=2025-10-01\n",
    "end_date=2025-10-04\n",
    "\n",
    "aws ce get-cost-and-usage --time-period Start=$start_date,End=$end_date --granularity DAILY --metrics \"BlendedCost\" --group-by Type=DIMENSION,Key=SERVICE --region $region --profile=$named_profile | jq '.ResultsByTime[].Groups[] | select(.Keys[0] | test(\"Bedrock\"; \"i\")) | {service: .Keys[0], cost: .Metrics.BlendedCost.Amount}'\n",
    "```\n",
    "\n",
    "#### Cost of AWS Bedrock\n",
    "\n",
    "- on 10/01/2025 this was: `\"cost\": \"0.03760425\"` -- check tomorrow to see the costs from my testing runs today.\n",
    "- okay various runs cost just about $0.10 `  \"cost\": \"0.10061025\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7d6c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"service\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Amazon Bedrock\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"cost\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0.10061025\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"service\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Amazon Bedrock\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"cost\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0.04100085\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"service\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Amazon Bedrock\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"cost\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0.03763545\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Filter for Bedrock services specifically from all services with costs\n",
    "!aws ce get-cost-and-usage --time-period Start=2025-09-30,End=2025-10-05 --granularity DAILY --metrics \"BlendedCost\" --group-by Type=DIMENSION,Key=SERVICE --region us-east-1 --profile=atn-developer | jq '.ResultsByTime[].Groups[] | select(.Keys[0] | test(\"Bedrock\"; \"i\")) | {service: .Keys[0], cost: .Metrics.BlendedCost.Amount}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bcde40",
   "metadata": {},
   "source": [
    "## SCOTUS Analysis Tools (LangChain + `getout_of_text_3`)\n",
    "\n",
    "This section documents two complementary LangChain Tool implementations for exploratory forensic / statutory‐interpretation analysis over a locally prepared SCOTUS corpus.\n",
    "\n",
    "### 1. Purpose\n",
    "You can (a) execute an on‑the‑fly keyword search + AI analysis, or (b) feed in *already filtered* JSON results for reproducible, cost‑controlled, deterministic re‑analysis with AI tools. This is for exploratory and demonstrative purposes only; do **NOT** rely on these tools for authoritative legal research or advice.\n",
    "\n",
    "### 2. Tool Inventory\n",
    "| Tool Name | Class | Performs Corpus Search? | Input Content Source | Best For | Cost / Token Control | Notes |\n",
    "|-----------|-------|-------------------------|----------------------|----------|----------------------|-------|\n",
    "| `scotus_analysis` | `ScotusAnalysisTool` | YES (internal `got3.search_keyword_corpus`) | Raw SCOTUS corpus dict (`db_dict_formatted`) | Quick ad‑hoc exploration, first look | Lower control (dynamic result size) | Returns free‑form model text. Not suited for strict reproducibility. |\n",
    "| `scotus_filtered_analysis` | `ScotusFilteredAnalysisTool` | NO (analysis only) | Pre‑filtered JSON (already produced elsewhere) | Stable reports, batching, auditing, caching | High control (you decide slice + cap contexts) | Optional structured JSON output (`return_json=True`). |\n",
    "\n",
    "### 3. Input Schemas (Pydantic)\n",
    "1. `ScotusAnalysisInput`:\n",
    "   - `keyword: str` – term/phrase to search.\n",
    "   - `analysis_focus: str = 'general'` – one of: `general`, `evolution`, `judicial_philosophy`, `custom`.\n",
    "2. `ScotusFilteredAnalysisInput`:\n",
    "   - `keyword: str` – label only (no searching performed).\n",
    "   - `results_json: str | dict` – JSON/dict from a *previous* `got3.search_keyword_corpus` call (after any manual filtering).\n",
    "   - `analysis_focus: str = 'general'` – same options as above.\n",
    "   - `max_contexts: int | None = None` – OPTIONAL cap; if None, 0, or <1 then ALL contexts are included (assumes you pre-trimmed).\n",
    "   - `return_json: bool = False` – if `True` the prompt instructs the model to emit **strict JSON** (post‑validated / auto‑repaired if malformed).\n",
    "\n",
    "### 4. Typical Workflow Patterns\n",
    "| Scenario | Recommended Flow |\n",
    "|----------|------------------|\n",
    "| Rapid hypothesis check | Use `scotus_analysis` with a single keyword, review summary. |\n",
    "| Iterative refinement / human curation | Run raw search once, manually prune / cluster results, then pass curated JSON to `scotus_filtered_analysis`. |\n",
    "| Batch reporting (multiple keywords) | Precompute & persist each keyword’s JSON → loop over `scotus_filtered_analysis` with `return_json=True`. |\n",
    "| Cost‑sensitive environment | Always pre‑filter & throttle with `max_contexts` (e.g. 40–80). |\n",
    "| Need structured downstream ingestion | Use `return_json=True` and parse validated keys (`reasoning_content`, `summary`, etc.). |\n",
    "\n",
    "### 5. Prompt Design Overview\n",
    "Both tools:\n",
    "- Enforce an “ONLY use supplied contexts” constraint (mitigates hallucination beyond current slice).\n",
    "- Provide distribution metadata (volumes, counts, occurrence sample) to steer higher‑level synthesis.\n",
    "- Offer `analysis_focus` to narrow stylistic / topical emphasis.\n",
    "\n",
    "`scotus_analysis` additionally:\n",
    "- Embeds a truncated JSON of search hits (first N characters) directly after retrieval.\n",
    "\n",
    "`scotus_filtered_analysis` adds:\n",
    "- Multi‑shape normalization for result structures: strings, lists, objects with `context`/`text` fields.\n",
    "- Optional context sampling cap (set `max_contexts` > 0). If no cap is provided ALL contexts (full length) are included — no 240‑char clipping.\n",
    "- Optional strict JSON response contract.\n",
    "\n",
    "### 6. Output Shapes\n",
    "| Mode | Example (abridged) |\n",
    "|------|--------------------|\n",
    "| `scotus_analysis` (text) | \"Usage Summary... Contextual Patterns...\" |\n",
    "| `scotus_filtered_analysis` (`return_json=False`) | Same narrative section headings as above. |\n",
    "| `scotus_filtered_analysis` (`return_json=True`) | `{ \"keyword\": \"ordinary meaning\", \"total_contexts\": 217, \"occurrences_summary\": \"217 snippet(s)...\", \"reasoning_content\": [\"...\"], \"summary\": \"...\", \"limitations\": \"...\" }` |\n",
    "\n",
    "### 7. Reliability & Error Handling\n",
    "- Async safety: attempts event‑loop reuse; falls back to sync if needed.\n",
    "- JSON robustness: if model returns malformed JSON, a salvage regex pass wraps content into a valid fallback structure.\n",
    "- Single‑occurrence safeguard: explicitly flags low‑evidence situations (e.g., only 1 context) to prevent over‑extrapolation.\n",
    "\n",
    "### 8. Performance & Cost Notes\n",
    "| Driver | Effect | Mitigation |\n",
    "|--------|--------|------------|\n",
    "| Large keyword result sets | Longer prompt → higher tokens | Pre‑filter & lower `max_contexts` |\n",
    "| Very common terms (e.g., \"the\", functional words) | Noise / inflated contexts | Encourage user to refine / phrase search |\n",
    "| High `max_contexts` + JSON mode | Larger instructions + response | Tune to 30–80; rarely need >100 contexts |\n",
    "| Duplicate or near‑duplicate contexts | Redundant token usage | Consider de‑dupe preprocessing before passing JSON |\n",
    "\n",
    "### 9. When NOT to Use These Tools\n",
    "- You need full‑text semantic retrieval (vector / embedding) across the corpus → integrate an embedding + retriever pipeline instead.\n",
    "- You require authoritative legal interpretation beyond provided snippets → domain attorney review required.\n",
    "- You want cross‑corpus comparative linguistics (e.g., COCA vs SCOTUS) in one call → design a composite prompt / multi‑tool pipeline.\n",
    "\n",
    "### 10. Minimal Usage Examples\n",
    "Live search (exploratory):\n",
    "```python\n",
    "result_text = scotus_tool._run(keyword=\"textualism\", analysis_focus=\"judicial_philosophy\")\n",
    "print(result_text[:800])\n",
    "```\n",
    "Pre‑filtered (deterministic):\n",
    "```python\n",
    "raw_results = got3.search_keyword_corpus(\n",
    "    keyword=\"ordinary meaning\",\n",
    "    db_dict=db_dict_formatted,\n",
    "    case_sensitive=False,\n",
    "    show_context=True,\n",
    "    context_words=30,\n",
    "    output=\"json\"\n",
    ")\n",
    "filtered = {k: v for k, v in raw_results.items() if v}  # prune empties\n",
    "json_str = json.dumps(filtered)\n",
    "analysis = filtered_scotus_tool._run(\n",
    "    keyword=\"ordinary meaning\",\n",
    "    results_json=json_str,\n",
    "    analysis_focus=\"general\",\n",
    "    max_contexts=50,\n",
    "    return_json=True\n",
    ")\n",
    "analysis[\"summary\"][:500]\n",
    "```\n",
    "\n",
    "### 11. Interpreting Structured JSON (Key Semantics)\n",
    "| Key | Meaning | Typical Consumer Action |\n",
    "|-----|---------|-------------------------|\n",
    "| `keyword` | Echo label for downstream grouping | Index in dataframe / vector store |\n",
    "| `total_contexts` | Sampled context count (post‑cap) | Assess evidence density |\n",
    "| `occurrences_summary` | Human‑readable distribution summary | Display directly in UI |\n",
    "| `reasoning_content` | Stepwise internal analysis (chain‑of‑thought lite) | Optional trust / audit pane |\n",
    "| `summary` | Main synthesized narrative | Persist / compare across keywords |\n",
    "| `limitations` | Self‑reported caveats (and JSON salvage note if any) | Flag for review / quality scoring |\n",
    "\n",
    "### 12. Extension Ideas (Future Work)\n",
    "- Add semantic clustering (MiniLM embeddings) before sampling to maximize diversity.\n",
    "- Integrate rate limiting & token accounting dashboards.\n",
    "- Provide a deterministic hash of the input JSON slice for reproducibility tracking.\n",
    "- Optional KWIC alignment / colorized keyword highlighting inside samples.\n",
    "\n",
    "### 13. Quick Decision Guide\n",
    "> If you can already see and trust the filtered JSON you want analyzed, **use `scotus_filtered_analysis`**. Otherwise, start with `scotus_analysis` to discover whether the keyword is even worth a curated run.\n",
    "\n",
    "---\n",
    "**Reminder:** Both tools operate strictly over *your supplied corpus slice*. They intentionally do **NOT** fetch external case law or enrich with outside doctrinal knowledge. This keeps analyses auditable and grounded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac1d47",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
