{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4e6c1bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: getout_of_text_3 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from getout_of_text_3) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from getout_of_text_3) (2.3.2)\n",
      "Requirement already satisfied: nltk>=3.8 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from getout_of_text_3) (3.9.1)\n",
      "Requirement already satisfied: click in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from nltk>=3.8->getout_of_text_3) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from nltk>=3.8->getout_of_text_3) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from nltk>=3.8->getout_of_text_3) (2025.8.29)\n",
      "Requirement already satisfied: tqdm in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from nltk>=3.8->getout_of_text_3) (4.67.1)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from getout_of_text_3) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from getout_of_text_3) (2.3.2)\n",
      "Requirement already satisfied: nltk>=3.8 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from getout_of_text_3) (3.9.1)\n",
      "Requirement already satisfied: click in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from nltk>=3.8->getout_of_text_3) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from nltk>=3.8->getout_of_text_3) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from nltk>=3.8->getout_of_text_3) (2025.8.29)\n",
      "Requirement already satisfied: tqdm in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from nltk>=3.8->getout_of_text_3) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0->getout_of_text_3) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0->getout_of_text_3) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install getout_of_text_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2125a2c",
   "metadata": {},
   "source": [
    "### Demonstration notebook for functionalities to include on GOT3 tool\n",
    "\n",
    "- working through some steps with `pandas` and`nltk`, to later roll into the toolset `getout_of_text_3` to streamline COCA Corpora searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa657b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getout_of_text_3 as got3\n",
    "got3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b2d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import necessary libraries for search functionality\n",
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e447c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NLTK libraries ready!\n",
      "âœ… Tokenization working: ['This', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"âœ… NLTK libraries ready!\")\n",
    "except:\n",
    "    print(\"âš ï¸ NLTK download may have failed, but will continue\")\n",
    "\n",
    "# Test tokenization\n",
    "try:\n",
    "    test_words = nltk.word_tokenize(\"This is a test.\")\n",
    "    print(f\"âœ… Tokenization working: {test_words}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Tokenization issue: {e}\")\n",
    "    print(\"Will use simple split() method as fallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418d6a8",
   "metadata": {},
   "source": [
    "## reading coca db and txt\n",
    "- dictionaries with genre as the key and dfs as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c516bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = ['acad', 'blog', 'fic', \n",
    "              'mag', 'news', 'spok',\n",
    "              'tvm', 'web']\n",
    "db_df = {}\n",
    "db_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfe00fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files for... acad\n",
      "  db_acad.txt: (1419500, 1)\n",
      "  text_acad.txt: (265, 1)\n",
      "Reading files for... blog\n",
      "  db_acad.txt: (1419500, 1)\n",
      "  text_acad.txt: (265, 1)\n",
      "Reading files for... blog\n",
      "  db_blog.txt: (1586094, 1)\n",
      "  text_blog.txt: (991, 1)\n",
      "Reading files for... fic\n",
      "  db_blog.txt: (1586094, 1)\n",
      "  text_blog.txt: (991, 1)\n",
      "Reading files for... fic\n",
      "  db_fic.txt: (1405902, 1)\n",
      "  text_fic.txt: (273, 1)\n",
      "Reading files for... mag\n",
      "  db_fic.txt: (1405902, 1)\n",
      "  text_fic.txt: (273, 1)\n",
      "Reading files for... mag\n",
      "  db_mag.txt: (1567102, 1)\n",
      "  text_mag.txt: (948, 1)\n",
      "Reading files for... news\n",
      "  db_mag.txt: (1567102, 1)\n",
      "  text_mag.txt: (948, 1)\n",
      "Reading files for... news\n",
      "  db_news.txt: (1389753, 1)\n",
      "  text_news.txt: (871, 1)\n",
      "Reading files for... spok\n",
      "  db_news.txt: (1389753, 1)\n",
      "  text_news.txt: (871, 1)\n",
      "Reading files for... spok\n",
      "  db_spok.txt: (1160506, 1)\n",
      "  text_spok.txt: (263, 1)\n",
      "Reading files for... tvm\n",
      "  db_spok.txt: (1160506, 1)\n",
      "  text_spok.txt: (263, 1)\n",
      "Reading files for... tvm\n",
      "  db_tvm.txt: (1567561, 1)\n",
      "  text_tvm.txt: (233, 1)\n",
      "Reading files for... web\n",
      "  db_tvm.txt: (1567561, 1)\n",
      "  text_tvm.txt: (233, 1)\n",
      "Reading files for... web\n",
      "  db_web.txt: (1423557, 1)\n",
      "  text_web.txt: (892, 1)\n",
      "\n",
      "âœ… Successfully created two dictionaries of DataFrames from COCA sample files\n",
      "   - db_df: 8 genres loaded\n",
      "   - db_text: 8 genres loaded\n",
      "  db_web.txt: (1423557, 1)\n",
      "  text_web.txt: (892, 1)\n",
      "\n",
      "âœ… Successfully created two dictionaries of DataFrames from COCA sample files\n",
      "   - db_df: 8 genres loaded\n",
      "   - db_text: 8 genres loaded\n"
     ]
    }
   ],
   "source": [
    "for genre in genre_dict:\n",
    "    print(f\"ðŸ“‚ Processing {genre}...\")\n",
    "    \n",
    "    # Load db file\n",
    "    try:\n",
    "        db_df[genre] = pd.read_csv(\"../coca-samples-db/db_{}.txt\".format(genre), \n",
    "                                   sep=\"\\t\", \n",
    "                                   header=None, \n",
    "                                   names=[\"text\"],\n",
    "                                   on_bad_lines='skip',\n",
    "                                   quoting=3)\n",
    "        print(f\"  âœ… db_{genre}.txt: {db_df[genre].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error reading db_{genre}: {e}\")\n",
    "    \n",
    "    # Load text file\n",
    "    try:\n",
    "        db_text[genre] = pd.read_csv(\"../coca-samples-text/text_{}.txt\".format(genre), \n",
    "                                     sep=\"\\t\", \n",
    "                                     header=None, \n",
    "                                     names=[\"text\"],\n",
    "                                     on_bad_lines='skip',\n",
    "                                     quoting=3)\n",
    "        print(f\"  âœ… text_{genre}.txt: {db_text[genre].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error reading text_{genre}: {e}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ SUMMARY:\")\n",
    "print(f\"   - db_df: {len(db_df)} genres loaded\") \n",
    "print(f\"   - db_text: {len(db_text)} genres loaded\")\n",
    "print(f\"   - Processed each genre exactly once âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5f585",
   "metadata": {},
   "source": [
    "## For collocate or keyword searches, we can use the following approach:\n",
    "1. loop through each genre\n",
    "2. do string filter hits for each instance of a string match in the dictionary key dataframe text column\n",
    "3. print out in an elegant manner\n",
    "\n",
    "use NLTK for this if that makes things easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3da5cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Search functions created successfully!\n"
     ]
    }
   ],
   "source": [
    "def search_keyword_corpus(keyword, db_dict, case_sensitive=False, show_context=True, context_words=5):\n",
    "    \"\"\"\n",
    "    Search for a keyword across all COCA genres and display results elegantly.\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: The word/phrase to search for\n",
    "    - db_dict: Dictionary of DataFrames (either db_df or db_text)\n",
    "    - case_sensitive: Whether to perform case-sensitive search\n",
    "    - show_context: Whether to show surrounding context\n",
    "    - context_words: Number of words to show on each side for context\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with search results by genre\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ” COCA Corpus Search: '{keyword}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    total_hits = 0\n",
    "    \n",
    "    # Prepare search pattern\n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    # Search through each genre\n",
    "    for genre, df in db_dict.items():\n",
    "        genre_hits = 0\n",
    "        print(f\"\\nðŸ“š {genre.upper()} Genre:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for idx, text in df['text'].items():\n",
    "            text_str = str(text)\n",
    "            matches = pattern.findall(text_str)\n",
    "            \n",
    "            if matches:\n",
    "                genre_hits += len(matches)\n",
    "                \n",
    "                if show_context:\n",
    "                    # Find all match positions and show context\n",
    "                    for match in pattern.finditer(text_str):\n",
    "                        start, end = match.span()\n",
    "                        \n",
    "                        # Get context words\n",
    "                        words = text_str.split()\n",
    "                        text_words = ' '.join(words)\n",
    "                        \n",
    "                        # Find word boundaries for context\n",
    "                        words_before_match = text_str[:start].split()\n",
    "                        words_after_match = text_str[end:].split()\n",
    "                        \n",
    "                        # Build context\n",
    "                        context_before = ' '.join(words_before_match[-context_words:]) if words_before_match else \"\"\n",
    "                        matched_word = text_str[start:end]\n",
    "                        context_after = ' '.join(words_after_match[:context_words]) if words_after_match else \"\"\n",
    "                        \n",
    "                        # Format the context nicely\n",
    "                        context_display = f\"...{context_before} **{matched_word}** {context_after}...\"\n",
    "                        context_display = context_display.replace(\"...\", \"\").strip()\n",
    "                        \n",
    "                        results[genre].append({\n",
    "                            'text_id': idx,\n",
    "                            'match': matched_word,\n",
    "                            'context': context_display,\n",
    "                            'full_text': text_str[:100] + \"...\" if len(text_str) > 100 else text_str\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"  ðŸ“ Text {idx}: {context_display}\")\n",
    "                else:\n",
    "                    results[genre].append({\n",
    "                        'text_id': idx,\n",
    "                        'matches': len(matches),\n",
    "                        'full_text': text_str[:100] + \"...\" if len(text_str) > 100 else text_str\n",
    "                    })\n",
    "        \n",
    "        if genre_hits > 0:\n",
    "            print(f\"  âœ… Found {genre_hits} occurrence(s) in {genre}\")\n",
    "        else:\n",
    "            print(f\"  âŒ No matches found in {genre}\")\n",
    "            \n",
    "        total_hits += genre_hits\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ SUMMARY:\")\n",
    "    print(f\"Total hits across all genres: {total_hits}\")\n",
    "    print(f\"Genres with matches: {len([g for g in results if results[g]])}\")\n",
    "    \n",
    "    return dict(results)\n",
    "\n",
    "# Helper function for frequency analysis\n",
    "def keyword_frequency_analysis(keyword, db_dict, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Analyze frequency of keyword across genres\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ“Š Frequency Analysis for '{keyword}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    freq_data = {}\n",
    "    \n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    for genre, df in db_dict.items():\n",
    "        total_words = 0\n",
    "        keyword_count = 0\n",
    "        \n",
    "        for text in df['text']:\n",
    "            text_str = str(text)\n",
    "            words = text_str.split()\n",
    "            total_words += len(words)\n",
    "            keyword_count += len(pattern.findall(text_str))\n",
    "        \n",
    "        # Calculate frequency per 1000 words\n",
    "        freq_per_1000 = (keyword_count / total_words * 1000) if total_words > 0 else 0\n",
    "        \n",
    "        freq_data[genre] = {\n",
    "            'count': keyword_count,\n",
    "            'total_words': total_words,\n",
    "            'freq_per_1000': round(freq_per_1000, 3)\n",
    "        }\n",
    "        \n",
    "        print(f\"{genre:8s}: {keyword_count:4d} occurrences | {freq_per_1000:6.3f} per 1000 words\")\n",
    "    \n",
    "    return freq_data\n",
    "\n",
    "print(\"âœ… Search functions created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "65513ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” COCA Corpus Search: 'textual'\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre:\n",
      "------------------------------\n",
      "  ðŸ“ Text 4: New Testaments , of multiple **textual** layers . In fact ,\n",
      "  ðŸ“ Text 4: little evolving world of complex **textual** strata . As in The\n",
      "  ðŸ“ Text 4: its own complicated pastiche of **textual** fragments recounted by at least\n",
      "  ðŸ“ Text 31: basis for multiple languages -- **textual** , graphic , photographic ,\n",
      "  ðŸ“ Text 73: describes television as \" the **textual** technology of information theory ,\n",
      "  ðŸ“ Text 78: a chorus-commentary underline the irritating **textual** bombardment , while a stereophonic\n",
      "  ðŸ“ Text 87: distinction between an intrinsic , **textual** \" you \" -- a\n",
      "  ðŸ“ Text 87: this page \" is both **textual** and extratextual : it refers\n",
      "  ðŸ“ Text 87: play with the location ( **textual** and/or extratextual ) of the\n",
      "  ðŸ“ Text 87: there is some evidence ( **textual** or historical ) to the\n",
      "  ðŸ“ Text 87: there is some evidence ( **textual** or historical ) to the\n",
      "  ðŸ“ Text 87: that involves discussion of the **textual** grounds for those experiences ,\n",
      "  ðŸ“ Text 88: , impersonal , nuclear , **textual** , performative , botanical ,\n",
      "  ðŸ“ Text 163: the Koran course and the **textual** passages that formed the basis\n",
      "  ðŸ“ Text 233: reminders for thematic issues and **textual** analysis revealed through classroom dialogue\n",
      "  ðŸ“ Text 247: as well as in the **textual** , visual , and epistemological\n",
      "  ðŸ“ Text 247: . \" Each of my **textual** examples , Elizabeth Gaskell 's\n",
      "  ðŸ“ Text 249: after finding ambiguity as to **textual** meaning.42 Second , if there\n",
      "  ðŸ“ Text 249: is a conflict between the **textual** meaning of a provision and\n",
      "  ðŸ“ Text 249: 314 and having a clear **textual** basis is neither a necessary\n",
      "  ðŸ“ Text 249: of a constitutional provision whose **textual** meaning is fairly clear but\n",
      "  âœ… Found 21 occurrence(s) in acad\n",
      "\n",
      "ðŸ“š BLOG Genre:\n",
      "------------------------------\n",
      "  ðŸ“ Text 263: subject and in providing a **textual** exegesis to the far-reaching deductions\n",
      "  ðŸ“ Text 980: hyperlinks , social bookmarks , **textual** content , graphics and also\n",
      "  âœ… Found 2 occurrence(s) in blog\n",
      "\n",
      "ðŸ“š FIC Genre:\n",
      "------------------------------\n",
      "  ðŸ“ Text 67: , a novel whose very **textual** structure disrupts all notions of\n",
      "  âœ… Found 1 occurrence(s) in fic\n",
      "\n",
      "ðŸ“š MAG Genre:\n",
      "------------------------------\n",
      "  âŒ No matches found in mag\n",
      "\n",
      "ðŸ“š NEWS Genre:\n",
      "------------------------------\n",
      "  âŒ No matches found in news\n",
      "\n",
      "ðŸ“š SPOK Genre:\n",
      "------------------------------\n",
      "  âŒ No matches found in spok\n",
      "\n",
      "ðŸ“š TVM Genre:\n",
      "------------------------------\n",
      "  âŒ No matches found in tvm\n",
      "\n",
      "ðŸ“š WEB Genre:\n",
      "------------------------------\n",
      "  âŒ No matches found in mag\n",
      "\n",
      "ðŸ“š NEWS Genre:\n",
      "------------------------------\n",
      "  âŒ No matches found in news\n",
      "\n",
      "ðŸ“š SPOK Genre:\n",
      "------------------------------\n",
      "  âŒ No matches found in spok\n",
      "\n",
      "ðŸ“š TVM Genre:\n",
      "------------------------------\n",
      "  âŒ No matches found in tvm\n",
      "\n",
      "ðŸ“š WEB Genre:\n",
      "------------------------------\n",
      "  ðŸ“ Text 48: @ @ @ @ @ **textual** matter , if any ,\n",
      "  ðŸ“ Text 571: that violates every rule of **textual** analysis and interpretation . If\n",
      "  âœ… Found 2 occurrence(s) in web\n",
      "\n",
      "ðŸŽ¯ SUMMARY:\n",
      "Total hits across all genres: 26\n",
      "Genres with matches: 4\n",
      "  ðŸ“ Text 48: @ @ @ @ @ **textual** matter , if any ,\n",
      "  ðŸ“ Text 571: that violates every rule of **textual** analysis and interpretation . If\n",
      "  âœ… Found 2 occurrence(s) in web\n",
      "\n",
      "ðŸŽ¯ SUMMARY:\n",
      "Total hits across all genres: 26\n",
      "Genres with matches: 4\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Search for a legal term across all genres\n",
    "keyword = \"textual\"\n",
    "search_results = search_keyword_corpus(keyword, db_text, case_sensitive=False, show_context=True, context_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "caf234ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š Frequency Analysis for 'textual'\n",
      "==================================================\n",
      "acad    :   21 occurrences |  0.015 per 1000 words\n",
      "blog    :    2 occurrences |  0.001 per 1000 words\n",
      "blog    :    2 occurrences |  0.001 per 1000 words\n",
      "fic     :    1 occurrences |  0.001 per 1000 words\n",
      "fic     :    1 occurrences |  0.001 per 1000 words\n",
      "mag     :    0 occurrences |  0.000 per 1000 words\n",
      "mag     :    0 occurrences |  0.000 per 1000 words\n",
      "news    :    0 occurrences |  0.000 per 1000 words\n",
      "spok    :    0 occurrences |  0.000 per 1000 words\n",
      "news    :    0 occurrences |  0.000 per 1000 words\n",
      "spok    :    0 occurrences |  0.000 per 1000 words\n",
      "tvm     :    0 occurrences |  0.000 per 1000 words\n",
      "tvm     :    0 occurrences |  0.000 per 1000 words\n",
      "web     :    2 occurrences |  0.001 per 1000 words\n",
      "web     :    2 occurrences |  0.001 per 1000 words\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Frequency analysis across genres\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "freq_results = keyword_frequency_analysis(keyword, db_text, case_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82238f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ”§ CLEANED COLLOCATE ANALYSIS:\n",
      "ðŸ”— Collocate Analysis for 'help' (window: Â±3 words)\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre Collocates:\n",
      "  Found 434 instances of 'help' in acad\n",
      "  the            : 126 times\n",
      "  and            :  67 times\n",
      "  can            :  56 times\n",
      "  that           :  42 times\n",
      "  with           :  36 times\n",
      "  students       :  36 times\n",
      "  will           :  28 times\n",
      "  may            :  27 times\n",
      "  for            :  27 times\n",
      "  them           :  20 times\n",
      "\n",
      "ðŸ“š BLOG Genre Collocates:\n",
      "  Found 434 instances of 'help' in acad\n",
      "  the            : 126 times\n",
      "  and            :  67 times\n",
      "  can            :  56 times\n",
      "  that           :  42 times\n",
      "  with           :  36 times\n",
      "  students       :  36 times\n",
      "  will           :  28 times\n",
      "  may            :  27 times\n",
      "  for            :  27 times\n",
      "  them           :  20 times\n",
      "\n",
      "ðŸ“š BLOG Genre Collocates:\n",
      "  Found 701 instances of 'help' in blog\n",
      "  the            : 189 times\n",
      "  you            :  98 times\n",
      "  and            :  97 times\n",
      "  that           :  69 times\n",
      "  can            :  55 times\n",
      "  with           :  51 times\n",
      "  will           :  50 times\n",
      "  for            :  47 times\n",
      "  get            :  33 times\n",
      "  your           :  33 times\n",
      "\n",
      "ðŸ“š FIC Genre Collocates:\n",
      "  Found 701 instances of 'help' in blog\n",
      "  the            : 189 times\n",
      "  you            :  98 times\n",
      "  and            :  97 times\n",
      "  that           :  69 times\n",
      "  can            :  55 times\n",
      "  with           :  51 times\n",
      "  will           :  50 times\n",
      "  for            :  47 times\n",
      "  get            :  33 times\n",
      "  your           :  33 times\n",
      "\n",
      "ðŸ“š FIC Genre Collocates:\n",
      "  Found 412 instances of 'help' in fic\n",
      "  the            :  80 times\n",
      "  you            :  74 times\n",
      "  could          :  44 times\n",
      "  and            :  41 times\n",
      "  with           :  32 times\n",
      "  but            :  32 times\n",
      "  can            :  29 times\n",
      "  him            :  27 times\n",
      "  for            :  27 times\n",
      "  her            :  24 times\n",
      "\n",
      "ðŸ“š MAG Genre Collocates:\n",
      "  Found 412 instances of 'help' in fic\n",
      "  the            :  80 times\n",
      "  you            :  74 times\n",
      "  could          :  44 times\n",
      "  and            :  41 times\n",
      "  with           :  32 times\n",
      "  but            :  32 times\n",
      "  can            :  29 times\n",
      "  him            :  27 times\n",
      "  for            :  27 times\n",
      "  her            :  24 times\n",
      "\n",
      "ðŸ“š MAG Genre Collocates:\n",
      "  Found 666 instances of 'help' in mag\n",
      "  the            : 173 times\n",
      "  you            : 116 times\n",
      "  can            :  75 times\n",
      "  and            :  70 times\n",
      "  will           :  56 times\n",
      "  with           :  55 times\n",
      "  that           :  54 times\n",
      "  them           :  39 times\n",
      "  your           :  38 times\n",
      "  for            :  37 times\n",
      "\n",
      "ðŸ“š NEWS Genre Collocates:\n",
      "  Found 666 instances of 'help' in mag\n",
      "  the            : 173 times\n",
      "  you            : 116 times\n",
      "  can            :  75 times\n",
      "  and            :  70 times\n",
      "  will           :  56 times\n",
      "  with           :  55 times\n",
      "  that           :  54 times\n",
      "  them           :  39 times\n",
      "  your           :  38 times\n",
      "  for            :  37 times\n",
      "\n",
      "ðŸ“š NEWS Genre Collocates:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ”§ CLEANED COLLOCATE ANALYSIS:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m collocate_results = \u001b[43mfind_collocates\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhelp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mfind_collocates\u001b[39m\u001b[34m(keyword, db_dict, window_size, min_freq, case_sensitive)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     31\u001b[39m     text_str = \u001b[38;5;28mstr\u001b[39m(text).lower() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m case_sensitive \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(text)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     words = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Find all positions of the keyword\u001b[39;00m\n\u001b[32m     35\u001b[39m     keyword_positions = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/__init__.py:120\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m tokenizer = _get_punkt_tokenizer(language)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1280\u001b[39m, in \u001b[36mPunktSentenceTokenizer.tokenize\u001b[39m\u001b[34m(self, text, realign_boundaries)\u001b[39m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   1277\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1278\u001b[39m \u001b[33;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1340\u001b[39m, in \u001b[36mPunktSentenceTokenizer.sentences_from_text\u001b[39m\u001b[34m(self, text, realign_boundaries)\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msentences_from_text\u001b[39m(\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1333\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   1334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1335\u001b[39m \u001b[33;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[32m   1336\u001b[39m \u001b[33;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[32m   1337\u001b[39m \u001b[33;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[32m   1338\u001b[39m \u001b[33;03m    follows the period.\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspan_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1340\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msentences_from_text\u001b[39m(\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1333\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   1334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1335\u001b[39m \u001b[33;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[32m   1336\u001b[39m \u001b[33;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[32m   1337\u001b[39m \u001b[33;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[32m   1338\u001b[39m \u001b[33;03m    follows the period.\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspan_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1328\u001b[39m, in \u001b[36mPunktSentenceTokenizer.span_tokenize\u001b[39m\u001b[34m(self, text, realign_boundaries)\u001b[39m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[32m   1327\u001b[39m     slices = \u001b[38;5;28mself\u001b[39m._realign_boundaries(text, slices)\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1457\u001b[39m, in \u001b[36mPunktSentenceTokenizer._realign_boundaries\u001b[39m\u001b[34m(self, text, slices)\u001b[39m\n\u001b[32m   1444\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1445\u001b[39m \u001b[33;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[32m   1446\u001b[39m \u001b[33;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1454\u001b[39m \u001b[33;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[32m   1455\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1456\u001b[39m realign = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1457\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_pair_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence2\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:324\u001b[39m, in \u001b[36m_pair_iter\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1430\u001b[39m, in \u001b[36mPunktSentenceTokenizer._slices_from_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m   1428\u001b[39m last_break = \u001b[32m0\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_potential_end_contexts(text):\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_contains_sentbreak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1431\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match.end())\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m match.group(\u001b[33m\"\u001b[39m\u001b[33mnext_tok\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1433\u001b[39m             \u001b[38;5;66;03m# next sentence starts after whitespace\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1478\u001b[39m, in \u001b[36mPunktSentenceTokenizer.text_contains_sentbreak\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m   1474\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1475\u001b[39m \u001b[33;03mReturns True if the given text includes a sentence break.\u001b[39;00m\n\u001b[32m   1476\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1477\u001b[39m found = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# used to ignore last token\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1478\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_annotate_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1479\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfound\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1612\u001b[39m, in \u001b[36mPunktSentenceTokenizer._annotate_second_pass\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m   1606\u001b[39m PUNCTUATION = \u001b[38;5;28mtuple\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m;:,.!?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1608\u001b[39m \u001b[38;5;66;03m# ////////////////////////////////////////////////////////////\u001b[39;00m\n\u001b[32m   1609\u001b[39m \u001b[38;5;66;03m# { Annotation Procedures\u001b[39;00m\n\u001b[32m   1610\u001b[39m \u001b[38;5;66;03m# ////////////////////////////////////////////////////////////\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_annotate_second_pass\u001b[39m(\n\u001b[32m   1613\u001b[39m     \u001b[38;5;28mself\u001b[39m, tokens: Iterator[PunktToken]\n\u001b[32m   1614\u001b[39m ) -> Iterator[PunktToken]:\n\u001b[32m   1615\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1616\u001b[39m \u001b[33;03m    Performs a token-based classification (section 4) over the given\u001b[39;00m\n\u001b[32m   1617\u001b[39m \u001b[33;03m    tokens, making use of the orthographic heuristic (4.1.1), collocation\u001b[39;00m\n\u001b[32m   1618\u001b[39m \u001b[33;03m    heuristic (4.1.2) and frequent sentence starter heuristic (4.1.3).\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token1, token2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(tokens):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def find_collocates(keyword, db_dict, window_size=5, min_freq=2, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Find words that frequently appear near the keyword (collocates)\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: Target word to find collocates for\n",
    "    - db_dict: Dictionary of DataFrames\n",
    "    - window_size: Number of words to look at on each side\n",
    "    - min_freq: Minimum frequency for a word to be considered a collocate\n",
    "    - case_sensitive: Whether to perform case-sensitive search\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”— Collocate Analysis for '{keyword}' (window: Â±{window_size} words)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    all_collocates = Counter()\n",
    "    genre_collocates = {}\n",
    "\n",
    "    for genre, df in db_dict.items():\n",
    "        print(f\"\\nðŸ“š {genre.upper()} Genre Collocates:\")\n",
    "        \n",
    "        # Create a fresh counter for each genre\n",
    "        genre_counter = Counter()\n",
    "        keyword_instances = 0\n",
    "        \n",
    "        for text in df['text']:\n",
    "            text_str = str(text).lower() if not case_sensitive else str(text)\n",
    "            words = nltk.word_tokenize(text_str)\n",
    "            \n",
    "            # Find all positions of the keyword\n",
    "            keyword_positions = []\n",
    "            for i, word in enumerate(words):\n",
    "                if (not case_sensitive and word.lower() == keyword.lower()) or (case_sensitive and word == keyword):\n",
    "                    keyword_positions.append(i)\n",
    "            \n",
    "            keyword_instances += len(keyword_positions)\n",
    "            \n",
    "            # Extract collocates around each keyword occurrence\n",
    "            for pos in keyword_positions:\n",
    "                start = max(0, pos - window_size)\n",
    "                end = min(len(words), pos + window_size + 1)\n",
    "                \n",
    "                # Get surrounding words (excluding the keyword itself)\n",
    "                context_words = words[start:pos] + words[pos+1:end]\n",
    "                \n",
    "                # Filter out punctuation and very short words\n",
    "                context_words = [w for w in context_words if w.isalpha() and len(w) > 2]\n",
    "                \n",
    "                genre_counter.update(context_words)\n",
    "                all_collocates.update(context_words)\n",
    "        \n",
    "        # Store the results for this genre\n",
    "        genre_collocates[genre] = genre_counter\n",
    "        \n",
    "        # Display top collocates for this genre\n",
    "        top_collocates = genre_counter.most_common(10)\n",
    "        if top_collocates:\n",
    "            print(f\"  Found {keyword_instances} instances of '{keyword}' in {genre}\")\n",
    "            # Show all results, but mark those below min_freq\n",
    "            for word, freq in top_collocates:\n",
    "                marker = \"  \" if freq >= min_freq else \"* \"\n",
    "                print(f\"{marker}{word:15s}: {freq:3d} times\")\n",
    "        else:\n",
    "            print(f\"  Found {keyword_instances} instances, but no significant collocates\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ TOP OVERALL COLLOCATES (min frequency: {min_freq}):\")\n",
    "    print(\"-\" * 40)\n",
    "    top_overall = all_collocates.most_common(20)\n",
    "    for word, freq in top_overall:\n",
    "        if freq >= min_freq:\n",
    "            print(f\"{word:15s}: {freq:3d} occurrences\")\n",
    "        \n",
    "    return {\n",
    "        'all_collocates': dict(all_collocates),\n",
    "        'by_genre': dict(genre_collocates),\n",
    "        #'top_overall': top_overall\n",
    "    }\n",
    "\n",
    "# Example 3: Find collocates for the keyword (CLEANED VERSION)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ”§ CLEANED COLLOCATE ANALYSIS:\")\n",
    "collocate_results = find_collocates('help', db_text, window_size=3, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74c346",
   "metadata": {},
   "source": [
    "### think about how it's helpful to have these various results displayed / consumed by the user\n",
    "\n",
    "The `got3` tool should have the following included steps for ease of access with working with COCA from BYU\n",
    "\n",
    "1. read the database files `got3.read_corpora(dir_of_text_files,corpora_name)`\n",
    "2. perform collocate analysis using `got3.find_collocates(keyword, db_dict, window_size, min_freq, case_sensitive)`\n",
    "3. perform keyword search using `got3.search_keyword_corpus(keyword, db_dict, case_sensitive, show_context, context_words)`\n",
    "4. perform keyword frequency analysis using `got3.keyword_frequency_analysis(keyword, db_dict, case_sensitive)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca4830",
   "metadata": {},
   "source": [
    "### Including BERT for Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7430da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee2169e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Tokenizing Academic Corpus...\n",
      "ðŸ¤— Tokenizing corpus with BERT (max_length=512)...\n",
      "âœ… Tokenized 265 documents\n",
      "âœ… Tokenized 265 documents\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model for embeddings\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "def bert_tokenize_corpus(corpus_data, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize corpus texts using BERT tokenizer\n",
    "    \n",
    "    Parameters:\n",
    "    - corpus_data: pandas Series or dict containing text data\n",
    "    - max_length: maximum sequence length for BERT\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with tokenized inputs\n",
    "    \"\"\"\n",
    "    tokenized_corpus = {}\n",
    "    \n",
    "    # Handle pandas Series\n",
    "    if hasattr(corpus_data, 'items'):\n",
    "        items = corpus_data.items()\n",
    "    else:\n",
    "        items = enumerate(corpus_data)\n",
    "    \n",
    "    print(f\"ðŸ¤— Tokenizing corpus with BERT (max_length={max_length})...\")\n",
    "    \n",
    "    for doc_id, text in items:\n",
    "        if pd.isna(text) or str(text).strip() == '':\n",
    "            continue\n",
    "            \n",
    "        text_str = str(text)\n",
    "        \n",
    "        # Tokenize with BERT\n",
    "        inputs = tokenizer(\n",
    "            text_str, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=max_length, \n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        tokenized_corpus[doc_id] = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'original_text': text_str[:200] + \"...\" if len(text_str) > 200 else text_str\n",
    "        }\n",
    "    \n",
    "    print(f\"âœ… Tokenized {len(tokenized_corpus)} documents\")\n",
    "    return tokenized_corpus\n",
    "\n",
    "# Tokenize the academic corpus\n",
    "print(\"ðŸŽ¯ Tokenizing Academic Corpus...\")\n",
    "bert_corpus = bert_tokenize_corpus(db_text['acad']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1e73b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Generating BERT embeddings for 10 documents...\n",
      "  âœ… Processed batch 1/3\n",
      "  âœ… Processed batch 1/3\n",
      "  âœ… Processed batch 2/3\n",
      "  âœ… Processed batch 3/3\n",
      "ðŸŽ¯ Generated embeddings for 10 documents\n",
      "  âœ… Processed batch 2/3\n",
      "  âœ… Processed batch 3/3\n",
      "ðŸŽ¯ Generated embeddings for 10 documents\n"
     ]
    }
   ],
   "source": [
    "def generate_bert_embeddings(tokenized_corpus, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for tokenized corpus\n",
    "    \n",
    "    Parameters:\n",
    "    - tokenized_corpus: Output from bert_tokenize_corpus\n",
    "    - batch_size: Number of documents to process at once\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with document embeddings\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    doc_ids = list(tokenized_corpus.keys())\n",
    "    \n",
    "    print(f\"ðŸ§  Generating BERT embeddings for {len(doc_ids)} documents...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(doc_ids), batch_size):\n",
    "            batch_ids = doc_ids[i:i+batch_size]\n",
    "            \n",
    "            # Prepare batch\n",
    "            input_ids_batch = torch.cat([tokenized_corpus[doc_id]['input_ids'] \n",
    "                                       for doc_id in batch_ids], dim=0)\n",
    "            attention_masks_batch = torch.cat([tokenized_corpus[doc_id]['attention_mask'] \n",
    "                                             for doc_id in batch_ids], dim=0)\n",
    "            \n",
    "            # Get BERT outputs\n",
    "            outputs = model(input_ids=input_ids_batch, attention_mask=attention_masks_batch)\n",
    "            \n",
    "            # Use [CLS] token embedding as document representation\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: [batch_size, 768]\n",
    "            \n",
    "            # Store embeddings\n",
    "            for j, doc_id in enumerate(batch_ids):\n",
    "                embeddings[doc_id] = {\n",
    "                    'embedding': cls_embeddings[j].numpy(),\n",
    "                    'original_text': tokenized_corpus[doc_id]['original_text']\n",
    "                }\n",
    "            \n",
    "            print(f\"  âœ… Processed batch {i//batch_size + 1}/{(len(doc_ids)-1)//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Generated embeddings for {len(embeddings)} documents\")\n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings for a sample of the corpus (first 10 documents for demo)\n",
    "sample_corpus = {k: v for k, v in list(bert_corpus.items())[:10]}\n",
    "bert_embeddings = generate_bert_embeddings(sample_corpus, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3eff267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Finding documents similar to: 'get out of text free card...'\n",
      "ðŸ“Š Top 3 most similar documents:\n",
      "  1. Doc 0 (similarity: 0.6984)\n",
      "     Preview: @@4000241 I think it is safe to say that ours is the only dining room in West Los Angeles on whose table -- an eight-foot-long , two-hundred-pound beh...\n",
      "\n",
      "  2. Doc 5 (similarity: 0.6897)\n",
      "     Preview: @@4000941 The expression \" sent up the river \" finds its roots in New York penal history . From the early 1800s on , convicted felons from New York Ci...\n",
      "\n",
      "  3. Doc 1 (similarity: 0.6870)\n",
      "     Preview: @@4000341 The high point of my freshman-year English literature survey course , taught by that sweet man and Emily Dickinson biographer Richard Sewall...\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ”Ž BERT-based keyword analysis for: 'help'\n",
      "ðŸ“ˆ Found 'help' in 0 documents\n"
     ]
    }
   ],
   "source": [
    "def find_similar_documents(query_text, bert_embeddings, tokenizer, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Find documents most similar to a query text using BERT embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    - query_text: Text to find similar documents for\n",
    "    - bert_embeddings: Dictionary of document embeddings\n",
    "    - tokenizer: BERT tokenizer\n",
    "    - model: BERT model\n",
    "    - top_k: Number of most similar documents to return\n",
    "    \n",
    "    Returns:\n",
    "    - List of tuples (doc_id, similarity_score, text_preview)\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Finding documents similar to: '{query_text[:100]}...'\")\n",
    "    \n",
    "    # Generate embedding for query text\n",
    "    with torch.no_grad():\n",
    "        query_inputs = tokenizer(\n",
    "            query_text, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=512, \n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        query_outputs = model(input_ids=query_inputs['input_ids'], \n",
    "                            attention_mask=query_inputs['attention_mask'])\n",
    "        query_embedding = query_outputs.last_hidden_state[:, 0, :].numpy()[0]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for doc_id, doc_data in bert_embeddings.items():\n",
    "        doc_embedding = doc_data['embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = 1 - cosine(query_embedding, doc_embedding)\n",
    "        similarities.append((doc_id, similarity, doc_data['original_text']))\n",
    "    \n",
    "    # Sort by similarity and return top k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"ðŸ“Š Top {top_k} most similar documents:\")\n",
    "    for i, (doc_id, sim_score, text_preview) in enumerate(similarities[:top_k]):\n",
    "        print(f\"  {i+1}. Doc {doc_id} (similarity: {sim_score:.4f})\")\n",
    "        print(f\"     Preview: {text_preview[:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "def bert_based_keyword_context(keyword, bert_embeddings, tokenizer, model, context_window=50):\n",
    "    \"\"\"\n",
    "    Find documents containing a keyword and analyze their BERT-based semantic context\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: Keyword to search for\n",
    "    - bert_embeddings: Dictionary of document embeddings  \n",
    "    - tokenizer: BERT tokenizer\n",
    "    - model: BERT model\n",
    "    - context_window: Characters around keyword to show\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with keyword contexts and their embeddings\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”Ž BERT-based keyword analysis for: '{keyword}'\")\n",
    "    \n",
    "    keyword_contexts = {}\n",
    "    \n",
    "    for doc_id, doc_data in bert_embeddings.items():\n",
    "        text = doc_data['original_text']\n",
    "        \n",
    "        # Find keyword occurrences\n",
    "        text_lower = text.lower()\n",
    "        keyword_lower = keyword.lower()\n",
    "        \n",
    "        if keyword_lower in text_lower:\n",
    "            # Find all occurrences\n",
    "            start = 0\n",
    "            occurrences = []\n",
    "            \n",
    "            while True:\n",
    "                pos = text_lower.find(keyword_lower, start)\n",
    "                if pos == -1:\n",
    "                    break\n",
    "                \n",
    "                # Extract context\n",
    "                context_start = max(0, pos - context_window)\n",
    "                context_end = min(len(text), pos + len(keyword) + context_window)\n",
    "                context = text[context_start:context_end]\n",
    "                \n",
    "                occurrences.append({\n",
    "                    'position': pos,\n",
    "                    'context': context,\n",
    "                    'keyword_start': pos - context_start,\n",
    "                    'keyword_end': pos - context_start + len(keyword)\n",
    "                })\n",
    "                \n",
    "                start = pos + 1\n",
    "            \n",
    "            if occurrences:\n",
    "                keyword_contexts[doc_id] = {\n",
    "                    'embedding': doc_data['embedding'],\n",
    "                    'occurrences': occurrences,\n",
    "                    'full_text': text\n",
    "                }\n",
    "    \n",
    "    print(f\"ðŸ“ˆ Found '{keyword}' in {len(keyword_contexts)} documents\")\n",
    "    \n",
    "    # Show context examples\n",
    "    for doc_id, data in list(keyword_contexts.items())[:3]:  # Show first 3 examples\n",
    "        print(f\"\\nðŸ“ Document {doc_id}:\")\n",
    "        for i, occ in enumerate(data['occurrences'][:2]):  # Show first 2 occurrences per doc\n",
    "            context = occ['context']\n",
    "            start, end = occ['keyword_start'], occ['keyword_end']\n",
    "            highlighted = context[:start] + f\"**{context[start:end]}**\" + context[end:]\n",
    "            print(f\"   Context {i+1}: ...{highlighted}...\")\n",
    "    \n",
    "    return keyword_contexts\n",
    "\n",
    "# Example usage: Find documents similar to a legal query\n",
    "query = \"get out of text free card\"\n",
    "similar_docs = find_similar_documents(query, bert_embeddings, tokenizer, model, top_k=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example usage: BERT-based keyword analysis\n",
    "keyword_analysis = bert_based_keyword_context(\"help\", bert_embeddings, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b35713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Clustering 10 documents into 3 clusters using BERT embeddings...\n",
      "\n",
      "ðŸ“Š CLUSTER ANALYSIS RESULTS:\n",
      "==================================================\n",
      "\n",
      "ðŸ—‚ï¸  Cluster_0 (5 documents):\n",
      "   â€¢ Doc 1: @@4000341 The high point of my freshman-year English literature survey course , taught by that sweet...\n",
      "   â€¢ Doc 2: @@4000441 There seems only one cause behind all forms of social misery : bigness . It appears to be ...\n",
      "   â€¢ Doc 3: @@4000541 The papers that comprise this symposium are adapted from remarks delivered on 31 May 2002 ...\n",
      "   ... and 2 more documents\n",
      "\n",
      "ðŸ—‚ï¸  Cluster_1 (2 documents):\n",
      "   â€¢ Doc 7: @@4001341 A computerized block design task was developed which records temporal and nontemporal meas...\n",
      "   â€¢ Doc 8: @@4001441 Our purpose in this study is to view theories of psychotherapy from a general social-polit...\n",
      "\n",
      "ðŸ—‚ï¸  Cluster_2 (3 documents):\n",
      "   â€¢ Doc 0: @@4000241 I think it is safe to say that ours is the only dining room in West Los Angeles on whose t...\n",
      "   â€¢ Doc 5: @@4000941 The expression \" sent up the river \" finds its roots in New York penal history . From the ...\n",
      "   â€¢ Doc 9: @@4001541 In April 1990 , I was in Japan for a United Nations conference , held in the industrial ci...\n",
      "\n",
      "ðŸ“ˆ Creating visualization...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0wAAAK9CAYAAAAXJOy/AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAg7lJREFUeJzt3Qd8U9X///FPWaXsIqsIMlW2MmSqiExR+ap83Sg4vyhLERX8iggiiIIigiLuDQ4cX1QcCCIiKhQQWcpGtrJXgTb/x/v0l/zTkdJCQpvk9fQR09x7k5zccxPO555zPjfG4/F4DAAAAACQQb6MiwAAAAAAQsAEAAAAAAEQMAEAAABAAARMAAAAABAAARMAAAAABEDABAAAAAABEDABAAAAQAAETAAAAAAQAAETAAAAAARAwAQgKC666CKrV6+ehZN169ZZTEyMvf7667ldFJwg1Z3qUHUZjqpWrWo9evTItfffuHGjFS5c2H788UeLJHnluMisHPqt1O1UOxXvO3DgQGvWrFlI3wPIDQRMQB7i/cfV/1auXDlr06aNffnllxm2T7+t/61nz56+7dQg818XGxtrZ511lj3yyCN2+PBhX8Mtq9fz3qIxuJg1a1aG/Ve+fHnX+BgxYoTt2LEjt4uYp7377rs2duzYHD0nOTnZXnvtNbePS5cu7fa5jtFbbrnF5s+fb6fKF198YY8++qhFqmHDhrkGbqtWrQL+XpQoUcLOOeccGzNmjCUlJWV4jUWLFlm3bt2scuXKrp5UX+3atXP1p3pMb/fu3S5I02svX748W+Xs0qWLFSlSxPbt2xdwmxtvvNEKFSpk//zzj0WrZcuWueM1twLFe+65xxYvXmyfffZZrrw/ECoFQvbKAE6qEVOtWjXzeDy2bds2F6R07tzZ/ve//9lll12WZtv27dvbzTffnOE1FBD5U0Pm5Zdfdn/v2bPHPv30U3vsscds9erV9s4777gG7f79+9M0FN977z175plnrEyZMr7lLVu2tGjVt29fO++881wjUEHS3LlzbciQIfb000/b+++/bxdffHFuFzHPBky///67a0xlx6FDh+yqq66y6dOn24UXXmgPPfSQa4SrEaj9/MYbb9iGDRusUqVKIS+7vgcTJkwIWdC0cuVKy5cvd85d6hjWvtQtPf/fCwU4H330kQ0YMMB+/fVXmzx5sm87baOTMzqBcNNNN9mZZ57pgpoZM2bYbbfdZlu2bHH15++DDz5wwVKFChXcb8/w4cOPW1YFQ/r9+/jjjzP9vTt48KD7TevUqZOddtpprizXXXed+xx5zddffx3SgGno0KHuRINOMJyq9/VSnf7rX/+y0aNHuyAXiBgeAHnGa6+95tHX8tdff02zfOfOnZ6CBQt6brjhhjTLtW2vXr2O+7rdu3f3FC1aNM2ylJQUT/PmzT0xMTGerVu3ZnjOU0895V5/7dq12Sp769atPXXr1vWEE302fUbt96zMnDnTbffBBx9kWLdo0SJPuXLlPKVKlfJs3rw5hKUNX5deeqmnSpUq2d5ex7T29zPPPJNh3bFjx9yxuXHjxjTfmewepznlLUsw6bt38OBBT257+umnPXFxcZ59+/Yd9/ciOTnZ06RJE7cvNm3a5Jb99NNPnvz583vOP/98z969ezO8vn7HMvtuXXjhhZ6rrrrKc++993qqVauWrbJqfxUvXtzTsWPHTNe/++67rmyTJ0/25CWhPj7T02+U3k+/Wbnlww8/dP+urF69OtfKAAQbQ/KAMFCqVCmLi4uzAgWC1ymsM7znn3++68Vas2ZN0F53wYIFrhdK5VUv2cSJE9OsP3LkiBsK2LhxYytZsqQVLVrULrjgAps5c2aG19KZbG1XvHhxNyyofv369uyzz6bZRme/1XPhHQ5Us2ZNGzVqlKWkpGTYTkON9J7an927d3fLTpaGKql3Tq81fvz4NOsWLlxol1xyiSt7sWLFrG3btjZv3rwMr6Hn3nvvve6MsD6Dek50Fv3vv//Ocj6Gd6ig7tPPJfvtt9+sdevWbhiT9smHH37o1n///fduCJbq5+yzz7Zvv/02Q3k2bdpkt956q+s1UHnq1q1rr776aqbvrR6fxx9/3JVZw6z0GVetWpWmPJ9//rmtX7/eN8Qr/Zlvf3/99Ze9+OKLruc0sx6p/Pnzu56OrHqX9B6Z9Qilny909OhRdzZevSIqu3om9J345ptv3Hptq94l72t6b146xlT32j96vvbXf/7zH9u1a1eG91XP8FdffWVNmjRx+16fMbMyeetac4r69+9vZcuWdd+RK6+8MsPQT72/PmfFihVdPWvornoYsjsv6pNPPnHHgo7N41EvmHf+i/c41L5TWdVLpO9oevqs6cuhnsEffvjB9f7otnbtWtdTezzaZ+p1VM/V9u3bM+3FVBm8vRqZfWc0lLNjx46ux9z7+6TjPKvvk/fzph+OrO+XPlv16tVd3atnRa+VneGA6ecSZTUc2lsWfX/uvvtu951V2XWsXn311Wk+n8qnZaJjIf1rZDaHSftSPYE6dvU59HuWvsfR+/nVazRp0iSrUaOG+11Qb7t6HNPTcExRjx8QKRiSB+RBGjKnxrKCGf2D9txzz7nhcponkJ7mIHkb1v7USNd4/qx4/7GNj48PSrnVUNTQwWuuucauv/5615i+6667XDm8DZO9e/e6YTxaf8cdd7jhO6+88opryPzyyy927rnnuu3UaNU2aoArABLNd1BDsl+/fr5hOAoK1MBXQ/WMM85wja9Bgwa5oUDeeTPajxomMmfOHDd8qHbt2m5oj4KmYPj3v//tGh0a8qLgQZYuXeoCQdXDAw88YAULFnSNZDVYvEGLqF61nT6b9lGjRo1cfWoOgIIH/+GQOakHNdDVIFUD6oUXXnB/q2GrIET74IYbbrCnnnrKlV0T/70NXg0Bbd68uWsg9e7d2zXYNX9On091lz6IeeKJJ1xjWkGMjtsnn3zSDZ/6+eef3fr//ve/brk+i4Z3SlYNdL3XsWPH3JCqUFOwMXLkSLv99tutadOm7vOpUZ2YmOgCNh1TmzdvdsfiW2+9leH5Wq9GquZVabimGv8KmhUo6zhVnfsPvdPxrOfouFfDNyt9+vRx30sN+dT3VMey6mPKlCm+bXSca39ffvnl7vujuSO6985LzIqCRTV29f3MLg3fFTXW9d1T8KIhk/reZZeG+SoA1PGphr8a3zouszPUV8eVGvP6XdG+8Nq5c6cLRrV/9ZqZ0e9ohw4d3PGsxAQ6aaL9OnXqVDsROiZ0okl1r2BJ33cFE7rXSRH/wPp40g+HFn1XNDdM+1pUV/pt0/dYJwtUdn2v9XuiIFkBs+pCx+G4cePcMEj9zon3PrOhr3q+TnBofyqA1HBJBYI6ieP9nfUPSvV7rWNYn0/HnoJY7Qf/Y10npVSv+g7oRBAQEYLeZwXgpIdvpL/FxsZ6Xn/99QzbZ7at9/bee+9lGGKzY8cOd1u1apVn9OjRbthEvXr13BChYAzJ0/ZjxozxLUtKSvKce+65bsjakSNHfEOqtNzfrl27POXLl/fceuutvmX9+vXzlChRwm0fyGOPPeY+1x9//JFm+cCBA91QoQ0bNrjHn3zyiSvbk08+6dtGr3vBBRec9JA8r3POOccTHx/ve3zFFVd4ChUqlGZYiobsaViRhiR5PfLII+61p06dmuE1vfUSaFiPt1z+w2+89aAhSl4rVqxwy/Lly+eZN2+eb/lXX32V4fPfdtttnoSEBM/ff/+d5r2uu+46T8mSJX1DybzvXbt27TT1+eyzz7rlS5YsOaEheRqmpecvXLgwW9tntm/0eMiQIRm2VRn0XfCvM5XtRIbk/fDDD275O++8k2b59OnTMyzX+2qZ1h2vTN7P065duzTfS+0XHdO7d+92jzWMtkCBAu448/foo4+65/u/Zmb0G6DtnnvuuQzrMvu9GDFihPu9aNCggdtm8eLF7vn6nuZE/fr1PTfeeKPv8UMPPeQpU6aM5+jRo8d9rr6zOjZbtGiRZvnEiRNdWXQ8BzouPv7440yHOx/v+xRo6G5mQyr1m6vtZs+eHbAc3u+oboG8//777jnDhg3L8v00JFLbvfnmm9kakpf+fceOHeu2ffvtt33L9Dut/VusWDHfMEvv5z/ttNPc8HCvTz/91C3/3//+l+G9OnTo4H4bgEjBkDwgD9IwIJ3B1O3tt992wyt0Fjyzs6HqOfFu63/Tc/wdOHDAnV3VTUO01COgzFgaNpGTs6FZ0ZBBnX30Us+SHuvsrobqeYdUeXu+NKRIZ4fVo6DhOzqz76UzwCqzd3hUZnQ2VL0zOhOvXhnvTUNClJhh9uzZvon7Kpv/2XSVQ2fxg0W9Jt4MXnpv9TZdccUVbsiOV0JCguvZUU+XejNEk+k1DEZDrtI70XpRWXQm2ku9GdqfOtPsn/LX+7d3SKZiDZVHPRb623+fqudCPUX+dSQ6w+7fk6n68H/NnPLul8yGeAWb9ol6BP78888cP1fHns6kqyfKfz9pCKn2f/ohpjp7r32YXXfeeWea+td+1XGloVmi3h19bzRMy192j2nv0LFAvcvpfy/UY9GiRQvXM3ui9aRhbEuWLHE9QV76W/tNPUTHo++sjuuffvopzVA09XxoSJl6o7Oqa5k2bZrrXTtZ/j1Z3l5+9cxK+u9ITqi3SD3N+l1/+OGHM30/lV/1p3rR5zrR99PvonrH/OtDPUXqpVKPl3rC/V177bVpjpesvuve32QgUjAkD8iDNDxIAYSX/kFr2LChGzahoSz+DVQNz/COGc+Kxqcry5RoeJSGUyiQCTSEJdAQDjWa/ekfXC/NpdBwm8yy9amB421QaFiNUhSvWLEiTeNFjUovNQQ19EZzgE4//XQ3nEZD/ZQFy0sNXTXC1KjLjHeugxqZClbSDwU73rConFADw9t41FwTDVnK7PUVtChQ1DA4zX3RMKeuXbtaMOmYSB9sqXGveV7pl4l3zo3KraE4GlqkW2bSzx9JPxzL26BKP48nuzSEUbJKHx3MbJRqmOoY1bwvHVsaCtigQYPjPlfHnr4LSvufnf3kf2xnx/H2qzdwUqPZn7IJ5mSIbWqHXNa/F5qvovL7zxs7kXrSyR/9Pugkgneem95Hc3g0LO/SSy/N1rA8DVdTkKQgTr9lmhOlRr4CqkA0dFffM8270vM1FE0nNHQC40Qy6elEj15L8yzT13X638jsUhCqIW76vXvzzTfTfIf126vho0rVriHI/vV2ou+nY0jz99JnafQO4fMeYyfyXVf5gnUiDsgLCJiAMKB/0NRjpIQHaqipoZ1Takz4B1Y6212rVi3XA5Tda2Zo/oR6FLLT4Mqq0aQx8mqs3H///a7BqbKpMeCdIyFarjH8OvOseS26qbGgZAjeSckKPHSGX3OEMpM+tXqoKOj7448/Qnbh3kANj8yucSOBGo6Blnvr0JsoQ3PlAs3vSh9MHO81c0rHpKgnwjufLVjS7y/N+dAxp15W9Qhqbp0a00pUoh7drGhf6RhVQz8z6YP4nJyYCMV+Tc87NyZQYJv+9yI9BWrqtVU9ZYfKrflL6rmqU6dOhvUKOnTS4XgJKNSDp2NEr6WASfd6bQVSx/sOKfGJ5hcpENTvinpydOJGy/S+Ofme6eSN5hTpN0zHqZ6vY0JBd/qEM9ml30XNmdNcTm9A6t9zqN8/zSFUT59Odqi86nE70ffLqZwckzquTmT+JZBXETABYULDbyT95OATpR4XTcjVWVI1GLy9P1lRkJXVEDn9Y68GkX8vkwIJ8WZGU6NFZ5g1vNC/gaLJ7empJ03Dw3RTo0C9TkqcMHjwYNdg08Ri7Y/j9bBVqVLFDWFK3yDTRPxg0GfSGWDvkCs1ljUJO7PXV6+aAmBvb48+g65RlBXvmdz0Wf3SnwE+WSq3esnUQMxOr2V25eRMs3oU1TBTYH2iiR+0v9LvK2VnVCKQ9NQjo5MAuun4UBClZBDegClQ2VVvyjCoYa05DYaCQce0qKfGv/dKQ7Wy07un3gKVW4kqToSOb1137LvvvnO9pel7L9PT8C71BqlXL30SApVXQxCVtS+zxDbpKTjSb4B6l9XTpF4SZWzLDv3O6abkLHquXku9RKrv7H7PVF79nui3Uxk/vU5kaKd/8hR9fv0uek8apP+N0UkMBXj+QwHTlzUn3zUdQ9qH+m3172XSb5R3/YnScaWhxkCkYA4TEAbUg6Ez4AogAmU8OhE6a6mGj/6xzm6QpYa0/y19UOdNl+xtpOqxGuI6M+x/ltL/rKQyqmlegr/06Xn1D7q3dyMpKcl3llfPy2z+gxoS3iBTmfv0t7JKeSkoUPbBk6XMZDrrq8ZWr169fJ9RQwjVc+E/10IZ6NRIU+pq7xlkDRPSa3jnhvjz7iM1zsU7J8tb/kDD5k6Uyq3yaB5TZkFc+rTW2aUAOrvDhtTwVhY5He+Z1Y8ad2o0qvEdiPaX/74S7av0PQXpjzEF0wrEvceXt+ySvmGqY0+vp4s/p6djLRgp67Oi+Trq4fE/piV9avtANFdFw36VFfBE6SSHjlEFtpmdyNG8RW9vsHc4nnpklJnR/6b6VtATqLcuPW9vkoIV9UIfr3fJG+Sk7wnx9mB661sBgr4D6Y+d559/Ps3jzH7DxJuVM6cUeGu+kjJKquc9M3rP9O+n70f6YzrQ8ZoZ/S5u3bo1TeZFHbt6XX0XNIzxROi7rp7baL7IOSIPPUxAHqThZ96zfBqqoka2zl4qHW76oRrqwVFjJD1NgtZwteMNy9GZdTUIlNb6ZIMxzWFSCnAFCRoOp3+I1aBRY9WbdlZzsHQWVUkONGdBZyI1BErDdPwbXTrjq3kCOoutuRM6y6t/yNXI8ZZTjS8NJ9RrajiLgjL1cGmYkM7IqhwaFqIeKvUEaP9pmd5LZcjp2H/NldBZXTVS1NhW2ly9v4bHKODxn881fPhw1xun4Eg9Y2rcKnhU40zzx7z0GVRWpf/WECF9Bn1uva72i87SagimzoorjbTWqVdEZ8W9AWEwKXhWwgIlhFBDVvtK76mJ5WrY6e+c0mfSsaDrCqknQI0x1UkgCojU4NK8FNWT6lcBqa7ho2QL+m74J7VIT8eOUqcr+NN3QAGpgur0Q4T02TSXReXTPlXwoLrwT1ntDfRVFvUgehMPqDGp4awaSqpjXAGyjnF9T1VGDZ9VMBAq+n4r7bP2la49pKFg+pz67dDnzE5Pg+ZvqZGuuTPpf1eyQw1iJajR8a1eEQVOCnw0r0nX/tExrO+BjnkF4aoLzVnKjD6D9pl+7wLNC/NSj5re23udn+wETArc9Dun3x0F1CrjSy+95D63AgfR91jfQ/3OaP9pOyWJSD9HSc9RT6S+xzqZpTlHCvBPtLdOc1R1Ukn7Lv1vufaZ6lrfAaW2Vxl13OpEkb6P3qGVXvp91DGq32H9vml+ln5DM9un6tXTb5J+OxXcahSAjn/9rin4O9HEKyqX91IOQMTI7TR9ALJOK164cGGXmvuFF17IkP47q7Ti/uljvWmCM6O010pXnD4N8YmkFa9bt65n/vz5Li2tyq2UyePHj0+znT6DUhRrndKlN2zY0DNt2jT3/v6pp3W1eKWmVUpypec+44wzPP/5z388W7ZsSfN6+/bt8wwaNMhTs2ZNt51SFLds2dKlTfemMpd//vnHc9NNN7lU5UqPrb+VujonacW9t4IFC3rKli3r0oM//vjjnu3bt2f6vMTERE/Hjh1dit4iRYp42rRp45k7d26G7VS23r17e04//XT3GSpVquT2h39qb9WTUk1rnykFu9Ixf/PNN5mmFVc9pKd9m1kKbT1fqbP9bdu2zS2rXLmy+6wVKlTwtG3b1jNp0qTjplrPLAXz/v37PTfccIOnVKlSbl12UowrhfTLL7/sUr+rvlQOPe+WW25Jk3I8s7TNycnJngcffNAdC9rvqgOlxk6fwnv48OGepk2bunLFxcV5atWq5erT/7hROfr06ePqW2m10/+zqX3SuHFj93yljFfa7AceeMClkD/evs8qrXj69NeZpbxW2QYPHuzqR+9/8cUXe5YvX+7SP/fs2fO4+1j1rNTkb731VprlWf1eZGbBggWufitWrOjqSen1dby88cYbri4++ugjV/ZXXnkl4GvMmjXLbaO09NkxYcIEt73qLzPpjwt9F6+//nr3O6LvkH5XLrvsMvd75U9p1Lt27eqOG30O/eb8/vvvGY7pv/76y3PllVe6Y0fH59VXX+3qPH1K++ykFc/qd9xb37r0go59HdP6PdExrcsFpD9+5KWXXvJUr17d/a77v0Zm6cx1DHhfV789On7T/x56v9P6NyG9zFL4X3vttZ7zzz8/03oBwlWM/pfbQRsAADh5Goql3jj17Kj36Hh0QWL1Uqv3FDhZGuKnHkD1gNPDhEjCHCYAAMKQEo2k551Ho6GG2Z2H9Ouvv7phWMDJ0vFXv359giVEHHqYAAAIQ6+//rq7aQ6O5oXpgshKs635VNm5ECwAIHtI+gAAQBhS1kglE1HyASVu8CaC0HA8AEDw0MMEAAAAAAEwhwkAAAAAAiBgAgAAAIAAomoOk64Qv3nzZncxtuxc1A8AAABAZNLMJF3IumLFipYvX+B+pKgKmBQsVa5cObeLAQAAACCP2Lhxo1WqVCng+qgKmNSz5N0pJUqUyO3iIMi9hzt27LCyZctmeYYA4Ye6jVzUbWSjfiMXdRu5oq1u9+7d6zpTvDFCIFEVMHmH4SlYImCKvC/44cOHXb1Gwxc8mlC3kYu6jWzUb+SibiNXtNZtzHGm6kTPngAAAACAHCJgAgAAAIAACJgAAAAAIIComsMEAAAAnEj66WPHjllycrJF+hymo0ePunlMkTCHKX/+/FagQIGTvpwQARMAAAAQwJEjR2zLli128OBBi4bAUEGTrk0UKdcsLVKkiCUkJFihQoVO+DUImAAAAIBMKHhYu3at66nQxU3V6I6UQCKrnrRg9Mrkhc+iYFdp0lWHZ5555gn3mhEwAQAAAJlQg1tBk67Vo56KSBdJAZPExcVZwYIFbf369a4uCxcubCci/AcnAgAAACEUCfN5olW+INQdtQ8AAAAAARAwAQAAAEAABEwAAABAlNJcpU8++SS3i5GnETABAAAAEWjr1q3Wp08fq169usXGxrrkFZdffrnNmDEjJO83a9YsF4Dt3r3bQmXnzp124403WokSJaxUqVJ222232f79+y2UyJIHAAAAhFpKitnSpWa7dpnFx5vVrauMBCF7u3Xr1lmrVq1cUPHUU09Z/fr13UVpv/rqK+vVq5etWLHC8nK2vuTkZJetLz0FS7ou1jfffOM+zy233GJ33nmnvfvuuyErDz1MAAAAQCjNnWvWrZvZzTeb9eyZeq/HWh4id999t+vt+eWXX6xr16521llnWd26da1///42b968gD1EutaUfw/RokWL3OsoABOl6FYvVXx8vBUtWtS95hdffOHWt2nTxm2jdXpOjx493GOlZh85cqRVq1bNpfo+55xz7MMPP0zzvtr+yy+/tMaNG7vesDlz5mQo3/Lly2369On28ssvW7Nmzez888+35557ziZPnmybN2+2UKGHCQAAAAgVBUUDBmgsmVlCgi4OZHbokNnChanLR482a9ky6MPWFFg8/vjjLqhJT71OJ6pXr17umkazZ892r71s2TIrVqyYG+730UcfueBs5cqVbsicgiNRsPT222/bxIkT3QVk9dxu3bpZ2bJlrXXr1r7XHjhwoI0ePdoNIVTQld5PP/3kyt6kSRPfsnbt2rnU4T///LNdeeWVFgoETAAAAECohuGNH58aLNWsqQwLqcuLFTOrUcNs9WqzCRPMmjcP6vC8VatWuWFttWrVsmDbsGGDC4o0xE8U3HiVLl3a3ZcrV84XlCUlJdmIESPs22+/tRYtWvieox6kF198MU3ANGzYMGvfvn2Wc7L02v40bE/vq3WhQsAEAAAAhILmLC1fntqz5A2WvPS4QgWzZctSt/u/ACQYFCyFSt++fe2uu+6yr7/+2vXuKHhq0KBBlsHbwYMHMwRC6qVq2LBhmmX+PUd5CXOYAAAAgFBQgoekpNRheJnRcq3XdkGkYW+aE5TTxA4a2pY+4Dp69GiabW6//XZbs2aN3XTTTbZkyRIX5GgeUSDeDHaff/65mw/lvWkon/88Jsls+KC/ChUq2Pbt29MsO3bsmBuCqHWhQsAEAAAAhILm4cTGps5ZyoyWa30m83VOhoaodezY0SZMmGAHDhzIsD5Q2m/NKRJlofNatGhRhu00X6lnz542depUu+++++yll15yy5UwQpThzqtOnTouiYOG8tWsWTPNTa+TExrSp7IvWLDAt+y7775zSSWUBCJUCJgAAACAUFDq8Nq1NflG3TZp1+mxltepk7pdkClYUuDStGlTl4zhzz//dFnmxo0b55tLlJ43iBk6dKjbXr1CY8aMSbPNPffc41KTr1271hITE23mzJlWW5/RzKpUqeJ6tqZNm2Y7duxwvUvFixe3AQMG2L333mtvvPGGrV692j1PvVJ6nBN6n06dOtkdd9zhsv/9+OOP1rt3b7vuuuusYsWKFioETACQFycJL1liNnt26r0eAwDCj4a49e6d2oOkBA8anqbeF93rsZb36hWS6zEpsYICE6X6Vi9QvXr13DwiXbT2hRdeyPQ5BQsWtLfeessN5dO8pFGjRtnw4cPTbKMgTJnyvMGL0pU///zzbt3pp5/ugi1luytfvrwLZuSxxx6zwYMHu2x53ucpGFOa8Zx65513XDKLtm3bWufOnV1q8UmTJlkoxXhCOSssj9m7d6+VLFnS9uzZ41IdInKoK1ZjWpU5xTv+FpEh6upW6WeVUUmThDWuXUM1dOZO/+gEOe1sbou6uo0y1G/kiqa6PXz4sOtJUcO+cOHCwf1tV8+SgqU89NuusEBzgpR5Tj1FkV6He7MZG5AlDwCi+FodAIBTQL/dSh2ubHhK8KCeJQ3Di/CAM1IQMAFAFF+rAwBwiui3O4ipw3Hq8K8uAITbtToAAMApQ8AEAFF8rQ4AAJA1AiYAiOJrdQAAgKwRMAFAlF+rAwAABEbABABRfq0OAAAQGP/yAkBeSjur1OENG5rt3m22bl3qfaNGpBQHACCXkFYcAPISrtUBAECewr/AAJBXr9Vx4YWp9wRLAIAQiYmJsU8++SS3i5Gn8a8wAAAAEIG2bt1qffr0serVq1tsbKxVrlzZLr/8cpsxY0ZI3m/WrFkuANut4eQh8vjjj1vLli2tSJEiVqpUKTsVGJIHAAAAhFhKyqkdbb1u3Tpr1aqVCyqeeuopq1+/vh09etS++uor69Wrl61YscLyKo/HY8nJyVagQMZQ5ciRI3b11VdbixYt7JVXXjkl5aGHCQAAAAihuXPNunUzu/lms549U+/1WMtD5e6773a9Pb/88ot17drVzjrrLKtbt67179/f5s2bF7CHqFChQml6iBYtWuReRwGYrF+/3vVSxcfHW9GiRd1rfvHFF259mzZt3DZap+f06NHDPU5JSbGRI0datWrVLC4uzs455xz78MMP07yvtv/yyy+tcePGrjdszpw5mZZx6NChdu+997oA8FShhwkAAAAIEQVFAwaY7dxplpBgFheXei3yhQtTl4ciCerOnTtt+vTpbviagpr0TmYoW69evVwvz+zZs91rL1u2zIoVK+aG+3300UcuOFu5cqWVKFHCBUeiYOntt9+2iRMn2plnnume261bNytbtqy1bt3a99oDBw600aNHuyGECrryCgImAAAAIETD8MaPTw2WatZUgoXU5cWKmdWokXqZvQkTUpOjBnN43qpVq9ywtlq1almwbdiwwQVF3h4eBTdepUuXdvflypXzBWVJSUk2YsQI+/bbb90wOu9z1IP04osvpgmYhg0bZu3bt7e8hoAJAAAACAHNWVq+PLVnyRsseelxhQpmy5albhfMEWYKlkKlb9++dtddd9nXX39t7dq1c8FTgwYNsgzeDh48mCEQUi9VQ1130E+TJk0sL2IOEwAAABACSvCQlJQ6DC8zWq712i6YNOxNc4Jymtgh3/91c/kHXEePHk2zze23325r1qyxm266yZYsWeKCnOeeey7ga+7fv9/df/75524+lPemoXz+85gks+GDeQEBEwAAABACmoYTG5s6ZykzWq71wZ6uo6FxHTt2tAkTJtiBAwcyrA+U9ltzimTLli2+ZYsWLcqwneYr9ezZ06ZOnWr33XefvfTSS265EkaIMtx51alTxyVx0FC+mjVrprnpdcIBARMAAAAQAkodXru2roekXpu06/RYy+vUSd0u2BQsKXBp2rSpS8bw559/2vLly23cuHG+uUTpeYMYZaLT9uoVGjNmTJpt7rnnHpeafO3atZaYmGgzZ8602vqQZlalShXXszVt2jTbsWOH610qXry4DRgwwGW2e+ONN2z16tXueeqV0uOcUuClIE73+nzeHitvT1YoEDABAAAAIaARbr17p/YgKcGD2vTqfNG9Hmt5r16huR6TEisoMFGqb/UC1atXz80j0kVrX3jhhUyfU7BgQXvrrbfcUD7NSxo1apQNHz48zTYKUpQpT0FSp06dXLry559/3q07/fTTXbClbHfly5e33vrwZvbYY4/Z4MGDXbY87/MUjCnNeE498sgjbu7TkCFDXJCkv3WbP3++hUqMJ5SzwvKYvXv3WsmSJW3Pnj0u1SEih/L7b9++3WVl8Y6/RWSgbiMXdRvZqN/IFU11e/jwYdeTooZ94cKFTyq1uLLlKQGE5ixpGJ56lhQsBTul+MlQWHDs2DF3wVj1FEV6He7NZmxAljwAAAAghBQUKXW4suEpwYN6ljQML8LjzYhBwAQAAACEmIKjYKYOx6lDXAsAAAAAARAwAQAAAEAABEwAAAAAEAABEwAAAAAEQMAEAAAAAAEQMAEAAABAAARMAAAAABAAARMAAAAQpWJiYuyTTz7J7WLkaQRMAAAAQIileFJsybYlNnv9bHevx6G2detW69Onj1WvXt1iY2OtcuXKdvnll9uMGTNC8n6zZs1yAdju3btD8vrr1q2z2267zapVq2ZxcXFWo0YNGzJkiB05csRCqUBIXx0AAACIcnM3zrXxv4y35TuWW1JyksXmj7XaZWtb76a9rWXlliELLlq1amWlSpWyp556yurXr29Hjx61r776ynr16mUrVqywvMrj8VhycrIVKJA2VFGZU1JS7MUXX7SaNWva77//bnfccYcdOHDARo8eHbLy0MMEAAAAhDBYGvD1AEvckmil4kpZ1VJV3f3CLQvdcq0Phbvvvtv19vzyyy/WtWtXO+uss6xu3brWv39/mzdvXsAeokKFCqXpIVq0aJF7HQVgsn79etdLFR8fb0WLFnWv+cUXX7j1bdq0cdtonZ7To0cP91hBzsiRI309Q+ecc459+OGHad5X23/55ZfWuHFj1xs2Z86cDOXr1KmTvfbaa9ahQwfXa9alSxcbMGCATZ061UKJHiYAAAAgBDTsTj1LOw/ttJqla7qgQIoVKmY1Stew1TtX24RfJljzSs0tX0zw+jF27txp06dPt8cff9wFNemp1+lE9erVyw2Bmz17tnvtZcuWWbFixdxwv48++sgFZytXrrQSJUq44EgULL399ts2ceJEO/PMM91zu3XrZmXLlrXWrVv7XnvgwIGup0jBkIKu7NizZ4+VLl3aQomACQAAAAiBpduXumF4CcUTfMGSlx5XKF7Blu1Y5rarX75+0N531apVblhbrVq1LNg2bNjggiIN8RMFN17ewKVcuXK+oCwpKclGjBhh3377rbVo0cL3HPUgaWidf8A0bNgwa9++fY4+53PPPRfS4XhCwAQAAACEwK7Du9ycpbgCqT0t6Wn5tuRtbrtgUrAUKn379rW77rrLvv76a2vXrp0Lnho0aJBlUHPw4MEMgZB6qRo2bJhmWZMmTbJdjk2bNrkheldffbWbxxRKzGECAAAAQiC+cLxL8HDo2KFM12u51mu7YNKwN/Vg5TSxQ758+TIEXEePHk2zze23325r1qyxm266yZYsWeKCHPXyBLJ//353//nnn7v5UN6bhvL5z2OSzIYPZmbz5s1uvlTLli1t0qRJFmphGzA98cQT7kC45557crsoAAAAQAZ1y9V12fC27tuaoddHj7W8Ttk6brtg0tC4jh072oQJE1wGufQCpf3WnCLZsmWLb9miRYsybKf5Sj179nTJFu677z576aWX3HIljBBluPOqU6eOS+KgoXzKbOd/0+vklHqWLrroIpccQgkgvEFeKIVlwPTrr7+6MY9Zdf8BAAAAuUmJHJQ6PD4u3iV42H9kvyWnJLt7PdbyXk17BTXhg5eCJQUuTZs2dckY/vzzT1u+fLmNGzfON5coPW8QM3ToULe9eoXGjBmTZht1Vig1+dq1ay0xMdFmzpxptWvXduuqVKniOjSmTZtmO3bscL1LxYsXd5ns7r33XnvjjTds9erV7nnqldLjnPAGS2eccYabt6T30LWmdAulsAuYtONvvPFGF8lmN3sGAAAAkBt0naXRHUZbw4SGtvvQblu3e527b5TQyC0P1XWYlFhBgYmGrqkXqF69em4ekS5a+8ILL2T6nIIFC9pbb73lhvKpY2LUqFE2fPjwNNsoCFOmPAVJmkOkdOXPP/+8W3f66ae7YEvZ7sqXL2+9e/d2yx977DEbPHiwy5bnfZ6CMaUZz4lvvvnGzYnSZ6hUqZIlJCT4bqEU4wnlrLAQ6N69u+tmfOaZZ1yEee6559rYsWMz3VZZOXTz2rt3r4uad+3a5VIdInIov7/OMqgr+VR0zeLUoW4jF3Ub2ajfyBVNdXv48GF3fSE17AsXLnzSKcaVDU8JHjRnScPwQtGzdLI0Z0mBUyTV4dq1a61q1aoZ6lCxgTpglJo8q9ggrLLkTZ482UXKGpKXHYpiFeWmpy+5dh4i68dbB7vi/0j/8Y421G3kom4jG/UbuaKpbhU86PMeO3bM3U5W7dNSh65JSnKK6b+8RHXqnX+UPg16uFK9qQ7/+eefDIHgvn37svUaYRMwbdy40fr16+e64rIb4Q8aNMhdzTh9D5POiNDDFFn0RdAXOxrOdkUb6jZyUbeRjfqNXNFUtzrBrkZ1gQIF3C1aRFIPU4ECBdxxetppp2WIIbIbU4RNzS9YsMC2b99ujRo18i1TBKwrBY8fP94NvcufP3+a5ygjh27paadF+hc8GunHm7qNTNRt5KJuIxv1G7mipW71+fRZvbdIpx4m7+eMlM8b8391l9nxmt3jN2wCprZt27pc7/5uueUWdwXjBx98MEOwBAAAAAAnK2wCJqUkVHaP9Be3Uvda+uUAAABAsIRZjjQEue4iux8VAAAAOMm5PAcPHsztouAEeevuZOZlhU0PU2ZmzZqV20UAAABAhNKUj1KlSrl59FKkSJGImdsTqDdGWeWUKCHcP6fH43HBkupOdXgy03fCOmACAAAAQqlChQru3hs0RTIFGcqC6E12EQlKlSrlq8MTRcAEAAAABKDAISEhwcqVK+euyxTJvNcrUo6ASMiAWLBgwaAkhiNgAgAAAI5DDe9Iz8qsgElBhq5PFAkBU7CwJwAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIgYAIAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIgYAIAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIgYAIAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAKWA4sX77cJk+ebD/88IOtX7/eDh48aGXLlrWGDRtax44drWvXrhYbG5uTlwQAAACA8O5hSkxMtHbt2rnAaM6cOdasWTO755577LHHHrNu3bqZx+Ox//73v1axYkUbNWqUJSUlhb7kAAAAAJAXepjUc3T//ffbhx9+aKVKlQq43U8//WTPPvusjRkzxh566KFglhMAAAAA8mbA9Mcff1jBggWPu12LFi3c7ejRo8EoGwAAAADk/SF5gYKlw4cP52h7AAAAAIjoLHkpKSlu7tLpp59uxYoVszVr1rjlgwcPtldeeSUUZQQAAACA8AiYhg8fbq+//ro9+eSTVqhQId/yevXq2csvvxzs8gEAAABA+ARMb775pk2aNMluvPFGy58/v2/5OeecYytWrAh2+QAAAAAgfAKmTZs2Wc2aNTMdqkeyBwAAAABRHTDVqVPHXbg2PaUc13WaAAAAACCq0or7e+SRR6x79+6up0m9SlOnTrWVK1e6oXrTpk0LTSkBAAAAIBx6mP71r3/Z//73P/v222+taNGiLoBavny5W9a+ffvQlBIAAAAAwqGHSS644AL75ptvgl8aAAAAAAjngOnXX391Q/GaNWuWZvnPP//ssuY1adIkmOUDgMBSUsyWLjXbtcssPt6sbl2zfDnuOAcAAAgoxy2LXr162caNGzMs15wmrQOAU2LuXLNu3cxuvtmsZ8/Uez3WcgAAgNwKmJYtW2aNGjXKsFwZ8rQOAEJOQdGAAWaJiWalSplVrZp6v3Bh6nKCJgAAkFsBU2xsrG3bti3D8i1btliBAic0JQoAcjYMb/x4s507zXRNuGLFzHQRbd3XqJE6PG/ChNTtAAAATnXA1KFDBxs0aJDt2bPHt2z37t320EMPkSUPQOhpztLy5WYJCWYxMWnX6XGFCuoKT90OAADgJOW4S2j06NF24YUXWpUqVXwXql20aJGVL1/e3nrrrZMtDwBkTT1ISUlmcXGZr9dy9YJrOwAAgFMdMJ1++un222+/2TvvvGOLFy+2uLg4u+WWW+z666+3ggULnmx5ACBryoYXG2t26FDqMLz0tFzrtR0AAMBJOqFJR7pg7Z133nmy7w0AOafU4bVrpyZ40Jwl/2F5Ho/Z1q1mSkyj7QAAAHIjYPrzzz9t5syZtn37dndNJn+PPPLIyZYJAALTdZZ6907Nhrd6deqcJQ3DU8+SgiX1LOkSB1yPCQAA5EbA9NJLL9ldd91lZcqUsQoVKliM39ld/U3ABCDkWrbUhMrUbHlKAKE5SxqGp54lBUtaDwAAkBsB0/Dhw+3xxx+3Bx98MBjvDwAnRkFR8+ap2fCU4EE9SxqGR88SAADIzYBp165ddvXVVwezDABwYhQc1a+f26UAAAARLMenYhUsff3116EpDQAAAACEcw9TzZo1bfDgwTZv3jyrX79+hlTiffv2DWb5AAAAACB8AqZJkyZZsWLF7Pvvv3c3f0r6QMAEAAAAIGoDprVr14amJAAAAACQx5BOCgAAAACCeeHav/76yz777DPbsGGDHTlyJM26p59++kReEgAAAADCP2CaMWOGdenSxapXr24rVqywevXq2bp168zj8VgjXTQSAAAAAKJ1SN6gQYNswIABtmTJEitcuLB99NFHtnHjRmvdujXXZwIAAAAQ3QHT8uXL7eabb3Z/FyhQwA4dOuSy5g0bNsxGjRoVijICAAAAQHgETEWLFvXNW0pISLDVq1f71v3999/BLR0AAAAAhNMcpubNm9ucOXOsdu3a1rlzZ7vvvvvc8LypU6e6dQAAAAAQtQGTsuDt37/f/T106FD395QpU+zMM88kQx4AAACA6A6YlB3Pf3jexIkTg10mAAAAAMgTuHAtAAAAAJxMD1Pp0qXtjz/+sDJlylh8fLzFxMQE3Hbnzp3ZeUkAAAAAiIyA6ZlnnrHixYu7v8eOHRvqMgEAAABA+ARM3bt3d/fHjh1zvUsdO3a08uXLh7psAAAAABA+c5h0odqePXva4cOHQ1ciAAAAAAjXpA9Nmza1hQsXhqY0AAAAABDOacXvvvtud7Hav/76yxo3buxSi/tr0KBBMMsHAAAAAOETMF133XXuvm/fvr5lmtfk8XjcfXJycnBLCAAAAADhEjCtXbs2NCUBAAAAgHAPmKpUqRKakgAAAABAuAdMXsuWLbMNGzbYkSNH0izv0qVLMMoFAAAAALkuxwHTmjVr7Morr7QlS5b45i6J/hbmMAEAAACI2rTi/fr1s2rVqtn27dutSJEitnTpUps9e7Y1adLEZs2aFZpSAgAAAEA49DD99NNP9t1331mZMmUsX7587nb++efbyJEjXeY8rtEEAAAAIGp7mDTkrnjx4u5vBU2bN2/2JYNYuXJl8EsIAAAAAOHSw1SvXj1bvHixG5bXrFkze/LJJ61QoUI2adIkq169emhKCQAAAADhEDA9/PDDduDAAff3sGHD7LLLLrMLLrjATjvtNJsyZUooyggAAAAA4REwdezY0fd3zZo1bcWKFbZz506Lj4/3ZcoDAAAAgKicw/T222/7epi8SpcuTbAEAAAAIOLkOGC69957rXz58nbDDTfYF198wXWXAAAAAESsHAdMW7ZsscmTJ7sepWuuucYSEhKsV69eNnfu3NCUEAAAAADCJWAqUKCAS/TwzjvvuIvXPvPMM7Zu3Tpr06aN1ahRIzSlBAAAAIBwSPrgr0iRIi4JxK5du2z9+vW2fPny4JUMAAAAAMKth0kOHjzoepg6d+5sp59+uo0dO9auvPJKW7p0afBLCAAAAADh0sN03XXX2bRp01zvkuYwDR482Fq0aBGa0gEAAABAOAVM+fPnt/fff98NxdPfAAAAABCpchwwaSgeAAAAAESDE5rDBAAAAADRgIAJAAAAAMI9YBo5cqSdd955Vrx4cStXrpxdccUVtnLlytwuFgAAAIAIFjYB0/fff2+9evWyefPm2TfffGNHjx61Dh062IEDB3K7aAAAAAAi1ElduPbzzz+3WbNmWXJysrVq1cq6du1qoTJ9+vQ0j19//XXX07RgwQK78MILQ/a+AAAAAKLXCQdMuv7S1KlT7dJLLzWPx2P33nuvC56ee+45OxX27Nnj7kuXLh1wm6SkJHfz2rt3r7tPSUlxN0QO1aeOQ+o18lC3kYu6jWzUb+SibiNXtNVtSjY/Z4xHeyUb5s+fb02aNPE9Puuss2zx4sUWFxfnHuvviy66yHbt2mWn4sN16dLFdu/ebXPmzAm43aOPPmpDhw7NsPyPP/5wc6EQOXRMKIguWbKk5csXNiNNkQ3UbeSibiMb9Ru5qNvIFW11u2/fPhfT6DOXKFHi5HuYevbsaeeff76NGDHCihQpYtWrV7cxY8bY1VdfbUeOHLEXXnjBveGpoLlMv//+e5bBkgwaNMj69++fpoepcuXKVrZs2Sx3CsLzCx4TE+PqNhq+4NGEuo1c1G1ko34jF3UbuaKtbgsXLpyt7bIdMP388882evRoa9SokT311FP26quvWp8+feyZZ55xc5gUTL377rsWar1797Zp06bZ7NmzrVKlSlluGxsb627p6QCIhoMg2ugLTt1GJuo2clG3kY36jVzUbeSKprrNl83PmO2AKX/+/Pbggw+6HqW77rrLihYtauPHj7eKFSvaqaCRgwrQPv74YzdXqlq1aqfkfQEAAABErxyHjhqK99VXX9mVV17pstNNmDDBTtUwvLffftv1Ymn+0datW93t0KFDp+T9AQAAAESfbAdMSrDwwAMP2OWXX24PP/ywC5g0TO/XX3+15s2b25IlS0JaUM2R0oQsJZZISEjw3aZMmRLS9wUAAAAQvbI9JK979+4uaLr++uttxowZbljeW2+95a6HpMfXXnutC6ZGjRoVkoJmM5kfAAAAAJz6gOm7776zhQsXWs2aNe2OO+5w915t27a1xMREGzZsWPBKBgAAAADhMiTvzDPPtEmTJrlrGE2cONGqVKmSIS2fUo4DAAAAQNQFTEojrl6mhg0busQLmlMEAAAAAJEs20Pyzj33XJs/f35oSwMAAAAAeUjkX5EKAAAAAE5FwPT8889bu3bt7JprrnGZ8fz9/fff7hpNAAAAABB1AdO4cePs/vvvt1q1allsbKx17tzZRo4c6VufnJxs69evD1U5AQAAACDvzmF68cUX7aWXXrIbbrjBPdZ1mK644go7dOgQ6cQBAAAARHfAtHbtWmvZsqXvsf5W1jwN0Tt69Kjdc889oSojAAAAAOTtgKlMmTK2ceNGq1q1qm9ZvXr1XNB08cUX2+bNm0NVRgAAAADI23OYzj//fJs6dWqG5XXq1HEJIL788stglw0AAAAAwqOHaeDAgbZgwYJM19WtW9f1NH300UfBLBsAAAAAhEfA1KBBA3cLRMPzdAMAAACAqAuYvNasWWNz5syxLVu2WL58+dy1l9q3b28lSpQITQkBAAAAIK8HTAcOHLAePXr4ht3FxMRYuXLlbMeOHRYXF2dPPPGE9erVK5RlBQAAAIC8mfShf//+rlfpt99+sz/++MOuuuoqu/nmm23v3r327LPP2gMPPGDvvvtuaEsLIHtSUsyWLDGbPTv1Xo8BAAAQuh4mZcibPn26b57SpEmTrGLFijZkyBC79dZb3QVsn3rqKd+FbQHkkrlzzcaPN1u+3CwpySw21qx2bbPevXUBtdwuHQAAQGT2MB07dizNPKVixYq5ZRqqJx06dLAVK1aEppQAsh8sDRhglphoVqqUma6bpvuFC1OXaz0AAACCHzCdd955buidl/4uW7asu8n+/ftdEAUgl2jYnXqWdu40q1lTZzXM8udPva9Rw2zXLrMJExieBwAAEIoheUrqoGx4SvpQqFAh27p1q73xxhu+9XPnzrXOnTvn5L0BBNPSpanD8BISlJUl7To9rlDBbNmy1O3q18+tUgIAAERmwNSoUSP7/fffbdq0aZaUlGQXX3yx1alTx7deGfLIkgfkIvUgac5SXFzm67V827bU7QAAABD86zAlJCTYHXfckZOnADhV4uNTEzwcOpQ6DC89Ldd6bQcAAIDgzmE6HqUc37BhQ7BeDkBO1a2bmg1v61YzjyftOj3WcvUKazsAAACc2oBJQ/SqVasWrJcDkFP58qWmDlcP0urVysRilpyceq/HWq5hs9oOAAAAwR+Sl5U333zTDh48GKyXA3AidJ2l0aP//3WYNGdJw/AaNUoNlrgOEwAAQO4ETEo7DiAPUFDUvHlqNjwleFDPkobh0bMEAABw6gOmo0ePWsGCBU/2ZQAEk4IjUocDAACctGyfcn7//fftyJEjvsfjx4+3KlWqWOHCha1MmTI2bNiwky8NAAAAAIRjD9P111/vMuGVK1fOXnvtNbv//vvtgQcesGbNmtnChQtt5MiRVrFiRbv99ttDW2IAAAAAyGsBk8cvTfHEiRNdj5KCJuncubOVLl3ann/+eQImAAAAABEjR7PAY2Ji3P2aNWusQ4cOadbp8apVq4JbOgAAAABhLyXFbMkSs9mzU+/1OCKTPkyfPt1Klizp5i2lTyF++PBhX0AFAAAAADJ37v+/4klSUuoVT2rXTr18ZDhc8SRHAVP37t19f3/33XfWokUL3+N58+ZZjRo1gls6AAAAAGEdLA0YYLZzp1lCgllcnNmhQ2YLF6Yu1+Uj83rQlO2AKeU4/Wbly5d3iR8AAAAAICUltWdJwVLNmprek7q8WDEz9bOsXm02YULq5SPz8uUig3bh2ssuuyxYLwUAAAAgzC1dmjoMTz1L6Wfu6HGFCmbLlqVul5cvHxm0WO7AgQM2W7O4AAAAAES9XbtS5yxpGF5mtFzrtV1eFrSASRny2rRpE6yXAwAAABDG4uNTEzxozlJmtFzrtV1elodHCwIAAAAIV3XrpmbD27pV13RNu06PtbxOndTt8rJsz2HShWmzkpycHIzyAAAAAIgASuSg1OHKhqcED5qz5M2Sp2BJPUu9euXthA85CpiSkpLsrrvusvoBZmStX7/ehg4dGsyyAQAAAAhjLVumpg73Xodp27bUYXiNGqUGS3k9pXiOAqZzzz3XKleunOZaTP4WL15MwAQAAAAgDQVFSh2ubHhK8KCeJQ3Dy+s9SzkOmC699FLbvXt3lkP2br755mCVCwAAAECEyJcvb6cOD0rA9NBDD2W5Xr1Pr732WjDKBAAAAAB5Qph0hAEAAADAqUfABAAAAAABEDABAAAAQAAETAAAAAAQAAETAAAAAAQrYNq4caP99ddfvse//PKL3XPPPTZp0qScvhQAAAAARFbAdMMNN9jMmTPd31u3brX27du7oOm///2vDRs2LBRlBAAAAIDwCJh+//13a9q0qfv7/ffft3r16tncuXPtnXfesddffz0UZQQAAACA8AiYjh49arGxse7vb7/91rp06eL+rlWrlm3ZsiX4JQQAAACAcAmY6tataxMnTrQffvjBvvnmG+vUqZNbvnnzZjvttNNCUUYAAAAACI+AadSoUfbiiy/aRRddZNdff72dc845bvlnn33mG6oHAAAAAJGgQE6foEDp77//tr1791p8fLxv+Z133mlFihQJdvkAAAAAIHwCJsmfP3+aYEmqVq0arDIBAAAAQHgOydu2bZvddNNNVrFiRStQoIALnvxvAAAAABC1PUw9evSwDRs22ODBgy0hIcFiYmJCUzIAAAAACLeAac6cOS5D3rnnnhuaEgEAAABAuA7Jq1y5snk8ntCUBgAAAADCOWAaO3asDRw40NatWxeaEgEAAABAuA7Ju/baa+3gwYNWo0YNl0a8YMGCadbv3LkzmOUDAAAAgPAJmNTDBAAAAADRIMcBU/fu3UNTEgAAAAAIx4Bp7969VqJECd/fWfFuBwAAAABRETDFx8fbli1brFy5claqVKlMr72kzHlanpycHIpyAgAAAEDeDJi+++47K126tPt75syZoS4TAAAAAIRPwNS6detM/wYAAACASJbjpA9eSi2+YcMGO3LkSJrlDRo0CEa5AAAAACD8AqYdO3bYLbfcYl9++WWm65nDBAAAACBS5MvpE+655x7bvXu3/fzzzxYXF2fTp0+3N954w84880z77LPPQlNKAAAAAAiHHiYlgPj000+tSZMmli9fPqtSpYq1b9/epRMfOXKkXXrppaEpKQAAAADk9R6mAwcOuPTi3nTjGqIn9evXt8TExOCXEAAAAADCJWA6++yzbeXKle7vc845x1588UXbtGmTTZw40RISEkJRRgAAAAAIjyF5/fr1cxexlSFDhlinTp3snXfesUKFCtnrr78eijICAAAAQHgETN26dfP93bhxY1u/fr2tWLHCzjjjDCtTpkywywcAAAAA4XcdJq8iRYpYo0aNglMaAAAAAAi3gKl///7ZfsGnn376ZMoDAAAAAOEVMC1cuDBbLxYTE3Oy5QEAAACA8AqYZs6cGfqSAAAAAEC4pxX3t3HjRncDAAAAgEiU44Dp2LFjNnjwYCtZsqRVrVrV3fT3ww8/bEePHg1NKQEAAAAgHLLk9enTx6ZOnWpPPvmktWjRwi376aef7NFHH7V//vnHXnjhhVCUEwAAAADyfsD07rvv2uTJk+2SSy7xLWvQoIFVrlzZrr/+egImAAAAANE7JC82NtYNw0uvWrVqVqhQoWCVCwAAAADCL2Dq3bu3PfbYY5aUlORbpr8ff/xxtw4AAAAAonZInq7JNGPGDKtUqZKdc845btnixYvtyJEj1rZtW7vqqqt822quEwAAAABETcBUqlQp69q1a5plmr8EAAAAABbtAdNrr70WmpIAAAAAQLjPYVqxYkXAdV999dXJlgcAAAAAwjdgatSokU2YMCHNMiV9UMKHf/3rX8EsGwAAAACE15C8119/3e666y77/PPP3fC8LVu22A033GApKSn2ww8/hKaUyFtSUsyWLjXbtcssPt6sbl2zfDmOvQEAAIA8L8et3GuuucZlxTt69KjVrVvXWrRoYa1bt7bExEQ777zzQlNK5B1z55p162Z2881mPXum3uuxlgMAAAAR5oS7BZRGPDk52d0SEhKscOHCwS0Z8h4FRQMGmCUmKl2imS5grPuFC1OXEzQBAAAg2gOmyZMnW/369a1kyZL2xx9/uKF5kyZNsgsuuMDWrFkTmlIibwzDGz/ebOdOs5o1zYoVM8ufP/W+Ro3U4Xma26btAAAAgGgNmG677TYbMWKEffbZZ1a2bFlr3769LVmyxE4//XQ799xzQ1NK5D7NWVq+3CwhwSwmJu06Pa5QwWzZstTtAAAAgGhN+qC5SmeffXaaZfHx8fb+++/bW2+9FcyyIS9RD1JSkllcXObrtXzbttTtAAAAgGjtYVKwdOzYMfv222/txRdftH379rnlmzdvtiuvvDIUZUReoGx4sbFmhw5lvl7LtV7bAQAAANHaw7R+/Xrr1KmTbdiwwV1/SUPyihcvbqNGjXKPJ06cGJqSIncpdXjt2qkJHjRnyX9YnsdjtnWrLtKVuh0AAAAQrT1M/fr1syZNmtiuXbsszm94lnqXZsyYEezyIa/QdZZ6907tQVq92mz/frPk5NR7PdbyXr24HhMAAACiu4dJF6edO3euFSpUKM3yqlWr2qZNm4JZNuQ1LVuajR6dmi1PCSA0Z0nD8NSzpGBJ6wEAAIBoDphSUlLctZfS++uvv9zQPEQ4BUXNm6dmw1OCB/UsaRgePUsAAACIQDlu5Xbo0MHGjh3rexwTE2P79++3IUOGWOfOnYNdPuRFCo7q1ze78MLUe4IlAAAARKgc9zCNGTPGOnbsaHXq1LHDhw/bDTfcYH/++aeVKVPG3nvvvdCUEgAAAADCIWCqVKmSLV682KZMmeLu1buki9neeOONaZJAAAAAAEDUBUzuSQUKuABJNwAAAACIVNmafDJv3rxsv+DBgwdtqRIChMiECRNcRr7ChQtbs2bN7JdffgnZewEAAACIbtkKmG666SY3b+mDDz6wAwcOZLrNsmXL7KGHHrIaNWrYggULLBQ0DLB///4uwURiYqKdc845rlzbt28PyfsBAAAAiG7ZCpgUDF166aX28MMPW6lSpaxu3brWvn17u/zyy+388893CR8aNWpka9euta+//tpuvvnmkBT26aeftjvuuMNuueUWl3Ri4sSJVqRIEXv11VdD8n4AAAAAolu25jAVLFjQ+vbt627z58+3OXPm2Pr16+3QoUOul+fee++1Nm3aWOnSpUNW0CNHjrieq0GDBvmW5cuXz9q1a2c//fRTps9JSkpyN6+9e/f6riWlGyKH6tPj8VCvEYi6jVzUbWSjfiMXdRu5oq1uU7L5OXOc9KFJkybudqr9/fff7oK55cuXT7Ncj1esWJHpc0aOHGlDhw7NsHzHjh0uJToi64Dfs2eP+5IrkEbkoG4jF3Ub2ajfyEXdRq5oq9t9+/aFLkteuFBvlOY8+fcwVa5c2cqWLWslSpTI1bIh+F9wXURZdRsNX/BoQt1GLuo2slG/kYu6jVzRVreFCxeOrIBJ86Ty589v27ZtS7NcjytUqJDpc2JjY90tPR0A0XAQRBt9wanbyETdRi7qNrJRv5GLuo1c0VS3+bL5GcNmTxQqVMgaN25sM2bMSBMF63GLFi1ytWwAAAAAIlPY9DCJhtd1797dzaFq2rSpjR071qU5V9Y8AAAAAAi2HPcwvfnmm2kyz/lnsdO6ULr22mtt9OjR9sgjj9i5555rixYtsunTp2dIBAEAAAAAuRIwqTdH2TMyyzJxKnp6evfu7VKaK2j7+eefrVmzZiF/TwAAAADRKccBk9IMajJYen/99ZeVLFkyWOUCAAAAgPCZw9SwYUMXKOnWtm1bK1Dg/z9V10dau3atderUKVTlBAAAAIC8GzBdccUV7l7zhjp27GjFihVLk8GuatWq1rVr19CUEgAAAADycsA0ZMgQd6/ASMkXsnuhJwAAAACImrTiSuvtzYq3fft2dy0kf2eccUbwSgcAAAAA4RQw/fnnn3brrbfa3LlzM00GoflMAAAAABCVAVOPHj1cwodp06ZZQkJCphnzAAAAACAqAyYlfViwYIHVqlUrNCUCAAAAgHC9DlOdOnXs77//Dk1pAAAAACCcA6ZRo0bZAw88YLNmzbJ//vnH9u7dm+YGAAAAAFE7JK9du3buXhev9UfSBwAAAAAW7QHTzJkzQ1MSAAAAAAj3gKl169ahKQkAAAAAhPscJvnhhx+sW7du1rJlS9u0aZNb9tZbb9mcOXOCXT4AAAAACJ+A6aOPPrKOHTtaXFycJSYmWlJSklu+Z88eGzFiRCjKCAAAAADhETANHz7cJk6caC+99JIVLFjQt7xVq1YugAIAAACAqA2YVq5caRdeeGGG5SVLlrTdu3cHq1wAAAAAEH4BU4UKFWzVqlUZlmv+UvXq1YNVLgAAAAAIv4DpjjvusH79+tnPP//srru0efNme+edd2zAgAF21113haaUAAAAABAOacUHDhxoKSkp7sK1Bw8edMPzYmNjXcDUp0+f0JQSAAAAAMIhYFKv0n//+1+7//773dC8/fv3W506daxYsWKhKSEAAAAAhEvA5FWoUCEXKAEAAABApMpxwHTgwAF74oknbMaMGbZ9+3Y3PM/fmjVrglk+AAAAAAifgOn222+377//3m666SZLSEhwQ/QAAAAAIBLlOGD68ssv7fPPP3cXqgUAAACASJbjtOLx8fFWunTp0JQGAAAAAMI5YHrsscfskUcecSnFAQAAACCS5XhI3pgxY2z16tVWvnx5q1q1qhUsWDDN+sTExGCWDwAAAADCJ2C64oorQlMSAAAAAAj3gGnIkCGhKQkAAAAARMqFaxcsWGDLly93f9etW9caNmwYzHLhVNA1tJYuNdu1S9k8VJFm+XI8rQ0AAACIWDkOmHSx2uuuu85mzZplpUqVcst2795tbdq0scmTJ1vZsmVDUU4E29y5ZuPHmynoTUoyi401q13brHdvs5Ytc7t0AAAAQJ6Q4+6EPn362L59+2zp0qW2c+dOd/v9999t79691rdv39CUEsEPlgYMUIYOMwW9Vaum3i9cmLpc6wEAAADkvIdp+vTp9u2331pt9Ub8nzp16tiECROsQ4cOwS4fQjEMTz1LO3ea1axpFhOTurxYMbMaNcxWrzabMMGseXOG5wEAACDq5bhFnJKSkiGVuGiZ1iGP05wlDcNLSPj/wZKXHleoYLZsWep2AAAAQJTLccB08cUXW79+/Wzz5s2+ZZs2bbJ7773X2rZtG+zyIdiU4EFzluLiMl+v5Vqv7QAAAIAol+OAafz48W6+ki5aW6NGDXerVq2aW/bcc8+FppQIHmXDU4KHQ4cyX6/lWq/tAAAAgCiX4zlMlStXtsTERDePacWKFW6Z5jO1a9cuFOVDsCl1uOafKcGD5iz5D8vzeMy2bjVr1Ch1OwAAACDKndB1mGJiYqx9+/buhjCjRA5KHa5seErwoDlLGoanniUFS+pZ6tWLhA8AAADAiQzJkxkzZthll13mG5Knv9XjhDCh6yyNHm2miw3v3m22bl3qvXqWtJzrMAEAAAAn1sP0/PPPu6QP//73v929zJs3zzp37mzPPPOM9VLvBPI+BUVKHa5seErwoJ4lDcOjZwkAAAA48YBpxIgRLjDqrWFd/0cXrG3VqpVbR8AURhQc1a+f26UAAAAA8qwcdyfs3r3bOnXqlGG5Llq7Z8+eYJULAAAAAMIvYOrSpYt9/PHHGZZ/+umnbi4TAAAAAETtkLw6derY448/brNmzbIWLVr45jD9+OOPdt9999m4cePSDNUDAAAAgKgJmF555RWLj4+3ZcuWuZtXqVKl3Dr/1OMETAAAAACiKmBau3ZtaEoCAAAAAHkMOaQBAAAAIFg9TB6Pxz788EObOXOmbd++3VJSUtKsnzp1ak5fEgAAAAAiI2C655577MUXX7Q2bdpY+fLl3VwlAAAAAIhEOQ6Y3nrrLdeL1Llz59CUKAqleFJs6faltuvwLosvHG91y9W1fDGMlgQAAADCLmAqWbKkVa9ePTSliUJzN8618b+Mt+U7lltScpLF5o+12mVrW++mva1l5Za5XTwAAAAgquW4G+PRRx+1oUOH2qFDh0JToigLlgZ8PcAStyRaqbhSVrVUVXe/cMtCt1zrAQAAAIRRD9M111xj7733npUrV86qVq1qBQsWTLM+MTExmOWL6GF46lnaeWin1Sxd0zcXrFihYlajdA1bvXO1TfhlgjWv1JzheQAAAEC4BEzdu3e3BQsWWLdu3Uj6cBI0Z0nD8BKKJ2TYh3pcoXgFW7Zjmduufvn6uVZOAAAAIJrlOGD6/PPP7auvvrLzzz8/NCWKEkrwoDlLcQXiMl2v5duSt7ntAAAAAOSOHI/1qly5spUoUSI0pYkiyoanBA+HjmU+F0zLtV7bAQAAAAiTgGnMmDH2wAMP2Lp160JToiih1OHKhrd131Z3MWB/eqzldcrWcdsBAAAACJMheZq7dPDgQatRo4YVKVIkQ9KHnTt3BrN8EUuJHJQ6XNnwlOBBc5Y0DE89SwqW4uPirVfTXiR8AAAAAMIpYBo7dmxoShKFdJ2l0R1G+67DpDlLGobXKKGRC5a4DhMAAAAQhlnyEDwKipQ6XNnwlOBBc5Y0DI+eJQAAACAMAyZJTk62Tz75xJYvX+4e161b17p06WL58+cPdvmigoIjUocDAAAAERAwrVq1yjp37mybNm2ys88+2y0bOXKky56nlOOa2wQAAAAAkSDH47769u3rgqKNGzdaYmKiu23YsMGqVavm1gEAAABA1PYwff/99zZv3jwrXbq0b9lpp51mTzzxhLVq1SrY5QMAAACA8Olhio2NtX379mVYvn//fitUqFCwygUAAAAA4RcwXXbZZXbnnXfazz//7C6wqpt6nHr27OkSPwAAAABA1AZM48aNc3OYWrRoYYULF3Y3DcWrWbOmPfvss6EpJQAAAACEwxymUqVK2aeffuqy5XnTiteuXdsFTAAAAAAQtQHT3r17rVixYpYvXz4XIHmDpJSUFLeuRIkSoSonAAAAAOTdIXkff/yxNWnSxA4fPpxh3aFDh+y8886z//3vf8EuHwAAAADk/YDphRdesAceeMCKFCmSYV3RokXtwQcftPHjxwe7fAAAAACQ9wOm33//3S666KKA6y+88EJbsmRJsMoFAAAAAOETMO3atcuOHTsWcP3Ro0fdNgAAAAAQdQFT1apVbf78+QHXa12VKlWCVS4AAAAACJ+A6aqrrrL//ve/tm3btgzrtm7dag8//LB17do12OUDAAAAgLyfVnzgwIHu+ktnnnmmdevWzc4++2y3fMWKFfbOO+9Y5cqV3TYAAAAAEHUBU/Hixe3HH3+0QYMG2ZQpU3zzlXQhWwVQjz/+uNsGAAAAAKLywrUlS5a0559/3iZMmGB///23eTweK1u2rMXExISuhAAAAAAQDgGTlwIkBUoAAAAAEMmynfQBAAAAAKINARMAAAAABEDABAAAAAABEDABAAAAwMkkfRg3bpxlV9++fbO9LQAAAACEfcD0zDPPZDt7HgETAAAAgKgKmNauXRv6kgAAAABAJFyHCScnxZNiS7cvtV2Hd1l84XirW66u5YthOhkAAAAQEQHTX3/9ZZ999plt2LDBjhw5kmbd008/HayyRaS5G+fa+F/G2/Idyy0pOcli88da7bK1rXfT3taycsvcLh4AAACAkwmYZsyYYV26dLHq1avbihUrrF69erZu3TrzeDzWqFGjnL5c1AVLA74eYDsP7bSE4gkWVyDODh07ZAu3LHTLR3cYTdAEAAAA5CE5Hgc2aNAgGzBggC1ZssQKFy5sH330kW3cuNFat25tV199dWhKGSHD8NSzpGCpZumaVqxQMcufL7+7r1G6hu06tMsm/DLBbQcAAAAgTAOm5cuX28033+z+LlCggB06dMiKFStmw4YNs1GjRoWijBFBc5Y0DE89S8om6E+PKxSvYMt2LHPbAQAAAAjTgKlo0aK+eUsJCQm2evVq37q///47uKWLIErwoDlLGoaXGS3Xem0HAAAAIEznMDVv3tzmzJljtWvXts6dO9t9993nhudNnTrVrUPmlA1PCR40Z0nD8NLTcq3XdgAAAADCNGBSFrz9+/e7v4cOHer+njJlip155plkyMuCUocrG54SPGjOkv+wPCXM2LpvqzVKaOS2AwAAABCmAZOy4/kPz5s4cWKwyxSRdJ0lpQ5XNrzVO1e7OUveLHkKluLj4q1X015cjwkAAACIhAvXzp8/3yWAkDp16ljjxo2DWa6IpJThSh3uvQ7TtuRtbhieepYULJFSHAAAAAjzgEkXrb3++uvtxx9/tFKlSrllu3fvtpYtW9rkyZOtUqVKoShnxFBQ1LxSc5cNTwkeNGdJw/DoWQIAAADynhy30m+//XY7evSo613auXOnu+nvlJQUtw7Hp+Cofvn6dmGVC909wRIAAAAQIT1M33//vc2dO9fOPvts3zL9/dxzz9kFF1wQ7PIBAAAAQK7JcddG5cqVXQ9TesnJyVaxYsVglQsAAAAAwi9geuqpp6xPnz4u6YOX/u7Xr5+NHj062OUDAAAAgPAZktejRw87ePCgNWvWzAoUSH36sWPH3N+33nqru3lpfhMAAAAARE3ANHbs2NCUBAAAAADCPWDq3r17aEoCAAAAAOEYMO3du9dKlCjh+zsr3u0AAAAAICqSPsTHx9v27dvd37pYrR6nv3mXh8K6devstttus2rVqllcXJzVqFHDhgwZYkeOHAnJ+wEAAABAtnuYvvvuOytdurT7e+bMmad8z61YscJdGPfFF1+0mjVr2u+//2533HGHHThwgMx8AAAAAHI3YGrdunWmf58qnTp1cjev6tWr28qVK+2FF14gYAIAAACQd5I+vPbaa1asWDG7+uqr0yz/4IMPXLrxU5UUYs+ePb5er0CSkpLczcs7/0q9Vbohcqg+PR4P9RqBqNvIRd1GNuo3clG3kSva6jYlm58zxwHTyJEj3dC49MqVK2d33nnnKQmYVq1aZc8999xxe5dU1qFDh2ZYvmPHDjt8+HAIS4jcOOAVROtLni9fjq/HjDyMuo1c1G1ko34jF3UbuaKtbvft25et7WI82iM5ULhwYTenqGrVqhkSM9SuXdsOHTqU7dcaOHCgjRo1Ksttli9fbrVq1fI93rRpkxsWeNFFF9nLL7+c4x6mypUr265du8jmF4FfcAXCZcuWjYoveDShbiMXdRvZqN/IRd1Grmir271797qkdQoSs4oNctzDpJ6k3377LUPAtHjxYjvttNNy9Fr33Xef9ejRI8ttNF/Ja/PmzdamTRtr2bKlTZo06bivHxsb627p6QCIhoMg2sTExFC3EYq6jVzUbWSjfiMXdRu5oqlu82XzM+Y4YLr++uutb9++Vrx4cbvwwgvdsu+//9769etn1113XY5eS9GrbtmhniUFS40bN3bzqKKhEgEAAADkrhwHTI899pgbfte2bVsrUKCAr/vu5ptvthEjRoSijC5Y0hC8KlWquHlL6ir0qlChQkjeEwAAAAByHDAVKlTIpkyZ4gInDcPThWTr16/vgplQ+eabb1yiB90qVaqUZl0Op2ABAAAAQOgCJq+zzjrL3U4FzXM63lwnAAAAAMj1gCk5Odlef/11mzFjhm3fvj1D/vLvvvsumOUDAAAAgPAJmJTcQQHTpZdeavXq1XOZNAAAAAAgEuU4YJo8ebK9//771rlz59CUCFlK8aTY0u1LbdfhXRZfON7qlqtr+WLIGAgAAADkmaQPNWvWDElhkLW5G+fa+F/G2/Idyy0pOcli88da7bK1rXfT3taycsvcLh4AAAAQcXLcNaGLzT777LNkp8uFYGnA1wMscUuilYorZVVLVXX3C7csdMu1HgAAAEAu9zDNmTPHZs6caV9++aXVrVvXChYsmGb91KlTg1k+/N8wPPUs7Ty002qWrumbN1asUDGrUbqGrd652ib8MsGaV2rO8DwAAAAgNwOmUqVK2ZVXXhnMMuA4NGdJw/ASiidkSLKhxxWKV7BlO5a57eqXr59r5QQAAAAs2gOm1157LTQlQUBK8KA5S3EF4jJdr+Xbkre57QAAAAAED+O3woCy4SnBw6FjhzJdr+Var+0AAAAAnOIepkaNGrkL1cbHx1vDhg2zvPZSYmJiEIsHUepwZcNTggfNWfLf/0q+sXXfVmuU0MhtBwAAAOAUB0z/+te/LDY21v19xRVXBPHtkR1K5KDU4cqGpwQPmrOkYXjqWVKwFB8Xb72a9iLhAwAAAJAbAdOQIUPcfXJysrVp08YaNGjgkj/g1NF1lkZ3GO27DpPmLGkYnnqWFCxxHSYAAAAgl5M+5M+f3zp06GDLly8nYMoFCoqUOlzZ8JTgQXOWNAyPniUAAAAgj2TJq1evnq1Zs8aqVasWmhIhSwqOSB0OAAAAnBo57poYPny4DRgwwKZNm2ZbtmyxvXv3prkBAAAAQNT2MHXu3Nndd+nSJUO2Nj3WPCcAAAAAiMqAaebMmaEpCQAAAACEc8CkXqSKFSvakSNH7Oyzz7YCBXIcbwEAAABA5M1hWrt2rUsnXqtWLXdfo0YNmz9/fmhLBwAAAADhEDDdf//9duzYMXv77bftww8/tEqVKtl//vOf0JYOAAAAAHJRtsfUzZkzxwVK559/vnvcvHlzFzQdOHDAihYtGsoyAgAAAEDe7mHavn27nXnmmb7HCQkJFhcX55YDAAAAQFT3MCll+P79+12Q5JUvXz7bt29fmusvlShRIvilBAAAAIC8HDApQ95ZZ52VYVnDhg19f3MdJgAAAABRGTBx/SUAAAAA0SbbAVPr1q1DWxIAAAAACNekDwAAAAAQbQiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgJPJknfVVVdZdk2dOjXb2wIAAABA2PcwlSxZ0ncrUaKEzZgxw+bPn+9bv2DBArdM6wEAAAAgqnqYXnvtNd/fDz74oF1zzTU2ceJEy58/v1uWnJxsd999twumAAAAACBq5zC9+uqrNmDAAF+wJPq7f//+bh0AAAAARG3AdOzYMVuxYkWG5VqWkpISrHIBAAAAQHgMyfN3yy232G233WarV6+2pk2bumU///yzPfHEE24dAAAAAERtwDR69GirUKGCjRkzxrZs2eKWJSQk2P3332/33XdfKMoIAAAAAOERMOXLl88eeOABd9u7d69bRrIHAAAAAJEoxwGTPwIlAAAAAJEsx0kftm3bZjfddJNVrFjRChQo4DLk+d8AAAAAIGp7mHr06GEbNmywwYMHu7lLMTExoSkZAAAAAIRbwDRnzhz74Ycf7Nxzzw1NiQAAAAAgXIfkVa5c2TweT2hKAwAAAADhHDCNHTvWBg4caOvWrQtNiQAAAAAgXIfkXXvttXbw4EGrUaOGFSlSxAoWLJhm/c6dO4NZPgAAAAAIn4BJPUwAAAAAEA1yHDB17949NCUBAAAAgEi4cG1ycrJ98skntnz5cve4bt261qVLF67DBAAAACC6A6ZVq1ZZ586dbdOmTXb22We7ZSNHjnTZ8z7//HM3twkAAAAAojJLXt++fV1QtHHjRktMTHQ3Xci2WrVqbh0AAAAARG0P0/fff2/z5s2z0qVL+5addtpp9sQTT1irVq2CXT4AAAAACJ8eptjYWNu3b1+G5fv377dChQoFq1wAAAAAEH4B02WXXWZ33nmn/fzzz+bxeNxNPU49e/Z0iR8AAAAAIGoDpnHjxrk5TC1atLDChQu7m4bi1axZ05599tnQlBIAAAAAwmEOU6lSpezTTz912fK8acVr167tAiYAAAAAsGi/DpMoQCJIAgAAABDJcjwkr2vXrjZq1KgMy5988km7+uqrg1UuAAAAAAi/gGn27NnuwrXpXXLJJW4dAAAAAERtwBQofXjBggVt7969wSoXAAAAAIRfwFS/fn2bMmVKhuWTJ0+2OnXqBKtcAAAAABB+SR8GDx5sV111la1evdouvvhit2zGjBn23nvv2QcffBCKMgIAAABAeARMl19+uX3yySc2YsQI+/DDDy0uLs4aNGhg3377rbVu3To0pQQAAACAcEkrfumll7obAAAAAESyHM9hkt27d9vLL79sDz30kO3cudMtS0xMtE2bNgW7fAAAAAAQPj1Mv/32m7Vr185Klixp69ats9tvv91Kly5tU6dOtQ0bNtibb74ZmpICAAAAQF7vYerfv7/16NHD/vzzTytcuLBvua7NxHWYAAAAAER1wPTrr7/af/7znwzLTz/9dNu6dWuwygUAAAAA4RcwxcbGZnqB2j/++MPKli0brHIBAAAAQPgFTF26dLFhw4bZ0aNH3eOYmBg3d+nBBx+0rl27hqKMAAAAABAeAdOYMWNs//79Vq5cOTt06JC79lLNmjWtePHi9vjjj4emlAAAAAAQDlnylB3vm2++sR9//NEWL17sgqdGjRq5zHkAAAAAYNF+4Vpp1aqVuwEAAACARfuQvJ9++smmTZuWZpmuuVStWjU3PO/OO++0pKSkUJQRAAAAAPJ2wKRED0uXLvU9XrJkid12221uKN7AgQPtf//7n40cOTJU5QQAAACAvBswLVq0yNq2bet7PHnyZGvWrJm99NJL7mK248aNs/fffz9U5QQAAACAvBsw7dq1y8qXL+97/P3339sll1zie3zeeefZxo0bg19CAAAAAMjrAZOCpbVr17q/jxw5YomJida8eXPf+n379lnBggVDU0oAAAAAyMsBU+fOnd1cpR9++MEGDRpkRYoUsQsuuMC3/rfffrMaNWqEqpwAAAAAkHfTij/22GN21VVXuQvVFitWzN544w0rVKiQb/2rr75qHTp0CFU5AQAAACDvBkxlypSx2bNn2549e1zAlD9//jTrP/jgA7ccAAAAAKL2wrUlS5bMdHnp0qWDUR4AAAAACL85TAAAAAAQbQiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIgYAIAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAACIlYEpKSrJzzz3XYmJibNGiRbldHAAAAAARLOwCpgceeMAqVqyY28UAAAAAEAXCKmD68ssv7euvv7bRo0fndlEAAAAARIECFia2bdtmd9xxh33yySdWpEiRbA/f081r79697j4lJcXdEDlUnx6Ph3qNQNRt5KJuIxv1G7mo28gVbXWbks3PGRYBkyquR48e1rNnT2vSpImtW7cuW88bOXKkDR06NMPyHTt22OHDh0NQUuTmAb9nzx53rOTLF1YdpzgO6jZyUbeRjfqNXNRt5Iq2ut23b1/eD5gGDhxoo0aNynKb5cuXu2F4+kCDBg3K0etr+/79+6fpYapcubKVLVvWSpQoccLlRt78gisRiOo2Gr7g0YS6jVzUbWSjfiMXdRu5oq1uCxcunPcDpvvuu8/1HGWlevXq9t1339lPP/1ksbGxadapt+nGG2+0N954I9Pnavv0zxEdANFwEEQbfcGp28hE3UYu6jayUb+Ri7qNXNFUt/my+RlzNWBS9Krb8YwbN86GDx/ue7x582br2LGjTZkyxZo1axbiUgIAAACIVmExh+mMM85I87hYsWLuvkaNGlapUqVcKhUAAACASBf5fW0AAAAAEMk9TOlVrVrVZe8AAAAAgFCihwkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIgYAIAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIgYAIAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIgYAIAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIoEGgFAAAAkJelpJgtXWq2a5dZfLxZ3bpm+egOQJARMAEAACDszJ1rNn682fLlZklJZrGxZrVrm/XubdayZW6XDpGEGBwAAABhFywNGGCWmGhWqpRZ1aqp9wsXpi7XeiBYCJgAAAAQVsPw1LO0c6dZzZpmxYqZ5c+fel+jRurwvAkTUrcDgoGACQAAAGFDc5Y0DC8hwSwmJu06Pa5QwWzZstTtgGAgYAIAAEDYUA+S5izFxWW+Xsu1XtsBwUDABAAAgLChbHhK8HDoUObrtVzrtR0QDARMAAAACBtKHa5seFu3mnk8adfpsZbXqZO6HRAMBEwAAAAIG7rOklKHqwdp9Wqz/fvNkpNT7/VYy3v14npMCB4OJQAAAIQVXWdp9Gizhg3Ndu82W7cu9b5Ro9TlXIcJwcSFawEAABB2FBQ1b56aDU8JHtSzpGF49Cwh2AiYAAAAEJYUHNWvn9ulQKQjBgcAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAiBgAgAAAIAACJgAAAAAIAACJgAAAAAIgIAJAAAAAAIgYAIAAACAAAiYAAAAACAAAiYAAAAACICACQAAAAACIGACAAAAgAAImAAAAAAgAAImAAAAAAiAgAkAAAAAAihgUcTj8bj7vXv35nZREGQpKSm2b98+K1y4sOXLx3mASELdRi7qNrJRv5GLuo1c0Va3e/8vJvDGCIFEVcCkA0AqV66c20UBAAAAkEdihJIlSwZcH+M5XkgVYVHz5s2brXjx4hYTE5PbxUGQzxAoEN64caOVKFEit4uDIKJuIxd1G9mo38hF3UauaKtbj8fjgqWKFStm2aMWVT1M2hGVKlXK7WIghPTljoYveDSibiMXdRvZqN/IRd1Grmiq25JZ9Cx5Rf7gRAAAAAA4QQRMAAAAABAAARMiQmxsrA0ZMsTdI7JQt5GLuo1s1G/kom4jF3WbuahK+gAAAAAAOUEPEwAAAAAEQMAEAAAAAAEQMAEAAABAAARMAAAAABAAARMiyrp16+y2226zatWqWVxcnNWoUcNlezly5EhuFw0naMKECVa1alUrXLiwNWvWzH755ZfcLhJO0siRI+28886z4sWLW7ly5eyKK66wlStX5naxEAJPPPGExcTE2D333JPbRUGQbNq0ybp162annXaa+3e2fv36Nn/+/NwuFk5ScnKyDR48OE376bHHHjNyw6Uq8H/3QERYsWKFpaSk2Isvvmg1a9a033//3e644w47cOCAjR49OreLhxyaMmWK9e/f3yZOnOiCpbFjx1rHjh1d41oNbYSn77//3nr16uWCpmPHjtlDDz1kHTp0sGXLllnRokVzu3gIkl9//dX9Fjdo0CC3i4Ig2bVrl7Vq1cratGljX375pZUtW9b+/PNPi4+Pz+2i4SSNGjXKXnjhBXvjjTesbt26Lgi+5ZZbrGTJkta3b1+LdqQVR8R76qmn3I/AmjVrcrsoyCEFSWpUjx8/3j1WMFy5cmXr06ePDRw4MLeLhyDZsWOHC4AVSF144YW5XRwEwf79+61Ro0b2/PPP2/Dhw+3cc891JzwQ3vS7++OPP9oPP/yQ20VBkF122WVWvnx5e+WVV3zLunbt6nqb3n77bYt2DMlDxNuzZ4+VLl06t4uBHNIwygULFli7du18y/Lly+ce//TTT7laNgT/Oyp8TyOHehAvvfTSNN9fhL/PPvvMmjRpYldffbU7ydGwYUN76aWXcrtYCIKWLVvajBkz7I8//nCPFy9ebHPmzLFLLrkkt4uWJzAkDxFt1apV9txzzzEcLwz9/fffbky1znj502MNvURkUK+h5rdomE+9evVyuzgIgsmTJ1tiYqIbkofIopEaGrGhodIaSqs61nCtQoUKWffu3XO7eDjJ3sO9e/darVq1LH/+/O7f38cff9xuvPHG3C5ankAPE8Lmi6yJw1nd0jeiNTG1U6dO7kyY5jEByJs9EZprqEY2wt/GjRutX79+9s4777hELYi8ExwaajlixAjXu3TnnXe6f181zxTh7f3333ff23fffded8NBcJp1s1j3oYUKYuO+++6xHjx5ZblO9enXf35s3b3aTUtXFPGnSpFNQQgRbmTJl3Fmubdu2pVmuxxUqVMi1ciF4evfubdOmTbPZs2dbpUqVcrs4CAINo92+fbtrVHvpTLXqWHMRk5KS3Pca4SkhIcHq1KmTZlnt2rXto48+yrUyITjuv/9+d3L6uuuuc4+V/XD9+vUuq2l3eg8JmBAelIlHt+xQz5KCpcaNG9trr73m5r0g/GiIh+pQY6qVdtp7dlOP1dBG+FKuISXu+Pjjj23WrFkujS0iQ9u2bW3JkiVplinTlob5PPjggwRLYU5DZ9NfAkBzXqpUqZJrZUJwHDx4MEN7Sd9X/bsLAiZEGAVLF110kfvxVleysm950SsRfjROXme2NMm4adOmLsuWUsSrAYbwHoanYR+ffvqpuxbT1q1b3XKlr1VGJoQv1Wf6uWhKFa9r9jBHLfzde++9buSGhuRdc8017rp4GsXBSI7wd/nll7s5S2eccYZLK75w4UJ7+umn7dZbb83touUJpBVHRHn99dcDNqY51MOThvEoNbwa1UpNPG7cOJduHOFLcw4zox7h4w29RfjRSSzSikcODaMdNGiQu/6Seod1Yot5wuFv37597sK16vnXsNqKFSva9ddfb4888ogb8RHtCJgAAAAAIAAmdwAAAABAAARMAAAAABAAARMAAAAABEDABAAAAAABEDABAAAAQAAETAAAAAAQAAETAAAAAARAwAQAAAAAARAwAciRiy66yO655x7Lq2bNmmUxMTG2e/fuoL2mXu+TTz6xYOrRo4ddccUVQX1NZM9NN91kI0aMOKXv+eijj9q5556b68cd8u7vTDA0b97cPvroo9wuBhBxCJgAZNqYV2Mg/W3VqlU2depUe+yxx07q9bPbEPR/75IlS1qrVq3su+++y/I5LVu2tC1btrjtg0Wvd8kll9ip5vF4bNKkSdasWTMrVqyYlSpVypo0aWJjx461gwcPnvLyRELjdfHixfbFF19Y3759La8L9XGnIK5WrVpWtGhRi4+Pt3bt2tnPP/+cZps//vjD/vWvf1mZMmWsRIkSdv7559vMmTNP6nXXrVtnt912m1WrVs3i4uKsRo0aNmTIEDty5EiabS688EL3GrrXY3+XXXZZrgYGofidCYaHH37YBg4caCkpKbldFCCiEDAByFSnTp1cg8D/pgZO6dKlrXjx4gGf59/oCYbXXnvNvfePP/7oGm1qKK1ZsybTbY8ePWqFChWyChUquAZ0sOj1YmNjLTd6QtSbpwarGqmLFi2ywYMH26effmpff/31KS9PJHjuuefs6quvdgFoXhfq4+6ss86y8ePH25IlS2zOnDlWtWpV69Chg+3YscO3jb5vx44dcycqFixYYOecc45btnXr1hN+3RUrVrgG/YsvvmhLly61Z555xiZOnGgPPfSQ7zXuu+8+O/30090xn5CQYAMGDPCtmzJliuXLl8+6du1quSFUvzPBoAB737599uWXX+Z2UYDI4gGAdLp37+7517/+lem61q1be/r16+d7XKVKFc+wYcM8N910k6d48eLuuUlJSZ5evXp5KlSo4ImNjfWcccYZnhEjRvi210+P96bHgWj9xx9/7Hu8adMmt2zixIm+9c8//7zn8ssv9xQpUsQzZMgQz8yZM93yXbt2uW1ee+01T8mSJT3Tp0/31KpVy1O0aFFPx44dPZs3b07zXq+88oqnTp06nkKFCrlyq/yZlWPt2rXu8Xvvvedp0aKF+3x169b1zJo1y7f9sWPHPLfeequnatWqnsKFC3vOOussz9ixY7O9j2XKlCnufT755JMM61JSUjy7d+92fycnJ3uGDh3qOf30013ZzznnHM+XX37p29ZbXr3e+eef78rTpEkTz8qVKz2//PKLp3Hjxm6fdOrUybN9+/YM5Xv00Uc9ZcqUcXX7n//8x9Wt1+HDhz19+vTxlC1b1u2HVq1audf08tbFt99+694nLi7O7bMVK1ak+Tz6jA0bNnSvUa1aNfeeR48eTbP/X3rpJc8VV1zhXqNmzZqeTz/9NM3n87+p7JlRvehYmDZtWprl+hz33Xefp2LFiu44atq0qSu7HDp0yB0Xd9xxh2/7VatWeYoVK+aOGf9jTMeIyqbP0aFDB8+GDRt8z9Gxqbrx0n5q166d57TTTvOUKFHCc+GFF3oWLFiQplyZHXcfffSR56KLLnL7oUGDBp65c+f6tl+3bp3nsssu85QqVcp9DpX7888/92TXnj17fPUlO3bscI9nz57t22bv3r1u2TfffHPCr5uZJ5980tW9V+3atX3H8RdffOE+i+h7rX3sv28DGTRokKvL9LTf9J3JST0c73fm77//9lx33XXuGFLd1KtXz/Puu+9m+O3U9+X+++/3xMfHe8qXL+9ey59e78477/SUK1fO99vyv//9z7f+hx9+8H2PK1Wq5F5v//79aV7jlltu8XTr1u24+wdA9hEwATjpgEkNjdGjR7uGpG5PPfWUp3Llyq6hpUac/pH3Nh7UKFdDQ43MLVu2pGmkHy9g2rlzp1s2btw433o1LF599VXP6tWrPevXr880YCpYsKBrFP3666+uMaTG2A033OB7XTWG1ABRUOMNJJ555pksG65qrHz44YeeZcuWeW6//XYXUKjRJEeOHPE88sgj7v3WrFnjefvtt11DS0FLdvaxdOnSxXP22Wd7jufpp592+18BnAKRBx54wH3eP/74I015FSwqaFR5mzdv7gIYNbznzJnjSUxMdI3Qnj17pimfgoJrr73W8/vvv7sgQ4HRQw895Numb9++roGoBu3SpUvdc9QQ/Oeff9x6b100a9bMBZTa5oILLvC0bNnS9xo6RlT+119/3dXh119/7QJNBU3++1/7W8fQn3/+6d5XZdP7KAhSEKFtVHc6przBZHr6nNpu69ataZar/lQmlcV7/Kqx6t2HCxcudMGoAju9n/bflVde6Xu+9xhTIKoAZv78+a6h7v850wdMM2bM8Lz11lue5cuXuzq57bbbXANaAUlWx53qUXWhz/rvf//bff+8weWll17qad++vee3335z+1IN7e+//96THQqE9bkV+ClQ8gbmOga1f9Qo1/toG33n9F080dfNzH//+193THop+FAQqxMC99xzj3ssKov/dzMrOm61z1Sn6ZfpOMpJPRzvd+avv/5yn1PHirbRb1T+/Pk9P//8c5rfTh3rOrZ1bL3xxhuemJgYd8yLPquOLQVJWuatQ32/RJ9DJzf0+fX8H3/80Z1o6NGjR5rP/cILL2R5IgpAzhEwAchADV/9Y69/nL03Nc4CBUw68+9PZz0vvvhi1+DKTiAUiP92Bw4c8Nx9992uXIsXL/atV2PKX2YBU/pG04QJE1yjyEuNfjXYslMOb8P1iSee8K1XQ1IN+lGjRgV8DfVYde3aNdsBk4I6BU3Ho7I//vjjaZadd955bl/5l/fll1/2rVdwpWVqLHqNHDkyTYCm8pUuXdrtd/+GmAIVNezUgFaQ8M477/jWK1BUedRbkL6HyUs9Hlqmnhtp27atr/fRSw3YhIQE32Nt//DDD/se6721zNsDkb7OA1Ed6vjxPy7V+NUy9V76U7nUQ+Glz6Sett69e7uyeYNj/2Ns3rx5vmVqgGuZt8GcPmBKT/tUQbd/b0Jmx51/PSoA1TK9l9SvXz9NoJkdej99v9VwV9359xDKxo0bXSCj9dpP+uwKPE/2df0peFEgMWnSJN8yBSAKAHXiRfd6rOBPQakC5auvvtr1SKXv9UxP+1w94F6qUwXwOa2H4/3OZEblVtDnpd9O9Q6l/64++OCD7u+vvvrKky9fPhcMZ0bBnHqf/OlklJ7j/T6Jel+1TJ8FQHAwhwlAptq0aePmD3hv48aNC7itEhGkTxqh55x99tlucv3JzLe5/vrr3XwTzZvSJO9XXnnFGjRoEPC9M1OkSBE3sdxLcyK2b9/u/tb95s2brW3btjkqV4sWLXx/FyhQwJVj+fLlvmUTJkywxo0bW9myZV35lbxhw4YN2X791HZa1vbu3evKrmQY/vTYvyziv8/Kly/v7uvXr59mmXefeGm+ivad/2fev3+/bdy40VavXu3mcvi/d8GCBa1p06ZZvrf2vXjfS0kYhg0b5vaR93bHHXe4eWv+iS38X0OJAJSAIH15j+fQoUNuTpD/vBPNs0lOTnbzbvzL8P3337vP6D+nxjs359VXX7XTTjstzWvrGDjvvPN8j5X0QEk60u8Lr23btrnPeeaZZ7rEAfo82rfHO0ay2pf6rg0fPtzViZIo/Pbbb9n+ns+dO9fNW7zmmmt8r6djsFevXlauXDn74Ycf7JdffnGZHS+//HJXPyf6uv42bdrk1mtemfaHl+YvTZs2ze0P3Wv+4t133+3mOukz6vdg5cqV9ueff7q5UIHceOON9u677/o+z3vvveeW5bQejvc7o2NIyXD0ndI8Tx1DX331VYbX8a+/9L9F2l+VKlVyx1lm9F15/fXX0xynHTt2dPPB1q5d69tOiTS0LCkpKcsyA8i+AjnYFkAUUaO0Zs2a2d7WX6NGjdw/4Jp4/O2337rGkjJlffjhhzkuhyaE67lqzCj4ON57Z0YNeX9qMHsDEjUugm3y5MlukvqYMWNckKHG3VNPPZUhA1lW1GjS5Phg8d8H3oAh/bJQZdbK7L2976XG6dChQ+2qq67K8LzChQtn+honWl41uhWEKTGJJu173z9//vwuoYHu/fknhlCjVhnjtI0a6Wrkn4zu3bvbP//8Y88++6xVqVLFBXI6Vo6XNCWrfXn77be7BvTnn3/uTlKMHDnSHYN9+vQ57vdcN6WkVuCgkxKDBg1yiR4UrOzatcsFEvL888/bN998Y2+88YbLxnYir+ulYF+BlTLO6YRCVpQGXokjdBJCAY6CJu0LHTcqZ6DPqBMuDz74oCUmJrqAWcH+tddem+N6ON7vjL7feg1lsFTQpO2VsCX962R1HB/vt0jH6n/+859MMzyeccYZvr937tzp3j8Uv21AtKKHCUBIqIGlhslLL73kslqpd0j/kHsbDTojmx3KRKVGV2bBUjAomFEWrxkzZuToefPmzfP9rSxianDXrl3bPVZGPzUCdUa8YcOGrvz+vRXZccMNN7gGujLipadgb8+ePW4fV6xY0b2fPz2uU6eOnSyd0VYj0/8zK4ioXLmy67FT0OH/3upx+vXXX3P03gqu1VPgbVz735QJLTu8wc/xjinvdZCWLVvmW6b60fMUEKV/fx17XrfeeqtrCCtQUAM8fc+RjoH58+f7HuszKc2595hIT/tNDd/OnTtb3bp1XUP977//tpOluunZs6dL/69eMX3/csK/Z8Lbw5e+HvQ4p8Fq+h4P9Szpmm4KgJQJM6u61r5WL5H3cgaqLx1rovus6l09Nq1bt7Z33nnH3dq3b+96zIJdD3odZbPs1q2b65mtXr26+/7mhHqf/vrrr4DP03dFx25m3xXvd0B+//13d1wDCB4CJgBB9/TTT7uhL+oh0T/+H3zwgWt8aoiSeAMUpSbW2evcpuvG6Ey8hh2q90Bno5V+Oisacvfxxx+7z6hhS/ocalSLzqar8awhOfr8SgWuQCIn1CungFNnyHV2Xa+3fv16d8ZfPW7ea+Hcf//9NmrUKBeUqpGus/4a2tOvXz87WTo7ruvlqJGmaxdpmFfv3r1d41ZnsO+66y73/tOnT3fb6My/Gtl6TnY98sgj9uabb7peJqWYVuNYPXS6nkx2qWdAZ+q1b5S6WmfiM6OgW41Opbr278nTEK2bb77ZBRnqGdXQM/XOqKfGW9c//fSTC5a0rYal6d6/90AnAdTLoV5EBc8alqqeFQ1RzIyOkbfeest9Xj1Hr3eyPQLq0dAxp8+gY1jHSKCA7cCBAy6Nt4JgHVcqs45fBTIaHifqadF1lNQLo+BZx7LqW69/6aWXphl+qO9Cdl/XGyypV2T06NGuzvRbkFmqcp0cuPPOO11Ps7eXR0MOFQhq3+nYST8kNT3tWx1T+h3yH44XzHrQ66jnTUMQ9VrqCdJwv5xQYKdrTildul7L20uv75coUNfr6zuo77h+q3RCRY/9afikeuMABA8BE4CQ9No8+eSTbty/5nXoopNqcHvPIis4UYNAZ8PzwplQNQg1lEbDjXSWWdeZUWMkK0888YS76WyyGuCfffaZG/IlaixpqJACHl10VkN+1NuUEwoAdFZdwacu8qvGlM5AK7jTmWwNvRKdHe/fv7/rTVAPiBpXKosacCdL87r0OmrE6bN06dLFvb//PlDjTteLUiCiCxurwa5GdnbpcyjQ0RAyHSsKMtQ4VhCUXZrvooBLwaLmYqVvQPrTsDX1NPhTD4cCJu1DzbtTQKQAVw16BcQKEnRs6HgV/a1eCAXCXprrpQategbVgFdPnILYQDQ8TUG29pv2n+rRv+fjRKinRcG7giQNGVQwqLJmRkML9dlUf9pO85J0nKqxre+A6HjW8aQA9OKLL3bfZx3raqTruPdSoK4ez+y+rr77OlZ00kQ9QJrH472lp6F6qlN9J710DB4+fNh9t9S7os+clX//+9+uDArmVbehqAcF+HoNHc8KBnWCKP17ZYd64vU90IkS9dQ+8MADvh40ff81t06B6wUXXOB+O3XCQb3MXgpGFVTdcsstOX5vAIHFKPNDFusBAH4U/OkCvgsXLvQN8YpE6iHRkDIFa5FEQwwVFCmY8U/ccTI0EV+9O9pfQG5S0K4A8HhzwgDkDEkfAABRQ8OtNIwrGPOFgLxGvWPqcQYQXARMAICooiFTQCTSsFIAwceQPAAAAAAIgKQPAAAAABAAARMAAAAABEDABAAAAAABEDABAAAAQAAETAAAAAAQAAETAAAAAARAwAQAAAAAARAwAQAAAIBl7v8BfpeVcydy+AEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âš–ï¸  LEGAL CONCEPT SEMANTIC SEARCH\n",
      "==================================================\n",
      "\n",
      "ðŸ” Searching for: 'constitutional rights and amendments'\n",
      "ðŸ” Finding documents similar to: 'constitutional rights and amendments...'\n",
      "ðŸ“Š Top 3 most similar documents:\n",
      "  1. Doc 0 (similarity: 0.6513)\n",
      "     Preview: @@4000241 I think it is safe to say that ours is the only dining room in West Los Angeles on whose table -- an eight-foot-long , two-hundred-pound beh...\n",
      "\n",
      "  2. Doc 5 (similarity: 0.6355)\n",
      "     Preview: @@4000941 The expression \" sent up the river \" finds its roots in New York penal history . From the early 1800s on , convicted felons from New York Ci...\n",
      "\n",
      "  3. Doc 6 (similarity: 0.6260)\n",
      "     Preview: @@4001041 ONE OF THE MOST PROMINENT causal factors of the seemingly rebounding addiction to supernaturalism as an explanation of the world and justifi...\n",
      "\n",
      "\n",
      "ðŸ” Searching for: 'judicial interpretation and precedent'\n",
      "ðŸ” Finding documents similar to: 'judicial interpretation and precedent...'\n",
      "ðŸ“Š Top 3 most similar documents:\n",
      "  1. Doc 6 (similarity: 0.6215)\n",
      "     Preview: @@4001041 ONE OF THE MOST PROMINENT causal factors of the seemingly rebounding addiction to supernaturalism as an explanation of the world and justifi...\n",
      "\n",
      "  2. Doc 2 (similarity: 0.6126)\n",
      "     Preview: @@4000441 There seems only one cause behind all forms of social misery : bigness . It appears to be the one and only problem permeating all creation ....\n",
      "\n",
      "  3. Doc 3 (similarity: 0.5920)\n",
      "     Preview: @@4000541 The papers that comprise this symposium are adapted from remarks delivered on 31 May 2002 in Washington , D.C. , at the NAS 's tenth nationa...\n",
      "\n",
      "\n",
      "ðŸ” Searching for: 'legal procedure and court process'\n",
      "ðŸ” Finding documents similar to: 'legal procedure and court process...'\n",
      "ðŸ“Š Top 3 most similar documents:\n",
      "  1. Doc 5 (similarity: 0.6509)\n",
      "     Preview: @@4000941 The expression \" sent up the river \" finds its roots in New York penal history . From the early 1800s on , convicted felons from New York Ci...\n",
      "\n",
      "  2. Doc 0 (similarity: 0.6329)\n",
      "     Preview: @@4000241 I think it is safe to say that ours is the only dining room in West Los Angeles on whose table -- an eight-foot-long , two-hundred-pound beh...\n",
      "\n",
      "  3. Doc 6 (similarity: 0.6317)\n",
      "     Preview: @@4001041 ONE OF THE MOST PROMINENT causal factors of the seemingly rebounding addiction to supernaturalism as an explanation of the world and justifi...\n",
      "\n",
      "ðŸ“Š Top 3 most similar documents:\n",
      "  1. Doc 6 (similarity: 0.6215)\n",
      "     Preview: @@4001041 ONE OF THE MOST PROMINENT causal factors of the seemingly rebounding addiction to supernaturalism as an explanation of the world and justifi...\n",
      "\n",
      "  2. Doc 2 (similarity: 0.6126)\n",
      "     Preview: @@4000441 There seems only one cause behind all forms of social misery : bigness . It appears to be the one and only problem permeating all creation ....\n",
      "\n",
      "  3. Doc 3 (similarity: 0.5920)\n",
      "     Preview: @@4000541 The papers that comprise this symposium are adapted from remarks delivered on 31 May 2002 in Washington , D.C. , at the NAS 's tenth nationa...\n",
      "\n",
      "\n",
      "ðŸ” Searching for: 'legal procedure and court process'\n",
      "ðŸ” Finding documents similar to: 'legal procedure and court process...'\n",
      "ðŸ“Š Top 3 most similar documents:\n",
      "  1. Doc 5 (similarity: 0.6509)\n",
      "     Preview: @@4000941 The expression \" sent up the river \" finds its roots in New York penal history . From the early 1800s on , convicted felons from New York Ci...\n",
      "\n",
      "  2. Doc 0 (similarity: 0.6329)\n",
      "     Preview: @@4000241 I think it is safe to say that ours is the only dining room in West Los Angeles on whose table -- an eight-foot-long , two-hundred-pound beh...\n",
      "\n",
      "  3. Doc 6 (similarity: 0.6317)\n",
      "     Preview: @@4001041 ONE OF THE MOST PROMINENT causal factors of the seemingly rebounding addiction to supernaturalism as an explanation of the world and justifi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def bert_document_clustering(bert_embeddings, n_clusters=3, visualize=True):\n",
    "    \"\"\"\n",
    "    Cluster documents based on BERT embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    - bert_embeddings: Dictionary of document embeddings\n",
    "    - n_clusters: Number of clusters to create\n",
    "    - visualize: Whether to create a visualization\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with cluster assignments and analysis\n",
    "    \"\"\"\n",
    "    print(f\"ðŸŽ¯ Clustering {len(bert_embeddings)} documents into {n_clusters} clusters using BERT embeddings...\")\n",
    "    \n",
    "    # Prepare embeddings matrix\n",
    "    doc_ids = list(bert_embeddings.keys())\n",
    "    embeddings_matrix = np.array([bert_embeddings[doc_id]['embedding'] for doc_id in doc_ids])\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings_matrix)\n",
    "    \n",
    "    # Organize results by cluster\n",
    "    clusters = {}\n",
    "    for i in range(n_clusters):\n",
    "        clusters[f'Cluster_{i}'] = []\n",
    "    \n",
    "    for doc_id, cluster_label in zip(doc_ids, cluster_labels):\n",
    "        cluster_name = f'Cluster_{cluster_label}'\n",
    "        clusters[cluster_name].append({\n",
    "            'doc_id': doc_id,\n",
    "            'text_preview': bert_embeddings[doc_id]['original_text'][:100] + \"...\"\n",
    "        })\n",
    "    \n",
    "    # Display cluster information\n",
    "    print(\"\\nðŸ“Š CLUSTER ANALYSIS RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for cluster_name, docs in clusters.items():\n",
    "        print(f\"\\nðŸ—‚ï¸  {cluster_name} ({len(docs)} documents):\")\n",
    "        for doc in docs[:3]:  # Show first 3 documents per cluster\n",
    "            print(f\"   â€¢ Doc {doc['doc_id']}: {doc['text_preview']}\")\n",
    "        if len(docs) > 3:\n",
    "            print(f\"   ... and {len(docs) - 3} more documents\")\n",
    "    \n",
    "    # Visualization\n",
    "    if visualize and len(bert_embeddings) > 2:\n",
    "        print(\"\\nðŸ“ˆ Creating visualization...\")\n",
    "        \n",
    "        # Reduce dimensionality for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings_matrix)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            cluster_mask = cluster_labels == i\n",
    "            plt.scatter(embeddings_2d[cluster_mask, 0], \n",
    "                       embeddings_2d[cluster_mask, 1], \n",
    "                       c=colors[i % len(colors)], \n",
    "                       label=f'Cluster {i}',\n",
    "                       alpha=0.7)\n",
    "        \n",
    "        plt.title('BERT-based Document Clustering (PCA Visualization)')\n",
    "        plt.xlabel(f'First Principal Component (explains {pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        plt.ylabel(f'Second Principal Component (explains {pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'clusters': clusters,\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'doc_ids': doc_ids,\n",
    "        'embeddings_matrix': embeddings_matrix,\n",
    "        'kmeans_model': kmeans\n",
    "    }\n",
    "\n",
    "def bert_semantic_search_legal_concepts(bert_embeddings, legal_concepts, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Search for documents related to specific legal concepts using BERT semantic similarity\n",
    "    \n",
    "    Parameters:\n",
    "    - bert_embeddings: Dictionary of document embeddings\n",
    "    - legal_concepts: List of legal concept queries\n",
    "    - tokenizer: BERT tokenizer\n",
    "    - model: BERT model\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping concepts to most relevant documents\n",
    "    \"\"\"\n",
    "    concept_matches = {}\n",
    "    \n",
    "    print(\"âš–ï¸  LEGAL CONCEPT SEMANTIC SEARCH\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for concept in legal_concepts:\n",
    "        print(f\"\\nðŸ” Searching for: '{concept}'\")\n",
    "        \n",
    "        # Find similar documents for this concept\n",
    "        similar_docs = find_similar_documents(\n",
    "            concept, bert_embeddings, tokenizer, model, top_k=3\n",
    "        )\n",
    "        \n",
    "        concept_matches[concept] = similar_docs\n",
    "    \n",
    "    return concept_matches\n",
    "\n",
    "# Example usage: Cluster the documents\n",
    "clustering_results = bert_document_clustering(bert_embeddings, n_clusters=3, visualize=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example usage: Semantic search for legal concepts\n",
    "legal_concepts = [\n",
    "    \"constitutional rights and amendments\",\n",
    "    \"judicial interpretation and precedent\", \n",
    "    \"legal procedure and court process\"\n",
    "]\n",
    "\n",
    "concept_search_results = bert_semantic_search_legal_concepts(\n",
    "    bert_embeddings, legal_concepts, tokenizer, model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec428d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ COMPREHENSIVE LEGAL CORPUS ANALYSIS\n",
      "============================================================\n",
      "Combining traditional got3 methods with BERT semantic analysis...\n",
      "\n",
      "ðŸ“š TRADITIONAL ANALYSIS (got3 methods):\n",
      "----------------------------------------\n",
      "\n",
      "ðŸ” Analyzing keyword: 'legal'\n",
      "ðŸ” COCA Corpus Search: 'legal'\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre:\n",
      "------------------------------\n",
      "  âœ… Found 443 occurrence(s) in acad\n",
      "\n",
      "ðŸ“š BLOG Genre:\n",
      "------------------------------\n",
      "  âœ… Found 139 occurrence(s) in blog\n",
      "\n",
      "ðŸ“š FIC Genre:\n",
      "------------------------------\n",
      "  âœ… Found 34 occurrence(s) in fic\n",
      "\n",
      "ðŸ“š MAG Genre:\n",
      "------------------------------\n",
      "  âœ… Found 160 occurrence(s) in mag\n",
      "\n",
      "ðŸ“š NEWS Genre:\n",
      "------------------------------\n",
      "  âœ… Found 122 occurrence(s) in news\n",
      "\n",
      "ðŸ“š SPOK Genre:\n",
      "------------------------------\n",
      "  âœ… Found 137 occurrence(s) in spok\n",
      "\n",
      "ðŸ“š TVM Genre:\n",
      "------------------------------\n",
      "  âœ… Found 54 occurrence(s) in tvm\n",
      "\n",
      "ðŸ“š WEB Genre:\n",
      "------------------------------\n",
      "  âœ… Found 160 occurrence(s) in mag\n",
      "\n",
      "ðŸ“š NEWS Genre:\n",
      "------------------------------\n",
      "  âœ… Found 122 occurrence(s) in news\n",
      "\n",
      "ðŸ“š SPOK Genre:\n",
      "------------------------------\n",
      "  âœ… Found 137 occurrence(s) in spok\n",
      "\n",
      "ðŸ“š TVM Genre:\n",
      "------------------------------\n",
      "  âœ… Found 54 occurrence(s) in tvm\n",
      "\n",
      "ðŸ“š WEB Genre:\n",
      "------------------------------\n",
      "  âœ… Found 124 occurrence(s) in web\n",
      "\n",
      "ðŸŽ¯ SUMMARY:\n",
      "Total hits across all genres: 1213\n",
      "Genres with matches: 8\n",
      "ðŸ“Š Frequency Analysis for 'legal'\n",
      "==================================================\n",
      "acad    :  443 occurrences |  0.312 per 1000 words\n",
      "  âœ… Found 124 occurrence(s) in web\n",
      "\n",
      "ðŸŽ¯ SUMMARY:\n",
      "Total hits across all genres: 1213\n",
      "Genres with matches: 8\n",
      "ðŸ“Š Frequency Analysis for 'legal'\n",
      "==================================================\n",
      "acad    :  443 occurrences |  0.312 per 1000 words\n",
      "blog    :  139 occurrences |  0.088 per 1000 words\n",
      "fic     :   34 occurrences |  0.024 per 1000 words\n",
      "mag     :  160 occurrences |  0.102 per 1000 words\n",
      "blog    :  139 occurrences |  0.088 per 1000 words\n",
      "fic     :   34 occurrences |  0.024 per 1000 words\n",
      "mag     :  160 occurrences |  0.102 per 1000 words\n",
      "news    :  122 occurrences |  0.088 per 1000 words\n",
      "spok    :  137 occurrences |  0.118 per 1000 words\n",
      "tvm     :   54 occurrences |  0.034 per 1000 words\n",
      "news    :  122 occurrences |  0.088 per 1000 words\n",
      "spok    :  137 occurrences |  0.118 per 1000 words\n",
      "tvm     :   54 occurrences |  0.034 per 1000 words\n",
      "web     :  124 occurrences |  0.087 per 1000 words\n",
      "ðŸ”— Collocate Analysis for 'legal' (window: Â±3 words)\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre Collocates:\n",
      "web     :  124 occurrences |  0.087 per 1000 words\n",
      "ðŸ”— Collocate Analysis for 'legal' (window: Â±3 words)\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre Collocates:\n",
      "  Found 440 instances of 'legal' in acad\n",
      "  the            : 131 times\n",
      "  and            :  89 times\n",
      "  system         :  41 times\n",
      "  that           :  35 times\n",
      "  practice       :  35 times\n",
      "  theory         :  24 times\n",
      "  for            :  20 times\n",
      "  our            :  19 times\n",
      "  have           :  16 times\n",
      "  positivism     :  16 times\n",
      "\n",
      "ðŸ“š BLOG Genre Collocates:\n",
      "  Found 440 instances of 'legal' in acad\n",
      "  the            : 131 times\n",
      "  and            :  89 times\n",
      "  system         :  41 times\n",
      "  that           :  35 times\n",
      "  practice       :  35 times\n",
      "  theory         :  24 times\n",
      "  for            :  20 times\n",
      "  our            :  19 times\n",
      "  have           :  16 times\n",
      "  positivism     :  16 times\n",
      "\n",
      "ðŸ“š BLOG Genre Collocates:\n",
      "  Found 139 instances of 'legal' in blog\n",
      "  and            :  31 times\n",
      "  the            :  31 times\n",
      "  are            :   9 times\n",
      "  system         :   8 times\n",
      "  for            :   8 times\n",
      "  you            :   7 times\n",
      "  was            :   6 times\n",
      "  from           :   6 times\n",
      "  with           :   6 times\n",
      "  about          :   6 times\n",
      "\n",
      "ðŸ“š FIC Genre Collocates:\n",
      "  Found 139 instances of 'legal' in blog\n",
      "  and            :  31 times\n",
      "  the            :  31 times\n",
      "  are            :   9 times\n",
      "  system         :   8 times\n",
      "  for            :   8 times\n",
      "  you            :   7 times\n",
      "  was            :   6 times\n",
      "  from           :   6 times\n",
      "  with           :   6 times\n",
      "  about          :   6 times\n",
      "\n",
      "ðŸ“š FIC Genre Collocates:\n",
      "  Found 34 instances of 'legal' in fic\n",
      "  the            :  19 times\n",
      "  over           :   4 times\n",
      "  and            :   4 times\n",
      "  was            :   3 times\n",
      "  age            :   2 times\n",
      "  only           :   2 times\n",
      "  exposure       :   2 times\n",
      "  community      :   2 times\n",
      "  church         :   2 times\n",
      "  guardianship   :   2 times\n",
      "\n",
      "ðŸ“š MAG Genre Collocates:\n",
      "  Found 34 instances of 'legal' in fic\n",
      "  the            :  19 times\n",
      "  over           :   4 times\n",
      "  and            :   4 times\n",
      "  was            :   3 times\n",
      "  age            :   2 times\n",
      "  only           :   2 times\n",
      "  exposure       :   2 times\n",
      "  community      :   2 times\n",
      "  church         :   2 times\n",
      "  guardianship   :   2 times\n",
      "\n",
      "ðŸ“š MAG Genre Collocates:\n",
      "  Found 157 instances of 'legal' in mag\n",
      "  the            :  63 times\n",
      "  and            :  40 times\n",
      "  that           :  12 times\n",
      "  for            :  11 times\n",
      "  system         :  10 times\n",
      "  with           :   8 times\n",
      "  has            :   6 times\n",
      "  from           :   5 times\n",
      "  all            :   5 times\n",
      "  have           :   5 times\n",
      "\n",
      "ðŸ“š NEWS Genre Collocates:\n",
      "  Found 157 instances of 'legal' in mag\n",
      "  the            :  63 times\n",
      "  and            :  40 times\n",
      "  that           :  12 times\n",
      "  for            :  11 times\n",
      "  system         :  10 times\n",
      "  with           :   8 times\n",
      "  has            :   6 times\n",
      "  from           :   5 times\n",
      "  all            :   5 times\n",
      "  have           :   5 times\n",
      "\n",
      "ðŸ“š NEWS Genre Collocates:\n",
      "  Found 120 instances of 'legal' in news\n",
      "  and            :  29 times\n",
      "  the            :  25 times\n",
      "  for            :   9 times\n",
      "  that           :   6 times\n",
      "  was            :   5 times\n",
      "  battle         :   5 times\n",
      "  with           :   5 times\n",
      "  against        :   4 times\n",
      "  will           :   4 times\n",
      "  from           :   4 times\n",
      "\n",
      "ðŸ“š SPOK Genre Collocates:\n",
      "  Found 120 instances of 'legal' in news\n",
      "  and            :  29 times\n",
      "  the            :  25 times\n",
      "  for            :   9 times\n",
      "  that           :   6 times\n",
      "  was            :   5 times\n",
      "  battle         :   5 times\n",
      "  with           :   5 times\n",
      "  against        :   4 times\n",
      "  will           :   4 times\n",
      "  from           :   4 times\n",
      "\n",
      "ðŸ“š SPOK Genre Collocates:\n",
      "  Found 137 instances of 'legal' in spok\n",
      "  the            :  38 times\n",
      "  and            :  23 times\n",
      "  that           :  15 times\n",
      "  status         :  12 times\n",
      "  they           :   9 times\n",
      "  there          :   8 times\n",
      "  for            :   7 times\n",
      "  this           :   7 times\n",
      "  affairs        :   7 times\n",
      "  not            :   6 times\n",
      "\n",
      "ðŸ“š TVM Genre Collocates:\n",
      "  Found 137 instances of 'legal' in spok\n",
      "  the            :  38 times\n",
      "  and            :  23 times\n",
      "  that           :  15 times\n",
      "  status         :  12 times\n",
      "  they           :   9 times\n",
      "  there          :   8 times\n",
      "  for            :   7 times\n",
      "  this           :   7 times\n",
      "  affairs        :   7 times\n",
      "  not            :   6 times\n",
      "\n",
      "ðŸ“š TVM Genre Collocates:\n",
      "  Found 54 instances of 'legal' in tvm\n",
      "  and            :  11 times\n",
      "  the            :   7 times\n",
      "  strategy       :   5 times\n",
      "  was            :   4 times\n",
      "  not            :   4 times\n",
      "  that           :   4 times\n",
      "  has            :   3 times\n",
      "  bring          :   3 times\n",
      "  our            :   3 times\n",
      "  totally        :   3 times\n",
      "\n",
      "ðŸ“š WEB Genre Collocates:\n",
      "  Found 54 instances of 'legal' in tvm\n",
      "  and            :  11 times\n",
      "  the            :   7 times\n",
      "  strategy       :   5 times\n",
      "  was            :   4 times\n",
      "  not            :   4 times\n",
      "  that           :   4 times\n",
      "  has            :   3 times\n",
      "  bring          :   3 times\n",
      "  our            :   3 times\n",
      "  totally        :   3 times\n",
      "\n",
      "ðŸ“š WEB Genre Collocates:\n",
      "  Found 123 instances of 'legal' in web\n",
      "  and            :  32 times\n",
      "  the            :  32 times\n",
      "  that           :  15 times\n",
      "  with           :   7 times\n",
      "  not            :   7 times\n",
      "  for            :   6 times\n",
      "  you            :   6 times\n",
      "  any            :   5 times\n",
      "  but            :   5 times\n",
      "  your           :   5 times\n",
      "\n",
      "ðŸŽ¯ TOP OVERALL COLLOCATES (min frequency: 1):\n",
      "----------------------------------------\n",
      "the            : 346 occurrences\n",
      "and            : 259 occurrences\n",
      "that           :  94 occurrences\n",
      "system         :  70 occurrences\n",
      "for            :  62 occurrences\n",
      "with           :  40 occurrences\n",
      "are            :  36 occurrences\n",
      "practice       :  36 occurrences\n",
      "our            :  36 occurrences\n",
      "their          :  35 occurrences\n",
      "have           :  32 occurrences\n",
      "not            :  31 occurrences\n",
      "from           :  31 occurrences\n",
      "they           :  29 occurrences\n",
      "status         :  27 occurrences\n",
      "but            :  26 occurrences\n",
      "was            :  26 occurrences\n",
      "there          :  25 occurrences\n",
      "this           :  24 occurrences\n",
      "theory         :  24 occurrences\n",
      "\n",
      "ðŸ” Analyzing keyword: 'constitutional'\n",
      "ðŸ” COCA Corpus Search: 'constitutional'\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre:\n",
      "------------------------------\n",
      "  âœ… Found 132 occurrence(s) in acad\n",
      "\n",
      "ðŸ“š BLOG Genre:\n",
      "------------------------------\n",
      "  âœ… Found 79 occurrence(s) in blog\n",
      "\n",
      "ðŸ“š FIC Genre:\n",
      "------------------------------\n",
      "  Found 123 instances of 'legal' in web\n",
      "  and            :  32 times\n",
      "  the            :  32 times\n",
      "  that           :  15 times\n",
      "  with           :   7 times\n",
      "  not            :   7 times\n",
      "  for            :   6 times\n",
      "  you            :   6 times\n",
      "  any            :   5 times\n",
      "  but            :   5 times\n",
      "  your           :   5 times\n",
      "\n",
      "ðŸŽ¯ TOP OVERALL COLLOCATES (min frequency: 1):\n",
      "----------------------------------------\n",
      "the            : 346 occurrences\n",
      "and            : 259 occurrences\n",
      "that           :  94 occurrences\n",
      "system         :  70 occurrences\n",
      "for            :  62 occurrences\n",
      "with           :  40 occurrences\n",
      "are            :  36 occurrences\n",
      "practice       :  36 occurrences\n",
      "our            :  36 occurrences\n",
      "their          :  35 occurrences\n",
      "have           :  32 occurrences\n",
      "not            :  31 occurrences\n",
      "from           :  31 occurrences\n",
      "they           :  29 occurrences\n",
      "status         :  27 occurrences\n",
      "but            :  26 occurrences\n",
      "was            :  26 occurrences\n",
      "there          :  25 occurrences\n",
      "this           :  24 occurrences\n",
      "theory         :  24 occurrences\n",
      "\n",
      "ðŸ” Analyzing keyword: 'constitutional'\n",
      "ðŸ” COCA Corpus Search: 'constitutional'\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre:\n",
      "------------------------------\n",
      "  âœ… Found 132 occurrence(s) in acad\n",
      "\n",
      "ðŸ“š BLOG Genre:\n",
      "------------------------------\n",
      "  âœ… Found 79 occurrence(s) in blog\n",
      "\n",
      "ðŸ“š FIC Genre:\n",
      "------------------------------\n",
      "  âœ… Found 2 occurrence(s) in fic\n",
      "\n",
      "ðŸ“š MAG Genre:\n",
      "------------------------------\n",
      "  âœ… Found 29 occurrence(s) in mag\n",
      "\n",
      "ðŸ“š NEWS Genre:\n",
      "------------------------------\n",
      "  âœ… Found 26 occurrence(s) in news\n",
      "\n",
      "ðŸ“š SPOK Genre:\n",
      "------------------------------\n",
      "  âœ… Found 37 occurrence(s) in spok\n",
      "\n",
      "ðŸ“š TVM Genre:\n",
      "------------------------------\n",
      "  âœ… Found 2 occurrence(s) in fic\n",
      "\n",
      "ðŸ“š MAG Genre:\n",
      "------------------------------\n",
      "  âœ… Found 29 occurrence(s) in mag\n",
      "\n",
      "ðŸ“š NEWS Genre:\n",
      "------------------------------\n",
      "  âœ… Found 26 occurrence(s) in news\n",
      "\n",
      "ðŸ“š SPOK Genre:\n",
      "------------------------------\n",
      "  âœ… Found 37 occurrence(s) in spok\n",
      "\n",
      "ðŸ“š TVM Genre:\n",
      "------------------------------\n",
      "  âœ… Found 1 occurrence(s) in tvm\n",
      "\n",
      "ðŸ“š WEB Genre:\n",
      "------------------------------\n",
      "  âœ… Found 39 occurrence(s) in web\n",
      "\n",
      "ðŸŽ¯ SUMMARY:\n",
      "Total hits across all genres: 345\n",
      "Genres with matches: 8\n",
      "ðŸ“Š Frequency Analysis for 'constitutional'\n",
      "==================================================\n",
      "acad    :  132 occurrences |  0.093 per 1000 words\n",
      "  âœ… Found 1 occurrence(s) in tvm\n",
      "\n",
      "ðŸ“š WEB Genre:\n",
      "------------------------------\n",
      "  âœ… Found 39 occurrence(s) in web\n",
      "\n",
      "ðŸŽ¯ SUMMARY:\n",
      "Total hits across all genres: 345\n",
      "Genres with matches: 8\n",
      "ðŸ“Š Frequency Analysis for 'constitutional'\n",
      "==================================================\n",
      "acad    :  132 occurrences |  0.093 per 1000 words\n",
      "blog    :   79 occurrences |  0.050 per 1000 words\n",
      "fic     :    2 occurrences |  0.001 per 1000 words\n",
      "blog    :   79 occurrences |  0.050 per 1000 words\n",
      "fic     :    2 occurrences |  0.001 per 1000 words\n",
      "mag     :   29 occurrences |  0.019 per 1000 words\n",
      "news    :   26 occurrences |  0.019 per 1000 words\n",
      "spok    :   37 occurrences |  0.032 per 1000 words\n",
      "mag     :   29 occurrences |  0.019 per 1000 words\n",
      "news    :   26 occurrences |  0.019 per 1000 words\n",
      "spok    :   37 occurrences |  0.032 per 1000 words\n",
      "tvm     :    1 occurrences |  0.001 per 1000 words\n",
      "web     :   39 occurrences |  0.027 per 1000 words\n",
      "ðŸ”— Collocate Analysis for 'constitutional' (window: Â±3 words)\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre Collocates:\n",
      "tvm     :    1 occurrences |  0.001 per 1000 words\n",
      "web     :   39 occurrences |  0.027 per 1000 words\n",
      "ðŸ”— Collocate Analysis for 'constitutional' (window: Â±3 words)\n",
      "============================================================\n",
      "\n",
      "ðŸ“š ACAD Genre Collocates:\n",
      "  Found 132 instances of 'constitutional' in acad\n",
      "  the            :  43 times\n",
      "  and            :  29 times\n",
      "  interpretation :  19 times\n",
      "  law            :  12 times\n",
      "  statutory      :  12 times\n",
      "  that           :  10 times\n",
      "  state          :   7 times\n",
      "  period         :   6 times\n",
      "  court          :   5 times\n",
      "  system         :   5 times\n",
      "\n",
      "ðŸ“š BLOG Genre Collocates:\n",
      "  Found 132 instances of 'constitutional' in acad\n",
      "  the            :  43 times\n",
      "  and            :  29 times\n",
      "  interpretation :  19 times\n",
      "  law            :  12 times\n",
      "  statutory      :  12 times\n",
      "  that           :  10 times\n",
      "  state          :   7 times\n",
      "  period         :   6 times\n",
      "  court          :   5 times\n",
      "  system         :   5 times\n",
      "\n",
      "ðŸ“š BLOG Genre Collocates:\n",
      "  Found 76 instances of 'constitutional' in blog\n",
      "  rights         :  43 times\n",
      "  the            :  20 times\n",
      "  our            :  10 times\n",
      "  their          :   8 times\n",
      "  and            :   8 times\n",
      "  have           :   7 times\n",
      "  for            :   7 times\n",
      "  right          :   7 times\n",
      "  not            :   6 times\n",
      "  law            :   5 times\n",
      "\n",
      "ðŸ“š FIC Genre Collocates:\n",
      "  Found 76 instances of 'constitutional' in blog\n",
      "  rights         :  43 times\n",
      "  the            :  20 times\n",
      "  our            :  10 times\n",
      "  their          :   8 times\n",
      "  and            :   8 times\n",
      "  have           :   7 times\n",
      "  for            :   7 times\n",
      "  right          :   7 times\n",
      "  not            :   6 times\n",
      "  law            :   5 times\n",
      "\n",
      "ðŸ“š FIC Genre Collocates:\n",
      "  Found 2 instances of 'constitutional' in fic\n",
      "  exercise       :   1 times\n",
      "  our            :   1 times\n",
      "  right          :   1 times\n",
      "  take           :   1 times\n",
      "  well           :   1 times\n",
      "  followed       :   1 times\n",
      "\n",
      "ðŸ“š MAG Genre Collocates:\n",
      "  Found 2 instances of 'constitutional' in fic\n",
      "  exercise       :   1 times\n",
      "  our            :   1 times\n",
      "  right          :   1 times\n",
      "  take           :   1 times\n",
      "  well           :   1 times\n",
      "  followed       :   1 times\n",
      "\n",
      "ðŸ“š MAG Genre Collocates:\n",
      "  Found 29 instances of 'constitutional' in mag\n",
      "  the            :  14 times\n",
      "  and            :   8 times\n",
      "  right          :   4 times\n",
      "  monarch        :   4 times\n",
      "  was            :   3 times\n",
      "  normal         :   3 times\n",
      "  its            :   3 times\n",
      "  law            :   2 times\n",
      "  system         :   2 times\n",
      "  free           :   2 times\n",
      "\n",
      "ðŸ“š NEWS Genre Collocates:\n",
      "  Found 29 instances of 'constitutional' in mag\n",
      "  the            :  14 times\n",
      "  and            :   8 times\n",
      "  right          :   4 times\n",
      "  monarch        :   4 times\n",
      "  was            :   3 times\n",
      "  normal         :   3 times\n",
      "  its            :   3 times\n",
      "  law            :   2 times\n",
      "  system         :   2 times\n",
      "  free           :   2 times\n",
      "\n",
      "ðŸ“š NEWS Genre Collocates:\n",
      "  Found 26 instances of 'constitutional' in news\n",
      "  the            :   7 times\n",
      "  right          :   6 times\n",
      "  that           :   6 times\n",
      "  for            :   5 times\n",
      "  and            :   2 times\n",
      "  our            :   2 times\n",
      "  iraq           :   2 times\n",
      "  there          :   2 times\n",
      "  amendment      :   2 times\n",
      "  convention     :   2 times\n",
      "\n",
      "ðŸ“š SPOK Genre Collocates:\n",
      "  Found 26 instances of 'constitutional' in news\n",
      "  the            :   7 times\n",
      "  right          :   6 times\n",
      "  that           :   6 times\n",
      "  for            :   5 times\n",
      "  and            :   2 times\n",
      "  our            :   2 times\n",
      "  iraq           :   2 times\n",
      "  there          :   2 times\n",
      "  amendment      :   2 times\n",
      "  convention     :   2 times\n",
      "\n",
      "ðŸ“š SPOK Genre Collocates:\n",
      "  Found 37 instances of 'constitutional' in spok\n",
      "  the            :   9 times\n",
      "  law            :   8 times\n",
      "  amendment      :   6 times\n",
      "  and            :   5 times\n",
      "  that           :   5 times\n",
      "  you            :   3 times\n",
      "  but            :   2 times\n",
      "  rights         :   2 times\n",
      "  need           :   2 times\n",
      "  there          :   2 times\n",
      "\n",
      "ðŸ“š TVM Genre Collocates:\n",
      "  Found 37 instances of 'constitutional' in spok\n",
      "  the            :   9 times\n",
      "  law            :   8 times\n",
      "  amendment      :   6 times\n",
      "  and            :   5 times\n",
      "  that           :   5 times\n",
      "  you            :   3 times\n",
      "  but            :   2 times\n",
      "  rights         :   2 times\n",
      "  need           :   2 times\n",
      "  there          :   2 times\n",
      "\n",
      "ðŸ“š TVM Genre Collocates:\n",
      "  Found 1 instances of 'constitutional' in tvm\n",
      "  the            :   2 times\n",
      "  responsibilities:   1 times\n",
      "\n",
      "ðŸ“š WEB Genre Collocates:\n",
      "  Found 1 instances of 'constitutional' in tvm\n",
      "  the            :   2 times\n",
      "  responsibilities:   1 times\n",
      "\n",
      "ðŸ“š WEB Genre Collocates:\n",
      "  Found 39 instances of 'constitutional' in web\n",
      "  the            :  11 times\n",
      "  rights         :   8 times\n",
      "  and            :   8 times\n",
      "  for            :   5 times\n",
      "  amendment      :   4 times\n",
      "  our            :   3 times\n",
      "  right          :   3 times\n",
      "  center         :   2 times\n",
      "  was            :   2 times\n",
      "  with           :   2 times\n",
      "\n",
      "ðŸŽ¯ TOP OVERALL COLLOCATES (min frequency: 1):\n",
      "----------------------------------------\n",
      "the            : 106 occurrences\n",
      "and            :  60 occurrences\n",
      "rights         :  57 occurrences\n",
      "law            :  28 occurrences\n",
      "that           :  25 occurrences\n",
      "right          :  25 occurrences\n",
      "for            :  22 occurrences\n",
      "our            :  21 occurrences\n",
      "interpretation :  20 occurrences\n",
      "amendment      :  14 occurrences\n",
      "statutory      :  12 occurrences\n",
      "not            :  11 occurrences\n",
      "have           :  10 occurrences\n",
      "their          :  10 occurrences\n",
      "court          :   9 occurrences\n",
      "system         :   9 occurrences\n",
      "this           :   8 occurrences\n",
      "but            :   8 occurrences\n",
      "state          :   8 occurrences\n",
      "with           :   7 occurrences\n",
      "\n",
      "ðŸ§  BERT SEMANTIC ANALYSIS:\n",
      "----------------------------------------\n",
      "ðŸ¤— Tokenizing corpus with BERT (max_length=256)...\n",
      "  Found 39 instances of 'constitutional' in web\n",
      "  the            :  11 times\n",
      "  rights         :   8 times\n",
      "  and            :   8 times\n",
      "  for            :   5 times\n",
      "  amendment      :   4 times\n",
      "  our            :   3 times\n",
      "  right          :   3 times\n",
      "  center         :   2 times\n",
      "  was            :   2 times\n",
      "  with           :   2 times\n",
      "\n",
      "ðŸŽ¯ TOP OVERALL COLLOCATES (min frequency: 1):\n",
      "----------------------------------------\n",
      "the            : 106 occurrences\n",
      "and            :  60 occurrences\n",
      "rights         :  57 occurrences\n",
      "law            :  28 occurrences\n",
      "that           :  25 occurrences\n",
      "right          :  25 occurrences\n",
      "for            :  22 occurrences\n",
      "our            :  21 occurrences\n",
      "interpretation :  20 occurrences\n",
      "amendment      :  14 occurrences\n",
      "statutory      :  12 occurrences\n",
      "not            :  11 occurrences\n",
      "have           :  10 occurrences\n",
      "their          :  10 occurrences\n",
      "court          :   9 occurrences\n",
      "system         :   9 occurrences\n",
      "this           :   8 occurrences\n",
      "but            :   8 occurrences\n",
      "state          :   8 occurrences\n",
      "with           :   7 occurrences\n",
      "\n",
      "ðŸ§  BERT SEMANTIC ANALYSIS:\n",
      "----------------------------------------\n",
      "ðŸ¤— Tokenizing corpus with BERT (max_length=256)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/rt/t4n5gxfx6tsbgty850mf96z40000gn/T/ipykernel_87059/2354231295.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[33m'constitutional interpretation and judicial review'\u001b[39m,\n\u001b[32m    106\u001b[39m     \u001b[33m'legal precedent and case law'\u001b[39m\n\u001b[32m    107\u001b[39m ]\n\u001b[32m    108\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m comprehensive_results = create_bert_enhanced_corpus_analysis(\n\u001b[32m    110\u001b[39m     db_text,\n\u001b[32m    111\u001b[39m     keywords_to_analyze,\n\u001b[32m    112\u001b[39m     legal_concepts_to_search\n",
      "\u001b[32m/var/folders/rt/t4n5gxfx6tsbgty850mf96z40000gn/T/ipykernel_87059/2354231295.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(corpus_data, keywords, legal_concepts)\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Use academic corpus for BERT analysis (sample)\u001b[39;00m\n\u001b[32m     52\u001b[39m     sample_texts = corpus_data\n\u001b[32m     53\u001b[39m \n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Tokenize and generate embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     bert_tokenized = bert_tokenize_corpus(sample_texts, max_length=\u001b[32m256\u001b[39m)\n\u001b[32m     56\u001b[39m     bert_embeds = generate_bert_embeddings(bert_tokenized, batch_size=\u001b[32m4\u001b[39m)\n\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# Semantic search for legal concepts\u001b[39;00m\n",
      "\u001b[32m/var/folders/rt/t4n5gxfx6tsbgty850mf96z40000gn/T/ipykernel_87059/3385800426.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(corpus_data, max_length)\u001b[39m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m     print(\u001b[33mf\"ðŸ¤— Tokenizing corpus with BERT (max_length={max_length})...\"\u001b[39m)\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc_id, text \u001b[38;5;28;01min\u001b[39;00m items:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pd.isna(text) \u001b[38;5;28;01mor\u001b[39;00m str(text).strip() == \u001b[33m''\u001b[39m:\n\u001b[32m     28\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m         text_str = str(text)\n",
      "\u001b[32m~/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1575\u001b[39m     @final\n\u001b[32m   1576\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __nonzero__(self) -> NoReturn:\n\u001b[32m-> \u001b[39m\u001b[32m1577\u001b[39m         raise ValueError(\n\u001b[32m   1578\u001b[39m             \u001b[33mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[39m\n\u001b[32m   1579\u001b[39m             \u001b[33m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[39m\n\u001b[32m   1580\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "## Integration with got3 toolkit for enhanced legal corpus analysis\n",
    "\n",
    "def create_bert_enhanced_corpus_analysis(corpus_data, keywords, legal_concepts):\n",
    "    \"\"\"\n",
    "    Combine traditional got3 analysis with BERT-based semantic analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - corpus_data: COCA corpus data from got3.read_corpora()\n",
    "    - keywords: List of keywords for traditional analysis\n",
    "    - legal_concepts: List of legal concepts for BERT semantic analysis\n",
    "    \n",
    "    Returns:\n",
    "    - Comprehensive analysis results combining both approaches\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'traditional_analysis': {},\n",
    "        'bert_analysis': {},\n",
    "        'combined_insights': {}\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸŽ¯ COMPREHENSIVE LEGAL CORPUS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Combining traditional got3 methods with BERT semantic analysis...\")\n",
    "    \n",
    "    # Traditional got3 analysis\n",
    "    print(\"\\nðŸ“š TRADITIONAL ANALYSIS (got3 methods):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        print(f\"\\nðŸ” Analyzing keyword: '{keyword}'\")\n",
    "        \n",
    "        # Traditional keyword search\n",
    "        search_results = search_keyword_corpus(keyword, corpus_data, show_context=False)\n",
    "        \n",
    "        # Frequency analysis  \n",
    "        freq_results = keyword_frequency_analysis(keyword, corpus_data)\n",
    "        \n",
    "        # Collocate analysis\n",
    "        collocate_results = find_collocates(keyword, corpus_data, window_size=3, min_freq=1)\n",
    "        \n",
    "        results['traditional_analysis'][keyword] = {\n",
    "            'search_results': search_results,\n",
    "            'frequency_results': freq_results,\n",
    "            'collocate_results': collocate_results\n",
    "        }\n",
    "    \n",
    "    # BERT-based analysis on a sample\n",
    "    print(f\"\\nðŸ§  BERT SEMANTIC ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Use academic corpus for BERT analysis (sample)\n",
    "    sample_texts = corpus_data\n",
    "    \n",
    "    # Tokenize and generate embeddings\n",
    "    bert_tokenized = bert_tokenize_corpus(sample_texts, max_length=256)\n",
    "    bert_embeds = generate_bert_embeddings(bert_tokenized, batch_size=4)\n",
    "    \n",
    "    # Semantic search for legal concepts\n",
    "    for concept in legal_concepts:\n",
    "        print(f\"\\nâš–ï¸  Semantic search for: '{concept}'\")\n",
    "        similar_docs = find_similar_documents(concept, bert_embeds, tokenizer, model, top_k=3)\n",
    "        results['bert_analysis'][concept] = similar_docs\n",
    "    \n",
    "    # Document clustering\n",
    "    print(f\"\\nðŸ—‚ï¸  Document clustering:\")\n",
    "    clustering = bert_document_clustering(bert_embeds, n_clusters=3, visualize=False)\n",
    "    results['bert_analysis']['clustering'] = clustering\n",
    "    \n",
    "    # Combined insights\n",
    "    print(f\"\\nðŸ’¡ COMBINED INSIGHTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Compare traditional keyword frequency with BERT semantic clusters\n",
    "    for keyword in keywords:\n",
    "        freq_data = results['traditional_analysis'][keyword]['frequency_results']\n",
    "        total_occurrences = sum(genre_data['count'] for genre_data in freq_data.values())\n",
    "        \n",
    "        if total_occurrences > 0:\n",
    "            insights.append(f\"'{keyword}' appears {total_occurrences} times across all genres\")\n",
    "        \n",
    "        # Top collocates\n",
    "        collocates = results['traditional_analysis'][keyword]['collocate_results']['all_collocates']\n",
    "        top_collocates = sorted(collocates.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        if top_collocates:\n",
    "            collocate_words = [word for word, count in top_collocates]\n",
    "            insights.append(f\"'{keyword}' most frequently appears with: {', '.join(collocate_words)}\")\n",
    "    \n",
    "    # BERT clustering insights\n",
    "    cluster_info = results['bert_analysis']['clustering']['clusters']\n",
    "    insights.append(f\"BERT analysis identified {len(cluster_info)} semantic clusters in the corpus\")\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(f\"  â€¢ {insight}\")\n",
    "    \n",
    "    results['combined_insights'] = insights\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Analysis complete! Traditional and BERT methods provide complementary insights.\")\n",
    "    return results\n",
    "\n",
    "# Example: Comprehensive analysis combining got3 and BERT\n",
    "keywords_to_analyze = ['legal', 'constitutional']\n",
    "legal_concepts_to_search = [\n",
    "    'constitutional interpretation and judicial review',\n",
    "    'legal precedent and case law'\n",
    "]\n",
    "\n",
    "comprehensive_results = create_bert_enhanced_corpus_analysis(\n",
    "    db_text, \n",
    "    keywords_to_analyze, \n",
    "    legal_concepts_to_search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b30bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
