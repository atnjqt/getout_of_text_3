{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c949ee11",
   "metadata": {},
   "source": [
    "## Demo for ds hugging fact openllm-france wikimedia collection\n",
    "\n",
    "- You can stream or download the collection locally. \n",
    "  - See here for the steps with `datasets`: https://huggingface.co/datasets/OpenLLM-France/wikimedia?library=datasets\n",
    "  - Find the full list of languages here: https://huggingface.co/datasets/OpenLLM-France/wikimedia\n",
    "  ```text\n",
    "    language\t# pages\t# words\t# characters\n",
    "    -----------------------------------------\n",
    "    en (English)\t16.46 M\t    6.93 B\t39.97 B\n",
    "    fr (French)\t9.66 M\t    3.07 B\t18.00 B\n",
    "    de (German)\t4.56 M\t    2.21 B\t14.83 B\n",
    "    es (Spanish)\t3.06 M\t    1.56 B\t9.07 B\n",
    "    it (Italian)\t2.75 M\t    1.48 B\t8.86 B\n",
    "    nl (Dutch)\t3.16 M\t    734.36 M\t4.40 B\n",
    "    pt (Portug.)\t1.76 M\t    710.99 M\t4.06 B\n",
    "    ca (Catalan)\t1.44 M\t    564.51 M\t3.33 B\n",
    "    ar (Arabic)\t1.46 M\t    562.65 M\t3.22 B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4567302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a1bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 496/496 [16:28<00:00,  1.99s/files]\n",
      "Generating train split: 100%|██████████| 16458534/16458534 [00:54<00:00, 301123.97 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"OpenLLM-France/wikimedia\", \"en\")\n",
    "#ds = load_dataset(\"OpenLLM-France/wikimedia\", \"de\")\n",
    "#ds = load_dataset(\"OpenLLM-France/wikimedia\", \"it\")\n",
    "#df = load_dataset(\"OpenLLM-France/wikimedia\", \"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1cc58a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (85/85 shards): 100%|██████████| 16458534/16458534 [02:05<00:00, 131314.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# export ds to json locally so I don't have to redownload\n",
    "ds.save_to_disk('wikimedia_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c32d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 190/190 [07:12<00:00,  2.28s/files]\n",
      "Generating train split: 100%|██████████| 9658605/9658605 [00:27<00:00, 353797.93 examples/s] \n",
      "Saving the dataset (40/40 shards): 100%|██████████| 9658605/9658605 [00:55<00:00, 174108.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"OpenLLM-France/wikimedia\", \"fr\")\n",
    "ds.save_to_disk('wikimedia_fr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b15a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 89/89 [03:23<00:00,  2.29s/files]\n",
      "Generating train split: 100%|██████████| 3057330/3057330 [00:13<00:00, 224644.76 examples/s]\n",
      "Saving the dataset (20/20 shards): 100%|██████████| 3057330/3057330 [00:06<00:00, 506896.48 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"OpenLLM-France/wikimedia\", \"es\")\n",
    "ds.save_to_disk('wikimedia_es')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75aa19",
   "metadata": {},
   "source": [
    "### Searching from local datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761714c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 85/85 [00:00<00:00, 154002.52files/s]\n",
      "Generating train split: 7631570 examples [00:27, 156326.57 examples/s]"
     ]
    }
   ],
   "source": [
    "ds_en = load_dataset(\"./wikimedia_en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fr = load_dataset(\"./wikimedia_fr\")\n",
    "ds_es = load_dataset(\"./wikimedia_es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f990b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping language codes to the word for 'vehicle' in each language\n",
    "vehicle_dict = {\n",
    "    \"en\": \"bank\",\n",
    "    \"fr\": \"banque\",\n",
    "    \"es\": \"banco\"\n",
    "}\n",
    "\n",
    "# Prepare a results dictionary grouped by language code\n",
    "results_by_lang = {lang: [] for lang in vehicle_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc381b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 Processing EN language - searching for 'bank'\n",
      " ✅ Total documents loaded: 50000\n",
      "📊 Processing 50000 documents for en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing en: 100%|██████████| 50000/50000 [00:00<00:00, 84307.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed en: 2194 DataFrames stored (filtered for 'bank')\n",
      "🌍 Processing FR language - searching for 'banque'\n",
      " ✅ Total documents loaded: 50000\n",
      "📊 Processing 50000 documents for fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fr: 100%|██████████| 50000/50000 [00:00<00:00, 112360.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed fr: 649 DataFrames stored (filtered for 'banque')\n",
      "🌍 Processing ES language - searching for 'banco'\n",
      " ✅ Total documents loaded: 50000\n",
      "📊 Processing 50000 documents for es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing es: 100%|██████████| 50000/50000 [00:00<00:00, 102085.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed es: 793 DataFrames stored (filtered for 'banco')\n",
      "🎉 All languages processed! Results structure: ['en: 2194 documents', 'fr: 649 documents', 'es: 793 documents']\n",
      "Number of languages: 3\n",
      "number of EN hits for bank: 2194\n",
      "number of FR hits for banque: 649\n",
      "number of ES hits for banco: 793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize results dictionary with language as top-level key\n",
    "results = {lang: [] for lang in vehicle_dict.keys()}\n",
    "\n",
    "# Loop through each language and its corresponding vehicle term\n",
    "for lang_code, vehicle_term in vehicle_dict.items():\n",
    "    print(f\"🌍 Processing {lang_code.upper()} language - searching for '{vehicle_term}'\")\n",
    "    \n",
    "    # Load dataset for the current language\n",
    "    ds = datasets.load_dataset(\"OpenLLM-France/wikimedia\", lang_code,\n",
    "        streaming=True,\n",
    "        split='train'\n",
    "    )\n",
    "    \n",
    "    # Use islice to create a finite iterable\n",
    "    TAKE_SIZE = 50000  # or any number you want\n",
    "    limited_ds = list(islice(ds, TAKE_SIZE))\n",
    "    \n",
    "    print(' ✅ Total documents loaded:', len(limited_ds))\n",
    "    print(f\"📊 Processing {len(limited_ds)} documents for {lang_code}\")\n",
    "    \n",
    "    \n",
    "    # Process each document and create DataFrames, filtering actual occurrences\n",
    "    lang_results = []\n",
    "    for data in tqdm(limited_ds, desc=f\"Processing {lang_code}\"):\n",
    "        text_content = data.get('text', '')\n",
    "        # Skip documents that do not contain the vehicle term\n",
    "        if vehicle_term.lower() not in text_content.lower():\n",
    "            continue\n",
    "        item_id = data['id']\n",
    "        \n",
    "        # Remove the 'id' key for the DataFrame\n",
    "        data_clean = {k: v for k, v in data.items() if k != 'id'}\n",
    "        \n",
    "        # Convert to DataFrame (single row)\n",
    "        df = pd.DataFrame([data_clean])\n",
    "        lang_results.append(df)\n",
    "    \n",
    "    # Store all DataFrames for this language\n",
    "    results[lang_code] = lang_results\n",
    "    print(f\"✅ Completed {lang_code}: {len(lang_results)} DataFrames stored (filtered for '{vehicle_term}')\")\n",
    "\n",
    "print(f\"🎉 All languages processed! Results structure: {[f'{k}: {len(v)} documents' for k, v in results.items()]}\")\n",
    "print('Number of languages:', len(results.keys()))\n",
    "\n",
    "for lang, docs in results.items():\n",
    "    print(f'number of {lang.upper()} hits for {vehicle_dict[lang.lower()]}:', len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e982ee23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
