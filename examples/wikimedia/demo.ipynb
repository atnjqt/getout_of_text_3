{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c949ee11",
   "metadata": {},
   "source": [
    "## Demo for ds hugging fact openllm-france wikimedia collection\n",
    "\n",
    "- You can stream or download the collection locally. \n",
    "  - See here for the steps with `datasets`: https://huggingface.co/datasets/OpenLLM-France/wikimedia?library=datasets\n",
    "  - Find the full list of languages here: https://huggingface.co/datasets/OpenLLM-France/wikimedia\n",
    "  ```text\n",
    "    language\t# pages\t# words\t# characters\n",
    "    -----------------------------------------\n",
    "    en (English)\t16.46 M\t    6.93 B\t39.97 B\n",
    "    fr (French)\t9.66 M\t    3.07 B\t18.00 B\n",
    "    de (German)\t4.56 M\t    2.21 B\t14.83 B\n",
    "    es (Spanish)\t3.06 M\t    1.56 B\t9.07 B\n",
    "    it (Italian)\t2.75 M\t    1.48 B\t8.86 B\n",
    "    nl (Dutch)\t3.16 M\t    734.36 M\t4.40 B\n",
    "    pt (Portug.)\t1.76 M\t    710.99 M\t4.06 B\n",
    "    ca (Catalan)\t1.44 M\t    564.51 M\t3.33 B\n",
    "    ar (Arabic)\t1.46 M\t    562.65 M\t3.22 B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4567302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 496/496 [16:28<00:00,  1.99s/files]\n",
      "Generating train split: 100%|██████████| 16458534/16458534 [00:54<00:00, 301123.97 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"OpenLLM-France/wikimedia\", \"en\")\n",
    "# export ds to json locally so I don't have to redownload\n",
    "ds.save_to_disk('wikimedia_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c32d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 190/190 [07:12<00:00,  2.28s/files]\n",
      "Generating train split: 100%|██████████| 9658605/9658605 [00:27<00:00, 353797.93 examples/s] \n",
      "Saving the dataset (40/40 shards): 100%|██████████| 9658605/9658605 [00:55<00:00, 174108.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"OpenLLM-France/wikimedia\", \"fr\")\n",
    "ds.save_to_disk('wikimedia_fr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b15a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 89/89 [03:23<00:00,  2.29s/files]\n",
      "Generating train split: 100%|██████████| 3057330/3057330 [00:13<00:00, 224644.76 examples/s]\n",
      "Saving the dataset (20/20 shards): 100%|██████████| 3057330/3057330 [00:06<00:00, 506896.48 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"OpenLLM-France/wikimedia\", \"es\")\n",
    "ds.save_to_disk('wikimedia_es')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75aa19",
   "metadata": {},
   "source": [
    "### Searching from local datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "761714c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_en = load_dataset(\"./wikimedia_en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fdc83c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 40/40 [00:00<00:00, 43007.48files/s]\n",
      "Generating train split: 9658605 examples [00:16, 595647.84 examples/s] \n",
      "Downloading data: 100%|██████████| 20/20 [00:00<00:00, 158875.15files/s]\n",
      "Generating train split: 3057330 examples [00:07, 386305.80 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds_fr = load_dataset(\"./wikimedia_fr\")\n",
    "ds_es = load_dataset(\"./wikimedia_es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb2b22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping language codes to the word for 'vehicle' in each language\n",
    "vehicle_dict = {\n",
    "    \"en\": \"bank\",\n",
    "    \"fr\": \"banque\",\n",
    "    \"es\": \"banco\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691df872",
   "metadata": {},
   "source": [
    "### TOOLS FOR FILTERING A LOCAL WIKIMEDIA DUMP\n",
    "\n",
    "- data dir is `./wikimedia_en` , `./wikimedia_fr`, and `./wikimedia_es`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d08ca160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-friendly streaming search over a saved Hugging Face \"datasets\" dataset\n",
    "# This cell is robust: it will prefer an already-loaded `ds_en`/`ds` variable in the kernel,\n",
    "# otherwise it will try a list of likely on-disk locations. Adjust parameters below and re-run.\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datasets import load_from_disk\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Parameters - edit as needed\n",
    "DATA_DIRS = [\n",
    "    \"./wikimedia_en\",\n",
    "    \"./wikimedia_fr\",\n",
    "    \"./wikimedia_es\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "32ea1158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16458534"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_en['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e7062753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3055983,\n",
       " 'title': 'Wigan Casino',\n",
       " 'url': 'https://en.wikipedia.org/wiki/Wigan_Casino',\n",
       " 'language': 'en',\n",
       " 'source': 'wikipedia',\n",
       " 'text': '# Wigan Casino\\n\\nThe Wigan Casino is the colloquial name for the nightclub the Casino Club, that operated in Wigan between Friday, August 27 1965 (with Shirley Bassey topping the bill) and 1981, associated with the Northern Soul movement in the UK. The club\\'s enduring dedication to Northern Soul \"all nighters\" made it an icon among fans of the genre, continuing the efforts that other clubs such as the Twisted Wheel in Manchester, the Chateau Impney (Droitwich), the Catacombs (Wolverhampton) and the Golden Torch (Tunstall, Stoke-on-Trent) had started. It remains one of the most famous clubs in Northern England. In 1978, allegedly the American music magazine Billboard voted Wigan Casino \"The Best Disco in the World\", ahead of New York City\\'s Studio 54, although there is no tangible evidence of this award ever being publicised.\\nThis England, a TV documentary about the Wigan Casino, was filmed in 1977. Russ Winstanley and Dave Nowell wrote a history of the club, Soul Survivors, The Wigan Casino Story, which was published in 1996. A stage play by Mick Martin about the Wigan Casino years, Once upon a time in Wigan, debuted in February 2003 at the Contact Theatre in Manchester and has since toured nationally.\\n\\n## History\\n\\n\\'The Casino Club\\' (known colloquially as Wigan Casino) was the name of the last incarnation of a Wigan ballroom called the Empress. Local DJs Brian Rigby and Alan Cain approached lease owner Gerry Marshall to run all-nighters. Venue manager Mike Walker brought in Russ Winstanley, who had a DJ set at the local rugby club, to the Casino. At 2\\xa0am on Sunday 23 September 1973, Wigan Casino started its first-ever Northern soul all-nighter, with Winstanley as the DJ. After Winstanley and his helper Ian Fishwick, Kev Roberts was the third DJ at Casino all-nighters, who was quickly joined by Richard Searling Soul performers that performed there include Jackie Wilson, Edwin Starr and Junior Walker.\\nYoung people from all over the UK regularly attended Wigan Casino to hear the latest northern soul artists and to dance. There were long queues to get in. The second dance floor, Mr M\\'s, stayed open until 6\\xa0am and played oldies songs from a variety of DJs including Dave Evison and Steve Whittle. All-nighters generally ended with three songs that became known as the \\'3 before 8\\': \"Time Will Pass You By\" by Tobi Legend, \"Long After Tonight Is All Over\" by Jimmy Radcliffe, and \"I\\'m on My Way\" by Dean Parrish. Parrish (born Phil Anastasi) remained active on the Northern soul circuit until his death in 2021.\\nWigan Casino\\'s 500th all-nighter was held on Saturday 16 May 1981, from midnight to 8\\xa0am. Over the eight years it was open, it reputedly had over one million people through its doors.\\nWigan Council owned the building and wanted to extend the nearby Civic Centre, but short of funding, it never went ahead. The club closed on 6 December 1981; that final night of Wigan Casino in its Northern soul state was DJ\\'d by Winstanley, and the \\'3 before 8\\' were played three times consecutively at the end of the night. The crowd refused to leave; according to Winstanley, to \"break this spell of hysteria\", he picked a 7\" at random from his box and played that. This final Wigan Casino song became one of the most famous Northern soul songs of all time, Frank Wilson\\'s \"Do I Love You (Indeed I Do)\". Annual reunions are held in Wigan and Blackpool hosted by various original DJs.\\nThe Casino is commemorated with a Blue plaque, which was installed in 2014, marking the place where the doors to the club once stood.\\nThe site is now occupied by the Grand Arcade shopping centre, which pays homage to the club with its Casino Café.'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's read a sample row from the ds_en to get the keys\n",
    "import random\n",
    "\n",
    "ds_en['train'][random.randint(0, len(ds_en['train']) - 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d057a4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 71380719,\n",
       " 'title': '2022 Bahamas boat capsizing',\n",
       " 'url': 'https://en.wikipedia.org/wiki/2022_Bahamas_boat_capsizing',\n",
       " 'language': 'en',\n",
       " 'source': 'wikipedia',\n",
       " 'text': '# 2022 Bahamas boat capsizing\\n\\nOn 24 July 2022, at least 17 people died while on a boat near the Bahamas. The boat was reportedly heading towards Florida, when the boat capsized seven miles off of the coast of New Providence. An additional three people were hospitalized in the capsizing.\\n\\n## Background\\n\\nThe Bahamas is a common transit route for Haitians attempting to reach the United States, although extreme conditions and rickety vessels in the Bahamas make traversing through the Bahamas dangerous.\\n\\n## Incident\\n\\nThe incident occurred at roughly 1 a.m. EDT, according to Bahamian prime minister Philip Davis. Authorities claim that the boat was carrying between 50 and 60 people, and that passengers paid $3,000 to $8,000 to board the boat.\\n\\n## Victims\\n\\nAt least 17 people died in the capsizing. Of the 17 people, 16 of them were female, including a child believed to be between the ages of 4 and 5. Authorities were able to recover 25 people from the boat, although an additional eight to 12 people are believed to be missing.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_en['train'][random.randint(0, len(ds_en['train']) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a87220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field candidates to search for text content (in priority order)\n",
    "FIELD_CANDIDATES = ['text', 'title', 'content']\n",
    "\n",
    "def _extract_text_from_row(row):\n",
    "    for f in FIELD_CANDIDATES:\n",
    "        if f in row and row[f]:\n",
    "            return row[f]\n",
    "    parts = []\n",
    "    for k, v in row.items():\n",
    "        if isinstance(v, str) and v:\n",
    "            parts.append(v)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def search_saved_dataset(dataset,\n",
    "                         keyword,\n",
    "                         batch_size,\n",
    "                         max_results,\n",
    "                         context_window,\n",
    "                         case_insensitive):\n",
    "    \n",
    "    # Resolve to a Dataset (select split 'train' if present)\n",
    "    if isinstance(dataset, dict) or hasattr(dataset, 'keys'):\n",
    "        split = 'train' if 'train' in dataset else list(dataset.keys())[0]\n",
    "        dataset = dataset[split]\n",
    "\n",
    "    flag = re.IGNORECASE if case_insensitive else 0\n",
    "    pattern = re.compile(re.escape(keyword), flag)\n",
    "\n",
    "    found = 0\n",
    "    seen = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate total number of batches for progress bar\n",
    "    total_rows = len(dataset)\n",
    "    total_batches = (total_rows + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=total_batches, desc=f\"Searching for '{keyword}'\", unit=\"batch\")\n",
    "\n",
    "    for batch in dataset.iter(batch_size=batch_size):\n",
    "        # batch is a dict of lists\n",
    "        try:\n",
    "            batch_len = len(next(iter(batch.values())))\n",
    "        except StopIteration:\n",
    "            batch_len = 0\n",
    "        for i in range(batch_len):\n",
    "            row = {k: (v[i] if isinstance(v, (list, tuple)) else v) for k, v in batch.items()}\n",
    "            text = _extract_text_from_row(row)\n",
    "            if not text:\n",
    "                continue\n",
    "            m = pattern.search(text)\n",
    "            if m:\n",
    "                seen += 1\n",
    "                start, end = m.span()\n",
    "                \n",
    "                # Split text into words and find word boundaries around the match\n",
    "                words = text.split()\n",
    "                # Find which word contains the match start\n",
    "                char_count = 0\n",
    "                match_word_idx = 0\n",
    "                for idx, word in enumerate(words):\n",
    "                    if char_count + len(word) >= start:\n",
    "                        match_word_idx = idx\n",
    "                        break\n",
    "                    char_count += len(word) + 1  # +1 for space\n",
    "                \n",
    "                # Get context window of words\n",
    "                start_word_idx = max(0, match_word_idx - context_window)\n",
    "                end_word_idx = min(len(words), match_word_idx + context_window + 1)\n",
    "                snippet = ' '.join(words[start_word_idx:end_word_idx])\n",
    "                \n",
    "                # highlight\n",
    "                hit_text = text[start:end]\n",
    "                highlight = snippet.replace(hit_text, f\"**{hit_text}**\")\n",
    "                \n",
    "                # Extract all metadata\n",
    "                doc_id = row.get('id', None)\n",
    "                title = row.get('title', '')\n",
    "                url = row.get('url', '')\n",
    "                language = row.get('language', '')\n",
    "                source = row.get('source', '')\n",
    "                \n",
    "                print(f\"[{seen}] id={doc_id} | title={title} | url={url}\")\n",
    "                print(f\"      language={language} | source={source}\")\n",
    "                print(f\"{highlight}\\n{'-'*100}\")\n",
    "                found += 1\n",
    "                \n",
    "                # Update progress bar postfix with found count\n",
    "                pbar.set_postfix({\"found\": found})\n",
    "                \n",
    "                if found >= max_results:\n",
    "                    pbar.close()\n",
    "                    total_time = time.time() - start_time\n",
    "                    print(f\"\\nReached max_results={max_results}. Time elapsed: {total_time:.2f}s\")\n",
    "                    return\n",
    "        \n",
    "        # Update progress bar after each batch\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nSearch complete. Found {found} hits in {total_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8faf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD = \"gabagool\"\n",
    "BATCH_SIZE = 1024*10\n",
    "MAX_RESULTS = 100\n",
    "CONTEXT_WINDOW = 30  # Number of words before and after the match\n",
    "CASE_INSENSITIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a8de168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for 'gabagool':   6%|▋         | 102/1608 [00:18<04:17,  5.86batch/s, found=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] id=77346990 | title=List of Chapo Trap House episodes (2016–2020) | url=https://en.wikipedia.org/wiki/List_of_Chapo_Trap_House_episodes_(2016%E2%80%932020)\n",
      "      language=en | source=wikipedia\n",
      "2017 (2017-07-06) | Premium; Friedland's 3rd episode | | 132 | 123 | \"UBIsoft\" | Clio Chang | July 10, 2017 (2017-07-10) | — | | 133 | 124 | \"**Gabagool**\" | — | July 13, 2017 (2017-07-13) | Premium | | 134 | 125 | \"Fast And Furious: Toledo Drifter\" | Tim Heidecker | July 15, 2017 (2017-07-15) | Heidecker's\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for 'gabagool':   9%|▉         | 143/1608 [00:26<05:25,  4.50batch/s, found=2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] id=7100818 | title=Gabagool! | url=https://en.wikipedia.org/wiki/Gabagool!\n",
      "      language=en | source=wikipedia\n",
      "# **Gabagool**! **Gabagool**! is an American comic book that began in 2002. It was created by cartoonist Mike Dawson and humorist Chris Radtke, and concerns the various misadventures of a 30-something super-nerd,\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for 'gabagool':  14%|█▍        | 226/1608 [00:51<07:30,  3.07batch/s, found=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] id=55177852 | title=List of Mr. Pickles and Momma Named Me Sheriff characters | url=https://en.wikipedia.org/wiki/List_of_Mr._Pickles_and_Momma_Named_Me_Sheriff_characters\n",
      "      language=en | source=wikipedia\n",
      "was one of the mob's most feared hitmen before he was caught by the police. They forced Vito to rat out the entire **Gabagool**ie criminal organization and he was then placed under different surgeries to portray the eponymous legendary figure by the witness protection program. Tommy and Mr. Pickles encounter Bigfoot in the woods and he agrees to help bake a\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for 'gabagool':  20%|█▉        | 317/1608 [01:24<06:45,  3.19batch/s, found=4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] id=54591700 | title=List of SuperMansion episodes | url=https://en.wikipedia.org/wiki/List_of_SuperMansion_episodes\n",
      "      language=en | source=wikipedia\n",
      "Ranger admits to as the FBI finds the dummy. When Black Saturn, Brad, Cooch, and Jewbot return, they find that dinner was made because Lucini's cousin Angelo makes the best **gabagool** while Kid Victory has cleared things up with the FBI. As Rex vows not to send the four of them shopping again, Cooch reveals that they are banned from the\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for 'gabagool':  22%|██▏       | 356/1608 [01:41<05:56,  3.51batch/s, found=5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] id=1548050 | title=Frank Vincent | url=https://en.wikipedia.org/wiki/Frank_Vincent\n",
      "      language=en | source=wikipedia\n",
      "The Sopranos | Phil Leotardo | 31 episodes | | 2008 | Stargate Atlantis | Poker Player #1 | Episode: \"Vegas\" | | 2014–2016 | Mr. Pickles | Jon **Gabagool**i | 2 episodes Voice | | 2016 | Law & Order: Special Victims Unit | Bishop Cattalano | Episode: \"Unholiest Alliance\" | | 2017 | Neo Yokio | Uncle Albert |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Reached max_results=5. Time elapsed: 101.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_saved_dataset(dataset=ds_en,\n",
    "                     keyword=KEYWORD,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     max_results=MAX_RESULTS,\n",
    "                     context_window=CONTEXT_WINDOW,\n",
    "                     case_insensitive=CASE_INSENSITIVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf4eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
