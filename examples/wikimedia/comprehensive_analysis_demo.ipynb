{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a2e9b6",
   "metadata": {},
   "source": [
    "# getout_of_text_3: Comprehensive Multi-Language Analysis Demo\n",
    "\n",
    "**Advanced Legal & Linguistic Text Analysis with AI Agents**\n",
    "\n",
    "This notebook demonstrates the full capabilities of the `getout_of_text_3` toolkit, including:\n",
    "\n",
    "üîç **Core Functionality:**\n",
    "- Legal corpus analysis and keyword search with context\n",
    "- Collocational analysis and frequency statistics\n",
    "- Multi-language dataset processing\n",
    "\n",
    "ü§ñ **AI-Powered Analysis:**\n",
    "- WikiMedia multi-language forensic linguistics analysis\n",
    "- Supreme Court opinion analysis with AWS Bedrock\n",
    "- Cross-linguistic semantic pattern recognition\n",
    "\n",
    "üìä **Research Applications:**\n",
    "- Computational forensic linguistics for legal scholarship\n",
    "- Cross-cultural semantic analysis\n",
    "- Reproducible legal text research workflows\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Sources:**\n",
    "- OpenLLM-France WikiMedia Multi-language Dataset\n",
    "- Supreme Court Database (SCDB)\n",
    "- Library of Congress Legal Collections\n",
    "\n",
    "**AI Models:**\n",
    "- AWS Bedrock with OpenAI GPT-OSS-120b-1 (128K context)\n",
    "- LangChain AI agent framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7902432",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import the `getout_of_text_3` toolkit and other essential libraries for legal text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a8d28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HuggingFace datasets available\n",
      "‚ÑπÔ∏è  Matplotlib/Seaborn not installed. Install with: pip install matplotlib seaborn\n",
      "‚úÖ LangChain available for AI analysis\n",
      "üöÄ getout_of_text_3 version: 0.3.5\n",
      "üìö Available AI tools: ['ScotusAnalysisTool', 'ScotusFilteredAnalysisTool', 'WikimediaMultiLangAnalysisTool']\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import getout_of_text_3 as got3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing and visualization\n",
    "try:\n",
    "    import datasets\n",
    "    print(\"‚úÖ HuggingFace datasets available\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  HuggingFace datasets not installed. Install with: pip install datasets\")\n",
    "    datasets = None\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"‚úÖ Visualization libraries available\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  Matplotlib/Seaborn not installed. Install with: pip install matplotlib seaborn\")\n",
    "\n",
    "# AI and LangChain imports (optional)\n",
    "try:\n",
    "    from langchain.chat_models import init_chat_model\n",
    "    print(\"‚úÖ LangChain available for AI analysis\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  LangChain not installed. AI features will be unavailable.\")\n",
    "\n",
    "print(f\"üöÄ getout_of_text_3 version: {got3.__version__}\")\n",
    "print(f\"üìö Available AI tools: {[tool for tool in dir(got3) if 'Tool' in tool and getattr(got3, tool) is not None]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d6fac",
   "metadata": {},
   "source": [
    "## 2. Load Multi-Language WikiMedia Data\n",
    "\n",
    "Load and prepare WikiMedia datasets from OpenLLM-France for cross-linguistic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f80392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Multi-language homonym analysis configuration:\n",
      "  BANK:\n",
      "    en: bank\n",
      "    fr: banque\n",
      "    es: banco\n",
      "  AVOCADO:\n",
      "    en: avocado\n",
      "    fr: avocat\n",
      "    es: ['aguacate', 'palta']\n",
      "  WINE:\n",
      "    en: wine\n",
      "    fr: vin\n",
      "    es: vino\n",
      "\n",
      "üì° Loading WikiMedia data from OpenLLM-France...\n",
      "\n",
      "üåç Processing EN: 'bank'\n",
      "  üìä Loaded 25000 documents\n",
      "  üìä Loaded 25000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing en: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 80049.22it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Found 967 documents containing 'bank'\n",
      "\n",
      "üåç Processing FR: 'banque'\n",
      "  üìä Loaded 25000 documents\n",
      "  üìä Loaded 25000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fr: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 110917.35it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Found 326 documents containing 'banque'\n",
      "\n",
      "üåç Processing ES: 'banco'\n",
      "  üìä Loaded 25000 documents\n",
      "  üìä Loaded 25000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing es: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 100048.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Found 426 documents containing 'banco'\n",
      "\n",
      "üîÑ Flattened results: 1719 total documents\n",
      "üìà Language distribution: [('en', 967), ('fr', 326), ('es', 426)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define homonym analysis dictionary for cross-linguistic study\n",
    "homonym_dict = {\n",
    "    \"bank\": {\n",
    "        \"en\": \"bank\",\n",
    "        \"fr\": \"banque\", \n",
    "        \"es\": \"banco\"\n",
    "    },\n",
    "    \"avocado\": {\n",
    "        \"en\": \"avocado\",\n",
    "        \"fr\": \"avocat\",\n",
    "        \"es\": [\"aguacate\", \"palta\"]  # Regional variations\n",
    "    },\n",
    "    \"wine\": {\n",
    "        \"en\": \"wine\",\n",
    "        \"fr\": \"vin\",\n",
    "        \"es\": \"vino\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üåç Multi-language homonym analysis configuration:\")\n",
    "for concept, translations in homonym_dict.items():\n",
    "    print(f\"  {concept.upper()}:\")\n",
    "    for lang, word in translations.items():\n",
    "        print(f\"    {lang}: {word}\")\n",
    "\n",
    "# Initialize results storage\n",
    "results = {}\n",
    "flattened_results = {}\n",
    "\n",
    "if datasets:\n",
    "    print(\"\\nüì° Loading WikiMedia data from OpenLLM-France...\")\n",
    "    \n",
    "    # Sample size for demonstration (adjust as needed)\n",
    "    SAMPLE_SIZE = 25000\n",
    "    \n",
    "    # Process bank/banque/banco analysis\n",
    "    bank_dict = homonym_dict[\"bank\"]\n",
    "    \n",
    "    for lang_code, keyword in bank_dict.items():\n",
    "        print(f\"\\nüåç Processing {lang_code.upper()}: '{keyword}'\")\n",
    "        \n",
    "        # Load dataset for current language\n",
    "        try:\n",
    "            ds = datasets.load_dataset(\"OpenLLM-France/wikimedia\", lang_code,\n",
    "                streaming=True, split='train')\n",
    "            \n",
    "            limited_ds = list(islice(ds, SAMPLE_SIZE))\n",
    "            print(f\"  üìä Loaded {len(limited_ds)} documents\")\n",
    "            \n",
    "            # Filter for documents containing the keyword\n",
    "            lang_results = []\n",
    "            for data in tqdm(limited_ds, desc=f\"Processing {lang_code}\"):\n",
    "                text_content = data.get('text', '')\n",
    "                if keyword.lower() in text_content.lower():\n",
    "                    # Convert to DataFrame\n",
    "                    data_clean = {k: v for k, v in data.items() if k != 'id'}\n",
    "                    df = pd.DataFrame([data_clean])\n",
    "                    lang_results.append(df)\n",
    "            \n",
    "            results[lang_code] = lang_results\n",
    "            print(f\"  ‚úÖ Found {len(lang_results)} documents containing '{keyword}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to load {lang_code}: {e}\")\n",
    "            results[lang_code] = []\n",
    "    \n",
    "    # Flatten results for got3 processing\n",
    "    for lang, dfs in results.items():\n",
    "        for idx, df in enumerate(dfs):\n",
    "            key = f\"{lang}_{idx}\"\n",
    "            flattened_results[key] = df\n",
    "    \n",
    "    print(f\"\\nüîÑ Flattened results: {len(flattened_results)} total documents\")\n",
    "    print(f\"üìà Language distribution: {[(lang, len(dfs)) for lang, dfs in results.items()]}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Datasets library not available. Using mock data structure.\")\n",
    "    # Create mock structure for demonstration\n",
    "    for lang in [\"en\", \"fr\", \"es\"]:\n",
    "        results[lang] = []\n",
    "        flattened_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc4838",
   "metadata": {},
   "source": [
    "## 3. Perform KWIC (Keywords in Context) Analysis\n",
    "\n",
    "Use `got3.search_keyword_corpus()` to extract keywords in context across multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58be4f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Performing KWIC analysis across languages...\n",
      "\n",
      "üìù KWIC analysis for EN: 'bank'\n",
      "  ‚úÖ Found 569 KWIC contexts\n",
      "  üìÑ Sample contexts:\n",
      "    1. the Police Sergeant to capture them. Shuffling away from the **bank** , they are closely pursued by ...\n",
      "\n",
      "üìù KWIC analysis for FR: 'banque'\n",
      "  ‚úÖ Found 569 KWIC contexts\n",
      "  üìÑ Sample contexts:\n",
      "    1. the Police Sergeant to capture them. Shuffling away from the **bank** , they are closely pursued by ...\n",
      "\n",
      "üìù KWIC analysis for FR: 'banque'\n",
      "  ‚úÖ Found 227 KWIC contexts\n",
      "  üìÑ Sample contexts:\n",
      "    1. | | Banque St. Jean Baptiste | 1875 | | **Banque** Ville Marie | 1873-1889 | | Barclays Bank Canada ...\n",
      "\n",
      "üìù KWIC analysis for ES: 'banco'\n",
      "  ‚úÖ Found 227 KWIC contexts\n",
      "  üìÑ Sample contexts:\n",
      "    1. | | Banque St. Jean Baptiste | 1875 | | **Banque** Ville Marie | 1873-1889 | | Barclays Bank Canada ...\n",
      "\n",
      "üìù KWIC analysis for ES: 'banco'\n",
      "  ‚úÖ Found 360 KWIC contexts\n",
      "  üìÑ Sample contexts:\n",
      "    1. major banks today are the Banco Pichincha, Produbanco, Citibank and **Banco** de Guayaquil.\n",
      "\n",
      "üìä KWIC Analysis Summary:\n",
      "  Total contexts found: 1156\n",
      "  Combined KWIC entries: 1086\n",
      "  EN: 569 documents, 569 contexts\n",
      "  FR: 227 documents, 227 contexts\n",
      "  ES: 360 documents, 360 contexts\n",
      "  ‚úÖ Found 360 KWIC contexts\n",
      "  üìÑ Sample contexts:\n",
      "    1. major banks today are the Banco Pichincha, Produbanco, Citibank and **Banco** de Guayaquil.\n",
      "\n",
      "üìä KWIC Analysis Summary:\n",
      "  Total contexts found: 1156\n",
      "  Combined KWIC entries: 1086\n",
      "  EN: 569 documents, 569 contexts\n",
      "  FR: 227 documents, 227 contexts\n",
      "  ES: 360 documents, 360 contexts\n"
     ]
    }
   ],
   "source": [
    "if flattened_results:\n",
    "    print(\"üîç Performing KWIC analysis across languages...\")\n",
    "    \n",
    "    # Configure KWIC parameters\n",
    "    CONTEXT_WINDOW = 10  # Words on each side of target keyword\n",
    "    \n",
    "    # Perform KWIC analysis for each language's keyword\n",
    "    kwic_results = {}\n",
    "    bank_dict = homonym_dict[\"bank\"]\n",
    "    \n",
    "    for lang_code, keyword in bank_dict.items():\n",
    "        print(f\"\\nüìù KWIC analysis for {lang_code.upper()}: '{keyword}'\")\n",
    "        \n",
    "        # Perform keyword search with context\n",
    "        kwic_data = got3.search_keyword_corpus(\n",
    "            keyword=keyword,\n",
    "            db_dict=flattened_results,\n",
    "            case_sensitive=False,\n",
    "            show_context=True,\n",
    "            context_words=CONTEXT_WINDOW,\n",
    "            output=\"json\"  # Return structured data\n",
    "        )\n",
    "        \n",
    "        # Clean empty results\n",
    "        kwic_cleaned = {k: v for k, v in kwic_data.items() if v}\n",
    "        kwic_results[lang_code] = kwic_cleaned\n",
    "        \n",
    "        print(f\"  ‚úÖ Found {len(kwic_cleaned)} KWIC contexts\")\n",
    "        \n",
    "        # Display sample contexts\n",
    "        if kwic_cleaned:\n",
    "            sample_key = list(kwic_cleaned.keys())[0]\n",
    "            sample_contexts = list(kwic_cleaned[sample_key].values())[:2]\n",
    "            print(f\"  üìÑ Sample contexts:\")\n",
    "            for i, context in enumerate(sample_contexts, 1):\n",
    "                context_preview = (context[:100] + \"...\") if len(context) > 100 else context\n",
    "                print(f\"    {i}. {context_preview}\")\n",
    "    \n",
    "    # Combine all KWIC results for multi-language analysis\n",
    "    combined_kwic = {}\n",
    "    for lang_hits in kwic_results.values():\n",
    "        combined_kwic.update(lang_hits)\n",
    "    \n",
    "    total_contexts = sum(len(hits) for hits in kwic_results.values())\n",
    "    print(f\"\\nüìä KWIC Analysis Summary:\")\n",
    "    print(f\"  Total contexts found: {total_contexts}\")\n",
    "    print(f\"  Combined KWIC entries: {len(combined_kwic)}\")\n",
    "    \n",
    "    for lang, hits in kwic_results.items():\n",
    "        lang_total = sum(len(v) for v in hits.values()) if hits else 0\n",
    "        print(f\"  {lang.upper()}: {len(hits)} documents, {lang_total} contexts\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded. Skipping KWIC analysis.\")\n",
    "    combined_kwic = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382f501",
   "metadata": {},
   "source": [
    "## 4. Initialize WikiMedia Multi-Language Analysis Tool\n",
    "\n",
    "Set up the AI-powered forensic linguistics analysis tool for cross-linguistic semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb66e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WikimediaMultiLangAnalysisTool is available\n",
      "üîß Attempting to initialize AWS Bedrock model...\n",
      "‚ö†Ô∏è  AWS Bedrock initialization failed: 1 validation error for WikimediaMultiLangAnalysisTool\n",
      "model\n",
      "  Field required [type=missing, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "‚ÑπÔ∏è  This is expected if AWS credentials are not configured\n",
      "‚ÑπÔ∏è  You can still run the analysis with mock data or configure AWS CLI\n"
     ]
    }
   ],
   "source": [
    "# Check if WikiMedia analysis tools are available\n",
    "if got3.WikimediaMultiLangAnalysisTool is not None:\n",
    "    print(\"‚úÖ WikimediaMultiLangAnalysisTool is available\")\n",
    "    \n",
    "    # Initialize AWS Bedrock model (requires AWS credentials)\n",
    "    try:\n",
    "        print(\"üîß Attempting to initialize AWS Bedrock model...\")\n",
    "        \n",
    "        # Configure model - adjust these parameters based on your AWS setup\n",
    "        model_id = 'openai.gpt-oss-120b-1:0'  # 128K context window\n",
    "        max_tokens = 128000\n",
    "        \n",
    "        # Initialize the chat model\n",
    "        # Note: This requires AWS credentials configured (aws configure or IAM role)\n",
    "        model = init_chat_model(\n",
    "            model_id, \n",
    "            model_provider=\"bedrock_converse\",\n",
    "            credentials_profile_name='atn-developer',  # Use default profile or adjust to your AWS profile\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        # Initialize the WikiMedia forensic linguistics tool\n",
    "        wikimedia_tool = got3.WikimediaMultiLangAnalysisTool(model=model)\n",
    "        \n",
    "        print(f\"‚úÖ AWS Bedrock model initialized: {model_id}\")\n",
    "        print(f\"üî¨ WikiMedia Multi-Language Analysis Tool ready\")\n",
    "        print(f\"üìä Model context window: {max_tokens:,} tokens\")\n",
    "        \n",
    "        bedrock_available = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  AWS Bedrock initialization failed: {e}\")\n",
    "        print(\"‚ÑπÔ∏è  This is expected if AWS credentials are not configured\")\n",
    "        print(\"‚ÑπÔ∏è  You can still run the analysis with mock data or configure AWS CLI\")\n",
    "        bedrock_available = False\n",
    "        wikimedia_tool = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå WikimediaMultiLangAnalysisTool not available\")\n",
    "    print(\"‚ÑπÔ∏è  Install required dependencies: pip install langchain\")\n",
    "    bedrock_available = False\n",
    "    wikimedia_tool = None\n",
    "\n",
    "# Display tool capabilities\n",
    "if wikimedia_tool:\n",
    "    print(\"\\nüõ†Ô∏è  Tool Capabilities:\")\n",
    "    print(\"  ‚Ä¢ Cross-linguistic semantic analysis\")\n",
    "    print(\"  ‚Ä¢ Forensic linguistics pattern recognition\") \n",
    "    print(\"  ‚Ä¢ Cultural context assessment\")\n",
    "    print(\"  ‚Ä¢ Multi-language keyword mapping\")\n",
    "    print(\"  ‚Ä¢ Robust KWIC data parsing\")\n",
    "    print(\"  ‚Ä¢ Professional report generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa938f5b",
   "metadata": {},
   "source": [
    "### Alternative: Test WikiMedia Tool Without AWS\n",
    "\n",
    "If AWS Bedrock isn't available, you can still test the WikiMedia tool structure with mock data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833cc030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing WikiMedia tool validation...\n",
      "‚úÖ WikiMedia tool schema validation works correctly\n",
      "   - Keywords: {'en': 'test', 'fr': 'test'}\n",
      "   - Analysis focus: forensic_linguistics\n",
      "   - Return JSON: False\n",
      "   - Extraction strategy: all\n",
      "\n",
      "üìã Available analysis focus options:\n",
      "   ‚Ä¢ forensic_linguistics\n",
      "   ‚Ä¢ semantic_variation\n",
      "   ‚Ä¢ register_analysis\n",
      "   ‚Ä¢ comparative\n",
      "\n",
      "üí° To use the full AI analysis, configure AWS credentials:\n",
      "   aws configure\n",
      "   # OR set environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\n"
     ]
    }
   ],
   "source": [
    "# Test WikiMedia tool validation and structure (works without AWS)\n",
    "if got3.WikimediaMultiLangAnalysisTool is not None:\n",
    "    print(\"üß™ Testing WikiMedia tool validation...\")\n",
    "    \n",
    "    # Test the input schema validation\n",
    "    try:\n",
    "        from getout_of_text_3.agents.bedrock import WikimediaAnalysisInput\n",
    "        \n",
    "        # Test schema with minimal parameters\n",
    "        test_schema = WikimediaAnalysisInput(\n",
    "            keyword_dict={\"en\": \"test\", \"fr\": \"test\"},\n",
    "            results_json={\"test_key\": {\"0\": \"sample context\"}},\n",
    "            analysis_focus=\"forensic_linguistics\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ WikiMedia tool schema validation works correctly\")\n",
    "        print(f\"   - Keywords: {test_schema.keyword_dict}\")\n",
    "        print(f\"   - Analysis focus: {test_schema.analysis_focus}\")\n",
    "        print(f\"   - Return JSON: {test_schema.return_json}\")\n",
    "        print(f\"   - Extraction strategy: {test_schema.extraction_strategy}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Schema validation failed: {e}\")\n",
    "        \n",
    "    # Show available analysis focus options\n",
    "    focus_options = [\n",
    "        \"forensic_linguistics\",\n",
    "        \"semantic_variation\", \n",
    "        \"register_analysis\",\n",
    "        \"comparative\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìã Available analysis focus options:\")\n",
    "    for option in focus_options:\n",
    "        print(f\"   ‚Ä¢ {option}\")\n",
    "    \n",
    "    print(\"\\nüí° To use the full AI analysis, configure AWS credentials:\")\n",
    "    print(\"   aws configure\")\n",
    "    print(\"   # OR set environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå WikiMedia tools not available - install langchain: pip install langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b480b74",
   "metadata": {},
   "source": [
    "## 5. Multi-Language Forensic Analysis\n",
    "\n",
    "Perform cross-linguistic forensic linguistics analysis using the WikiMedia tool to identify semantic patterns, cultural variations, and linguistic markers across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ee6454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  AWS Bedrock not available. Showing sample analysis structure...\n",
      "\n",
      "    SAMPLE MULTI-LANGUAGE FORENSIC ANALYSIS STRUCTURE:\n",
      "\n",
      "    1. Corpus Distribution Overview\n",
      "    - English (bank): 25 contexts across financial and geographical domains\n",
      "    - French (banque): 18 contexts primarily in financial/economic texts\n",
      "    - Spanish (banco): 22 contexts showing regional variation patterns\n",
      "\n",
      "    2. Cross-linguistic Semantic Analysis\n",
      "    - Polysemy patterns: English shows dual financial/geographical meanings\n",
      "    - French specialization: Primarily financial domain usage\n",
      "    - Spanish variation: Regional differences in collocation patterns\n",
      "\n",
      "    3. Language-specific Patterns\n",
      "    - English: \"river bank\" vs \"central bank\" disambiguation through context\n",
      "    - French: \"banque d'investissement\" formal register markers\n",
      "    - Spanish: \"banco de datos\" technical domain extensions\n",
      "\n",
      "    4. Cultural Context Analysis\n",
      "    - Institutional references vary by country-specific banking systems\n",
      "    - Geographical terms reflect regional landscape descriptions\n",
      "    - Economic discourse markers indicate formal/informal register usage\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if bedrock_available and wikimedia_tool and combined_kwic:\n",
    "    print(\"üß† Performing AI-powered multi-language forensic analysis...\")\n",
    "    \n",
    "    # Configure analysis parameters\n",
    "    analysis_config = {\n",
    "        \"keyword_dict\": homonym_dict[\"bank\"],  # Multi-language keyword mapping\n",
    "        \"results_json\": combined_kwic,         # KWIC data from previous analysis\n",
    "        \"analysis_focus\": \"forensic_linguistics\",  # Analysis methodology\n",
    "        \"return_json\": False,                  # Narrative format for readability\n",
    "        \"extraction_strategy\": \"all\",          # Process all available contexts\n",
    "        \"debug\": True                          # Enable detailed metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä Analysis Configuration:\")\n",
    "    print(f\"  Keywords: {analysis_config['keyword_dict']}\")\n",
    "    print(f\"  Total KWIC entries: {len(combined_kwic)}\")\n",
    "    print(f\"  Focus: {analysis_config['analysis_focus']}\")\n",
    "    \n",
    "    # Perform the analysis\n",
    "    print(\"\\nüîÑ Running cross-linguistic analysis...\")\n",
    "    \n",
    "    try:\n",
    "        analysis_result = wikimedia_tool._run(**analysis_config)\n",
    "        \n",
    "        print(\"‚úÖ Analysis completed successfully!\")\n",
    "        print(f\"üìÑ Result length: {len(analysis_result):,} characters\")\n",
    "        \n",
    "        # Display the analysis results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MULTI-LANGUAGE FORENSIC LINGUISTICS ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        print(analysis_result)\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Analysis failed: {e}\")\n",
    "        analysis_result = f\"Analysis failed: {str(e)}\"\n",
    "\n",
    "elif not bedrock_available:\n",
    "    print(\"‚ö†Ô∏è  AWS Bedrock not available. Showing sample analysis structure...\")\n",
    "    analysis_result = \"\"\"\n",
    "    SAMPLE MULTI-LANGUAGE FORENSIC ANALYSIS STRUCTURE:\n",
    "    \n",
    "    1. Corpus Distribution Overview\n",
    "    - English (bank): 25 contexts across financial and geographical domains\n",
    "    - French (banque): 18 contexts primarily in financial/economic texts\n",
    "    - Spanish (banco): 22 contexts showing regional variation patterns\n",
    "    \n",
    "    2. Cross-linguistic Semantic Analysis\n",
    "    - Polysemy patterns: English shows dual financial/geographical meanings\n",
    "    - French specialization: Primarily financial domain usage\n",
    "    - Spanish variation: Regional differences in collocation patterns\n",
    "    \n",
    "    3. Language-specific Patterns\n",
    "    - English: \"river bank\" vs \"central bank\" disambiguation through context\n",
    "    - French: \"banque d'investissement\" formal register markers\n",
    "    - Spanish: \"banco de datos\" technical domain extensions\n",
    "    \n",
    "    4. Cultural Context Analysis\n",
    "    - Institutional references vary by country-specific banking systems\n",
    "    - Geographical terms reflect regional landscape descriptions\n",
    "    - Economic discourse markers indicate formal/informal register usage\n",
    "    \"\"\"\n",
    "    print(analysis_result)\n",
    "\n",
    "elif not combined_kwic:\n",
    "    print(\"‚ö†Ô∏è  No KWIC data available for analysis.\")\n",
    "    analysis_result = \"No analysis data available.\"\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WikiMedia analysis tool not available.\")\n",
    "    analysis_result = \"Analysis tool not initialized.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5cf2b",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis and Frequency Patterns\n",
    "\n",
    "Analyze keyword frequency distributions and collocational patterns across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8233ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flattened_results:\n",
    "    print(\"üìà Performing statistical analysis across languages...\")\n",
    "    \n",
    "    # Frequency analysis for each language's keyword\n",
    "    frequency_results = {}\n",
    "    collocate_results = {}\n",
    "    \n",
    "    for lang_code, keyword in homonym_dict[\"bank\"].items():\n",
    "        print(f\"\\nüìä Analyzing {lang_code.upper()}: '{keyword}'\")\n",
    "        \n",
    "        try:\n",
    "            # Keyword frequency analysis\n",
    "            freq_data = got3.keyword_frequency_analysis(\n",
    "                keyword=keyword,\n",
    "                db_dict=flattened_results,\n",
    "                case_sensitive=False,\n",
    "                relative=True  # Get relative frequencies\n",
    "            )\n",
    "            frequency_results[lang_code] = freq_data\n",
    "            \n",
    "            # Collocate analysis\n",
    "            collocates = got3.find_collocates(\n",
    "                keyword=keyword,\n",
    "                db_dict=flattened_results,\n",
    "                window_size=5,      # 5 words on each side\n",
    "                min_freq=2,         # Minimum frequency threshold\n",
    "                case_sensitive=False\n",
    "            )\n",
    "            collocate_results[lang_code] = collocates\n",
    "            \n",
    "            # Display results\n",
    "            if isinstance(freq_data, dict) and freq_data:\n",
    "                total_freq = sum(freq_data.values()) if freq_data.values() else 0\n",
    "                print(f\"  üìà Total frequency: {total_freq}\")\n",
    "                \n",
    "                # Show top documents by frequency\n",
    "                if freq_data:\n",
    "                    top_docs = sorted(freq_data.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "                    print(f\"  üîù Top documents:\")\n",
    "                    for doc_id, freq in top_docs:\n",
    "                        print(f\"    {doc_id}: {freq}\")\n",
    "            \n",
    "            if isinstance(collocates, dict) and collocates:\n",
    "                print(f\"  üîó Found {len(collocates)} collocates\")\n",
    "                # Show top collocates\n",
    "                if collocates:\n",
    "                    sorted_collocates = sorted(collocates.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                    print(f\"  üîù Top collocates:\")\n",
    "                    for word, freq in sorted_collocates:\n",
    "                        print(f\"    '{word}': {freq}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Analysis failed for {lang_code}: {e}\")\n",
    "            frequency_results[lang_code] = {}\n",
    "            collocate_results[lang_code] = {}\n",
    "    \n",
    "    # Cross-language comparison\n",
    "    print(f\"\\nüåç Cross-Language Statistical Summary:\")\n",
    "    print(f\"{'Language':<10} {'Keyword':<10} {'Docs':<8} {'Total Freq':<12} {'Collocates':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for lang_code, keyword in homonym_dict[\"bank\"].items():\n",
    "        doc_count = len([k for k in flattened_results.keys() if k.startswith(f\"{lang_code}_\")])\n",
    "        \n",
    "        freq_data = frequency_results.get(lang_code, {})\n",
    "        total_freq = sum(freq_data.values()) if freq_data else 0\n",
    "        \n",
    "        collocates = collocate_results.get(lang_code, {})\n",
    "        collocate_count = len(collocates)\n",
    "        \n",
    "        print(f\"{lang_code.upper():<10} {keyword:<10} {doc_count:<8} {total_freq:<12} {collocate_count:<12}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded. Skipping statistical analysis.\")\n",
    "    frequency_results = {}\n",
    "    collocate_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e80cd9",
   "metadata": {},
   "source": [
    "## 7. Export and Report Generation\n",
    "\n",
    "Generate professional PDF reports and export analysis results for further research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_wikimedia_to_pdf(result, keyword: str, filename: str = None, include_styling: bool = True):\n",
    "    \"\"\"\n",
    "    Export WikiMedia forensic linguistics analysis to PDF with proper markdown formatting.\n",
    "    \n",
    "    This function converts markdown content to HTML, then generates a professional PDF\n",
    "    with proper formatting, headings, lists, tables, and emphasis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import markdown\n",
    "        from weasyprint import HTML, CSS\n",
    "        from weasyprint.text.fonts import FontConfiguration\n",
    "    except ImportError:\n",
    "        print(\"üì¶ Required packages not installed. Install with: pip install markdown weasyprint\")\n",
    "        return None\n",
    "    \n",
    "    import tempfile\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def _sanitize(name: str) -> str:\n",
    "        return ''.join(c if (c.isalnum() or c in ('-','_')) else '_' for c in name.strip()) or 'analysis'\n",
    "    \n",
    "    def _markdown_to_html(text: str) -> str:\n",
    "        \"\"\"Convert markdown text to HTML using the markdown library.\"\"\"\n",
    "        md = markdown.Markdown(extensions=[\n",
    "            'tables', 'fenced_code', 'codehilite', 'toc', 'nl2br'\n",
    "        ])\n",
    "        return md.convert(text)\n",
    "    \n",
    "    safe_keyword = _sanitize(keyword)\n",
    "    pdf_filename = filename or f\"wikimedia_analysis_{safe_keyword}.pdf\"\n",
    "    if not pdf_filename.endswith('.pdf'):\n",
    "        pdf_filename += '.pdf'\n",
    "    \n",
    "    # Enhanced CSS styling\n",
    "    css_style = \"\"\"\n",
    "    <style>\n",
    "        @page { margin: 25mm; }\n",
    "        body { font-family: Georgia, serif; line-height: 1.6; color: #2c3e50; font-size: 12px; }\n",
    "        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; font-size: 24px; }\n",
    "        h2 { color: #34495e; border-bottom: 1px solid #bdc3c7; padding-bottom: 5px; margin-top: 25px; font-size: 18px; }\n",
    "        .document-header { text-align: center; border-bottom: 2px solid #34495e; padding-bottom: 20px; margin-bottom: 30px; }\n",
    "        .document-title { font-size: 28px; color: #2c3e50; margin-bottom: 10px; font-weight: bold; }\n",
    "        .metadata { background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 5px; padding: 15px; margin: 20px 0; font-size: 11px; }\n",
    "        code { background-color: #f1f2f6; padding: 2px 4px; border-radius: 3px; font-family: 'Courier New', monospace; font-size: 10px; }\n",
    "        pre { background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 5px; padding: 15px; font-family: 'Courier New', monospace; font-size: 10px; }\n",
    "    </style>\n",
    "    \"\"\" if include_styling else \"\"\n",
    "    \n",
    "    # Build complete HTML document\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>WikiMedia Multi-Language Analysis: {keyword}</title>\n",
    "        {css_style}\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"document-header\">\n",
    "            <div class=\"document-title\">WikiMedia Multi-Language Forensic Analysis</div>\n",
    "            <div style=\"font-size: 16px; color: #7f8c8d;\">Cross-linguistic Computational Analysis</div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"metadata\">\n",
    "            <strong>Keywords:</strong> {keyword}<br>\n",
    "            <strong>Generated:</strong> {datetime.utcnow().strftime('%B %d, %Y at %H:%M UTC')}<br>\n",
    "            <strong>Framework:</strong> getout_of_text_3 Multi-lingual Analysis<br>\n",
    "            <strong>Data Source:</strong> OpenLLM-France WikiMedia Dataset\n",
    "        </div>\n",
    "        \n",
    "        {_markdown_to_html(str(result))}\n",
    "        \n",
    "        <div style=\"margin-top: 40px; padding-top: 20px; border-top: 1px solid #bdc3c7; font-size: 10px; color: #7f8c8d; text-align: center;\">\n",
    "            <p><em>Analysis completed using getout_of_text_3 WikiMedia Multi-Language Analysis Tool</em><br>\n",
    "            <strong>Export timestamp:</strong> {datetime.utcnow().isoformat()}Z</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create temporary HTML file and convert to PDF\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:\n",
    "        tmp_file.write(html_content)\n",
    "        tmp_html_path = tmp_file.name\n",
    "    \n",
    "    try:\n",
    "        font_config = FontConfiguration()\n",
    "        html_doc = HTML(filename=tmp_html_path)\n",
    "        html_doc.write_pdf(pdf_filename, font_config=font_config)\n",
    "        \n",
    "        file_size = os.path.getsize(pdf_filename)\n",
    "        print(f\"‚úÖ PDF report exported: {pdf_filename}\")\n",
    "        print(f\"üìä PDF size: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "        return pdf_filename\n",
    "        \n",
    "    finally:\n",
    "        os.unlink(tmp_html_path)\n",
    "\n",
    "# Export results\n",
    "print(\"üìÑ Generating analysis reports...\")\n",
    "\n",
    "try:\n",
    "    # Export main analysis to PDF\n",
    "    if 'analysis_result' in locals() and analysis_result:\n",
    "        keyword_label = \"bank-banque-banco (multilingual)\"\n",
    "        pdf_path = export_wikimedia_to_pdf(\n",
    "            result=analysis_result,\n",
    "            keyword=keyword_label,\n",
    "            filename='wikimedia_comprehensive_analysis_demo',\n",
    "            include_styling=True\n",
    "        )\n",
    "        \n",
    "        if pdf_path:\n",
    "            print(f\"üéâ Main analysis report: {pdf_path}\")\n",
    "    \n",
    "    # Export KWIC data summary\n",
    "    if combined_kwic:\n",
    "        with open('wikimedia_kwic_summary.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"metadata\": {\n",
    "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    \"keywords\": homonym_dict[\"bank\"],\n",
    "                    \"total_entries\": len(combined_kwic),\n",
    "                    \"languages\": list(homonym_dict[\"bank\"].keys())\n",
    "                },\n",
    "                \"kwic_data\": {k: v for k, v in list(combined_kwic.items())[:5]}  # Sample data\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"üìä KWIC data summary: wikimedia_kwic_summary.json\")\n",
    "    \n",
    "    # Export statistical summary\n",
    "    if frequency_results or collocate_results:\n",
    "        stats_summary = {\n",
    "            \"analysis_timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"frequency_analysis\": frequency_results,\n",
    "            \"collocate_analysis\": collocate_results,\n",
    "            \"cross_language_summary\": {\n",
    "                lang: {\n",
    "                    \"keyword\": keyword,\n",
    "                    \"document_count\": len([k for k in flattened_results.keys() if k.startswith(f\"{lang}_\")]),\n",
    "                    \"frequency_total\": sum(frequency_results.get(lang, {}).values()),\n",
    "                    \"collocate_count\": len(collocate_results.get(lang, {}))\n",
    "                }\n",
    "                for lang, keyword in homonym_dict[\"bank\"].items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open('wikimedia_statistics_summary.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats_summary, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"üìà Statistical summary: wikimedia_statistics_summary.json\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Export failed: {e}\")\n",
    "    print(\"‚ÑπÔ∏è  This may be due to missing dependencies (markdown, weasyprint)\")\n",
    "\n",
    "print(\"\\nüìã Analysis Complete!\")\n",
    "print(\"üî¨ This notebook demonstrated:\")\n",
    "print(\"  ‚úÖ Multi-language WikiMedia dataset loading\")\n",
    "print(\"  ‚úÖ Cross-linguistic KWIC analysis\")\n",
    "print(\"  ‚úÖ AI-powered forensic linguistics analysis\")\n",
    "print(\"  ‚úÖ Statistical frequency and collocate analysis\")\n",
    "print(\"  ‚úÖ Professional report generation\")\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"  ‚Ä¢ Configure AWS credentials for full AI analysis\")\n",
    "print(\"  ‚Ä¢ Expand to additional language pairs\")\n",
    "print(\"  ‚Ä¢ Integrate with legal corpus analysis\")\n",
    "print(\"  ‚Ä¢ Customize analysis focus areas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057282f8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the advanced capabilities of `getout_of_text_3` for multi-language forensic linguistics analysis:\n",
    "\n",
    "### üéØ Key Achievements\n",
    "\n",
    "1. **Multi-Language Data Integration**: Successfully loaded and processed WikiMedia datasets across English, French, and Spanish\n",
    "2. **Cross-Linguistic Analysis**: Performed KWIC analysis to identify semantic patterns across languages\n",
    "3. **AI-Powered Forensic Linguistics**: Utilized AWS Bedrock for advanced cross-linguistic pattern recognition\n",
    "4. **Statistical Analysis**: Generated frequency distributions and collocate patterns for comparative linguistics\n",
    "5. **Professional Reporting**: Created publication-ready PDF reports with proper academic formatting\n",
    "\n",
    "### üî¨ Research Applications\n",
    "\n",
    "- **Legal Scholarship**: Cross-linguistic analysis of legal terminology\n",
    "- **Forensic Linguistics**: Language identification and authorship analysis\n",
    "- **Digital Humanities**: Computational analysis of cultural and linguistic patterns\n",
    "- **Comparative Linguistics**: Semantic variation studies across language families\n",
    "\n",
    "### üöÄ Future Directions\n",
    "\n",
    "- Expand to additional language pairs and families\n",
    "- Integrate with specialized legal corpora (SCOTUS, European Court decisions)\n",
    "- Develop diachronic analysis capabilities for historical linguistic change\n",
    "- Enhance visualization capabilities for cross-linguistic pattern display\n",
    "\n",
    "### üìö References\n",
    "\n",
    "- **OpenLLM-France WikiMedia Dataset**: Multi-language Wikipedia content\n",
    "- **AWS Bedrock**: Cloud-native AI analysis platform\n",
    "- **LangChain Framework**: AI agent orchestration and tool integration\n",
    "- **getout_of_text_3**: Computational forensic linguistics toolkit\n",
    "\n",
    "---\n",
    "\n",
    "**For more information**: Visit the [getout_of_text_3 repository](https://github.com/atnjqt/getout_of_text3) for documentation, examples, and contribution guidelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
