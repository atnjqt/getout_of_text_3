{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0204710a",
   "metadata": {},
   "source": [
    "# Contemporary Corpus of American English (COCA) Full Text Analysis\n",
    "\n",
    "This notebook is helpful for you if you purchased the Contemporary Corpus of American English (COCA) Full Text and want to handle it offline versus using the web GUI at https://www.english-corpora.org/coca/. We demonstrate various functionalities in `getout_of_text3` that can be used to effectively read and analyze the COCA full text data.\n",
    "\n",
    "- Goals of this notebook include \n",
    "  - understanding how to use the COCA full text data\n",
    "  - staging a dictionary of dictionaries of dictionaries `{'genre': {'year': pd.df}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getout_of_text_3 as got3\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "got3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -la ../../data/english-corpora.org/coca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus = got3.read_corpus('../../data/english-corpora.org/coca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721457ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad123577",
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus['acad'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus['web'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da3574",
   "metadata": {},
   "source": [
    "____________________________\n",
    "## Search Keyword \n",
    "\n",
    "- using `bovine` as a test keyword across the full COCA corpus\n",
    "- COMPARE YOUR RESULTS TO THE OUTPUT HERE, IF POSSIBLE: https://www.english-corpora.org/coca/\n",
    "  - I get sometimes less and sometimes more hits! TBD and needs review...\n",
    "\n",
    "\n",
    "### Comparing parallel vs non-parallel kwic search\n",
    "\n",
    "- the `n_jobs` parameter will automatically use n-1 cores to use all but one of your CPU cores. This leads to much better performance on large corpora.\n",
    "- i.e. for `bovine` on the full COCA text corpus, I get (10-1=9 CPU cores):\n",
    "  - non-parallel: time elapsed: 0 days 00:01:01.157718\n",
    "  - parallel: time elapsed: 0 days 00:00:22.578978\n",
    "  - almost 3x faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = pd.Timestamp.now()\n",
    "bovine_kwic = got3.search_keyword_corpus('tribunal', coca_corpus, \n",
    "                                            case_sensitive=False,\n",
    "                                            show_context=True, \n",
    "                                            context_words=15,\n",
    "                                            output='print',\n",
    "                                            parallel=True)\n",
    "after = pd.Timestamp.now()\n",
    "print('time elapsed:', after - before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = pd.Timestamp.now()\n",
    "bovine_kwic = got3.search_keyword_corpus('bovine', coca_corpus, \n",
    "                                            case_sensitive=False,\n",
    "                                            show_context=True, \n",
    "                                            context_words=15,\n",
    "                                            output='print',\n",
    "                                            parallel=True)\n",
    "after = pd.Timestamp.now()\n",
    "print('time elapsed:', after - before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d25f4",
   "metadata": {},
   "source": [
    "_____________________________\n",
    "### Run keyword_frequency_analysis\n",
    "\n",
    "- get a distribution of keyword frequencies across the full COCA corpus genres\n",
    "- this shows `1178812039` (~1.1 billion tokens) in the COCA dataset -- on the site it's published as `1.0 billion` # of words.\n",
    "  - notably the English-Corpora site returns `bovine` with 1248 hits in the full COCA corpus, whereas I get `1252` hits here -- so there is a discrepancy of 4 hits. TBD why this is the case.\n",
    "\n",
    "> üö® discrepacy also in the kwic versus the frequency analysis here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798702d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = pd.Timestamp.now()\n",
    "bovine_freq = got3.keyword_frequency_analysis('bovine', \n",
    "                                              coca_corpus, \n",
    "                                              case_sensitive=False,\n",
    "                                              relative=True, # optionally to show column, per 10k words\n",
    "                                              parallel=True # use parallel processing\n",
    "                                              )\n",
    "after = pd.Timestamp.now()\n",
    "print('time elapsed:', after - before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55adb426",
   "metadata": {},
   "source": [
    "______________________________________\n",
    "## Finding collocates of `bovine` in the COCA corpus\n",
    "\n",
    "The `find_collocates` function in `getout_of_text_3` allows you to identify words that frequently appear near a target keyword within a large corpus, such as COCA. \n",
    "\n",
    "- Simply provide your `keyword` and `coca_corpus`, i.e. the corpus dictionary, and optional parameters like window size, minimum frequency, and parallel processing. \n",
    "\n",
    "- The function returns a dictionary of collocates and their counts, which you can easily convert to a DataFrame for further analysis or visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31394200",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = pd.Timestamp.now()\n",
    "bovine_collocates = got3.find_collocates('bovine', \n",
    "                                         coca_corpus,\n",
    "                                         window_size=15,\n",
    "                                         min_freq=2,\n",
    "                                         case_sensitive=False,\n",
    "                                         parallel=True)\n",
    "after = pd.Timestamp.now()\n",
    "print('time elapsed:', after - before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afa5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = pd.Timestamp.now()\n",
    "bovine_collocates = got3.find_collocates('bovine', \n",
    "                                         coca_corpus,\n",
    "                                         window_size=15,\n",
    "                                         min_freq=2,\n",
    "                                         case_sensitive=False,\n",
    "                                         parallel=True)\n",
    "after = pd.Timestamp.now()\n",
    "print('time elapsed:', after - before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed337f52",
   "metadata": {},
   "source": [
    "### Part of Speech tagging for collocates\n",
    "\n",
    "- using the `spacy` library to tag the collocates with their parts of speech (POS)\n",
    "- tbd if this will get included with `got3`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f426fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "download('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert collocates to DataFrame and show value counts\n",
    "collocates_df = pd.DataFrame(list(bovine_collocates['all_collocates'].items()), columns=['word', 'count'])\n",
    "collocates_df = collocates_df.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Get top 50 collocates\n",
    "top_collocates = collocates_df.head(50)['word'].tolist()\n",
    "\n",
    "# POS tagging\n",
    "collocate_pos = [(word, nlp(word)[0].pos_) for word in top_collocates]\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "collocate_pos_df = pd.DataFrame(collocate_pos, columns=['word', 'pos'])\n",
    "collocate_pos_df.head(20)\n",
    "\n",
    "# now merge the two so we get pos along with the counts\n",
    "collocates_with_pos = pd.merge(collocates_df, collocate_pos_df, on='word', how='left')\n",
    "collocates_with_pos.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052590c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hub_layout(G, keyword, group_attr=\"pos_group\", radius=10.0, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    groups = [g for g in sorted(set(nx.get_node_attributes(G, group_attr).values())) if g != \"KEYWORD\"]\n",
    "    n_groups = len(groups)\n",
    "\n",
    "    # keyword at center\n",
    "    pos = {keyword: (0,0)}\n",
    "\n",
    "    # arrange other groups in orbit\n",
    "    angles = np.linspace(0, 2*np.pi, n_groups, endpoint=False)\n",
    "    group_positions = {\n",
    "        grp: (radius*np.cos(a), radius*np.sin(a))\n",
    "        for grp, a in zip(groups, angles)\n",
    "    }\n",
    "\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if node == keyword: \n",
    "            continue\n",
    "        grp = data.get(group_attr, \"OTHER\")\n",
    "        cx, cy = group_positions.get(grp, (0,0))\n",
    "        jitter = rng.normal(scale=1.0, size=2)\n",
    "        pos[node] = (cx + jitter[0], cy + jitter[1])\n",
    "    return pos\n",
    "\n",
    "# usage:\n",
    "pos = hub_layout(G, keyword, group_attr=\"pos_group\", radius=3.5, seed=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Node size + color computation\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Color palette for groups\n",
    "pos_colors = {\n",
    "    'KEYWORD': '#d62728',  # red\n",
    "    'NOUN': '#1f77b4',     # blue\n",
    "    'VERB': '#2ca02c',     # green\n",
    "    'ADJ': '#ff7f0e',      # orange\n",
    "    'ADV': '#9467bd',      # purple\n",
    "    'PROPN': '#bcbd22',    # olive\n",
    "    'OTHER': '#7f7f7f'     # gray\n",
    "}\n",
    "\n",
    "# Group-relative sizing parameters\n",
    "BASE_MIN = 1500    # minimum size for non-keyword nodes\n",
    "BASE_MAX = 4500    # maximum size (largest within a group)\n",
    "KEYWORD_SIZE = 6000  # keyword node size (increased from 3200)\n",
    "\n",
    "from collections import defaultdict\n",
    "max_per_group = defaultdict(lambda: 1)\n",
    "for n in G.nodes:\n",
    "    if n == keyword:\n",
    "        continue\n",
    "    grp = G.nodes[n]['pos_group']\n",
    "    max_per_group[grp] = max(max_per_group[grp], G.nodes[n]['count'])\n",
    "\n",
    "# Compute node sizes\n",
    "node_sizes = []\n",
    "for n in G.nodes:\n",
    "    data = G.nodes[n]\n",
    "    if n == keyword:\n",
    "        node_sizes.append(KEYWORD_SIZE)\n",
    "        continue\n",
    "    grp = data['pos_group']\n",
    "    grp_max = max_per_group.get(grp, 1)\n",
    "    rel = data['count'] / grp_max if grp_max else 0\n",
    "    size = BASE_MIN + (BASE_MAX - BASE_MIN) * rel\n",
    "    node_sizes.append(size)\n",
    "\n",
    "# Compute node colors\n",
    "node_colors = [\n",
    "    pos_colors.get(G.nodes[n].get('pos_group', 'OTHER'), '#7f7f7f')\n",
    "    for n in G.nodes\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9edc7",
   "metadata": {},
   "source": [
    "### Thinking about crawling or querying wiki data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; MyBot/0.1; +https://example.com/bot)\"\n",
    "}\n",
    "url = \"https://en.wikipedia.org/wiki/Bovinae\"\n",
    "response = requests.get(url, headers=headers, timeout=3)\n",
    "print(response)\n",
    "print(response.status_code)\n",
    "print('response content:', response.content[:500])  # print first 500 characters\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Find any table whose class starts with 'infobox'\n",
    "    infobox = soup.find('table', class_=re.compile(r'^infobox'))\n",
    "    if infobox:\n",
    "        # Extract key info from infobox\n",
    "        info_text = infobox.get_text(separator=' | ', strip=True)\n",
    "        # Limit to first 200 chars for hover\n",
    "        print(info_text[:200] + \"...\" if len(info_text) > 200 else info_text)\n",
    "    else:\n",
    "        # Get first paragraph if no infobox\n",
    "        first_para = soup.find('p')\n",
    "        if first_para:\n",
    "            text = first_para.get_text(strip=True)\n",
    "            print(text[:150] + \"...\" if len(text) > 150 else text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a317f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. Build Graph (your earlier code)\n",
    "# ------------------------------------------------------------------\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "keyword = 'bovine'  # or whatever your keyword is\n",
    "G = nx.Graph()\n",
    "G.add_node(keyword, pos_group='KEYWORD', count=collocates_with_pos['count'].max())\n",
    "\n",
    "for _, row in top_pos_table.iterrows():\n",
    "    w = row['word']\n",
    "    posg = row['pos_group']\n",
    "    cnt = int(row['count'])\n",
    "    if w == keyword:\n",
    "        continue\n",
    "    G.add_node(w, pos_group=posg, count=cnt)\n",
    "    G.add_edge(keyword, w, weight=cnt)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Hub layout (so groups orbit the keyword)\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "def hub_layout(G, keyword, group_attr=\"pos_group\", radius=10.0, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    groups = [g for g in sorted(set(nx.get_node_attributes(G, group_attr).values())) if g != \"KEYWORD\"]\n",
    "    n_groups = len(groups)\n",
    "\n",
    "    pos = {keyword: (0,0)}  # keyword at center\n",
    "    angles = np.linspace(0, 2*np.pi, n_groups, endpoint=False)\n",
    "    group_positions = {\n",
    "        grp: (radius*np.cos(a), radius*np.sin(a))\n",
    "        for grp, a in zip(groups, angles)\n",
    "    }\n",
    "\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if node == keyword:\n",
    "            continue\n",
    "        grp = data.get(group_attr, \"OTHER\")\n",
    "        cx, cy = group_positions.get(grp, (0,0))\n",
    "        jitter = rng.normal(scale=1.0, size=2)\n",
    "        pos[node] = (cx + jitter[0], cy + jitter[1])\n",
    "    return pos\n",
    "\n",
    "pos = hub_layout(G, keyword, group_attr=\"pos_group\", radius=3.5, seed=4)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Interactive Plotly draw\n",
    "# ------------------------------------------------------------------\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "x_nodes = [pos[n][0] for n in G.nodes]\n",
    "y_nodes = [pos[n][1] for n in G.nodes]\n",
    "node_texts = [f\"{n}<br>POS: {G.nodes[n]['pos_group']}<br>Count: {G.nodes[n]['count']}\" \n",
    "              for n in G.nodes]\n",
    "\n",
    "x_edges, y_edges = [], []\n",
    "for u, v in G.edges:\n",
    "    x_edges += [pos[u][0], pos[v][0], None]\n",
    "    y_edges += [pos[u][1], pos[v][1], None]\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=x_edges, y=y_edges,\n",
    "    line=dict(width=0.8, color=\"#888\"),\n",
    "    hoverinfo=\"none\",\n",
    "    mode=\"lines\"\n",
    ")\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=x_nodes, y=y_nodes,\n",
    "    mode=\"markers+text\",\n",
    "    text=[n for n in G.nodes],\n",
    "    textposition=\"middle center\",\n",
    "    marker=dict(\n",
    "        size=[s/60 for s in node_sizes],  # rescale sizes for Plotly\n",
    "        color=node_colors,\n",
    "        line=dict(width=1.5, color=\"black\")\n",
    "    ),\n",
    "    hovertext=node_texts,\n",
    "    hoverinfo=\"text\"\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[edge_trace, node_trace])\n",
    "fig.update_layout(\n",
    "    title=f\"Collocates Network for ‚Äú{keyword}‚Äù (Interactive)\",\n",
    "    title_x=0.5,\n",
    "    height=800,\n",
    "    plot_bgcolor=\"white\",\n",
    "    showlegend=False,\n",
    "    margin=dict(l=20, r=20, t=40, b=20),\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291fa79c",
   "metadata": {},
   "source": [
    "### FUN! There are only a few hits for `gabagool`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ff059",
   "metadata": {},
   "outputs": [],
   "source": [
    "gabagool_kwic = got3.search_keyword_corpus('gabagool', coca_corpus,\n",
    "                                            case_sensitive=False,\n",
    "                                            show_context=False, \n",
    "                                            context_words=15,\n",
    "                                            output='print',\n",
    "                                            parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba418fad",
   "metadata": {},
   "source": [
    "![https://static0.cbrimages.com/wordpress/wp-content/uploads/2025/01/tony-and-gabagool-featured-image.PNG?w=1200&h=628&fit=crop](https://static0.cbrimages.com/wordpress/wp-content/uploads/2025/01/tony-and-gabagool-featured-image.PNG?w=1200&h=628&fit=crop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
