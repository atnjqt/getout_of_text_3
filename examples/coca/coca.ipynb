{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2125a2c",
   "metadata": {},
   "source": [
    "### Demonstration notebook for functionalities to include on GOT3 tool\n",
    "\n",
    "- working through some steps with `pandas` and`nltk`, to later roll into the toolset `getout_of_text_3` to streamline COCA Corpora searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa657b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getout_of_text_3 as got3\n",
    "got3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import necessary libraries for search functionality\n",
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"âœ… NLTK libraries ready!\")\n",
    "except:\n",
    "    print(\"âš ï¸ NLTK download may have failed, but will continue\")\n",
    "\n",
    "# Test tokenization\n",
    "try:\n",
    "    test_words = nltk.word_tokenize(\"This is a test.\")\n",
    "    print(f\"âœ… Tokenization working: {test_words}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Tokenization issue: {e}\")\n",
    "    print(\"Will use simple split() method as fallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418d6a8",
   "metadata": {},
   "source": [
    "## reading coca db and txt\n",
    "- dictionaries with genre as the key and dfs as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = ['acad', 'blog', 'fic', \n",
    "              'mag', 'news', 'spok',\n",
    "              'tvm', 'web']\n",
    "db_df = {}\n",
    "db_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe00fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in genre_dict:\n",
    "    print(f\"ðŸ“‚ Processing {genre}...\")\n",
    "    \n",
    "    # Load db file\n",
    "    try:\n",
    "        db_df[genre] = pd.read_csv(\"../coca-samples-db/db_{}.txt\".format(genre), \n",
    "                                   sep=\"\\t\", \n",
    "                                   header=None, \n",
    "                                   names=[\"text\"],\n",
    "                                   on_bad_lines='skip',\n",
    "                                   quoting=3)\n",
    "        print(f\"  âœ… db_{genre}.txt: {db_df[genre].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error reading db_{genre}: {e}\")\n",
    "    \n",
    "    # Load text file\n",
    "    try:\n",
    "        db_text[genre] = pd.read_csv(\"../coca-samples-text/text_{}.txt\".format(genre), \n",
    "                                     sep=\"\\t\", \n",
    "                                     header=None, \n",
    "                                     names=[\"text\"],\n",
    "                                     on_bad_lines='skip',\n",
    "                                     quoting=3)\n",
    "        print(f\"  âœ… text_{genre}.txt: {db_text[genre].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error reading text_{genre}: {e}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ SUMMARY:\")\n",
    "print(f\"   - db_df: {len(db_df)} genres loaded\") \n",
    "print(f\"   - db_text: {len(db_text)} genres loaded\")\n",
    "print(f\"   - Processed each genre exactly once âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5f585",
   "metadata": {},
   "source": [
    "## For collocate or keyword searches, we can use the following approach:\n",
    "1. loop through each genre\n",
    "2. do string filter hits for each instance of a string match in the dictionary key dataframe text column\n",
    "3. print out in an elegant manner\n",
    "\n",
    "use NLTK for this if that makes things easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keyword_corpus(keyword, db_dict, case_sensitive=False, show_context=True, context_words=5):\n",
    "    \"\"\"\n",
    "    Search for a keyword across all COCA genres and display results elegantly.\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: The word/phrase to search for\n",
    "    - db_dict: Dictionary of DataFrames (either db_df or db_text)\n",
    "    - case_sensitive: Whether to perform case-sensitive search\n",
    "    - show_context: Whether to show surrounding context\n",
    "    - context_words: Number of words to show on each side for context\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with search results by genre\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ” COCA Corpus Search: '{keyword}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    total_hits = 0\n",
    "    \n",
    "    # Prepare search pattern\n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    # Search through each genre\n",
    "    for genre, df in db_dict.items():\n",
    "        genre_hits = 0\n",
    "        print(f\"\\nðŸ“š {genre.upper()} Genre:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for idx, text in df['text'].items():\n",
    "            text_str = str(text)\n",
    "            matches = pattern.findall(text_str)\n",
    "            \n",
    "            if matches:\n",
    "                genre_hits += len(matches)\n",
    "                \n",
    "                if show_context:\n",
    "                    # Find all match positions and show context\n",
    "                    for match in pattern.finditer(text_str):\n",
    "                        start, end = match.span()\n",
    "                        \n",
    "                        # Get context words\n",
    "                        words = text_str.split()\n",
    "                        text_words = ' '.join(words)\n",
    "                        \n",
    "                        # Find word boundaries for context\n",
    "                        words_before_match = text_str[:start].split()\n",
    "                        words_after_match = text_str[end:].split()\n",
    "                        \n",
    "                        # Build context\n",
    "                        context_before = ' '.join(words_before_match[-context_words:]) if words_before_match else \"\"\n",
    "                        matched_word = text_str[start:end]\n",
    "                        context_after = ' '.join(words_after_match[:context_words]) if words_after_match else \"\"\n",
    "                        \n",
    "                        # Format the context nicely\n",
    "                        context_display = f\"...{context_before} **{matched_word}** {context_after}...\"\n",
    "                        context_display = context_display.replace(\"...\", \"\").strip()\n",
    "                        \n",
    "                        results[genre].append({\n",
    "                            'text_id': idx,\n",
    "                            'match': matched_word,\n",
    "                            'context': context_display,\n",
    "                            'full_text': text_str[:100] + \"...\" if len(text_str) > 100 else text_str\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"  ðŸ“ Text {idx}: {context_display}\")\n",
    "                else:\n",
    "                    results[genre].append({\n",
    "                        'text_id': idx,\n",
    "                        'matches': len(matches),\n",
    "                        'full_text': text_str[:100] + \"...\" if len(text_str) > 100 else text_str\n",
    "                    })\n",
    "        \n",
    "        if genre_hits > 0:\n",
    "            print(f\"  âœ… Found {genre_hits} occurrence(s) in {genre}\")\n",
    "        else:\n",
    "            print(f\"  âŒ No matches found in {genre}\")\n",
    "            \n",
    "        total_hits += genre_hits\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ SUMMARY:\")\n",
    "    print(f\"Total hits across all genres: {total_hits}\")\n",
    "    print(f\"Genres with matches: {len([g for g in results if results[g]])}\")\n",
    "    \n",
    "    return dict(results)\n",
    "\n",
    "# Helper function for frequency analysis\n",
    "def keyword_frequency_analysis(keyword, db_dict, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Analyze frequency of keyword across genres\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ“Š Frequency Analysis for '{keyword}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    freq_data = {}\n",
    "    \n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    for genre, df in db_dict.items():\n",
    "        total_words = 0\n",
    "        keyword_count = 0\n",
    "        \n",
    "        for text in df['text']:\n",
    "            text_str = str(text)\n",
    "            words = text_str.split()\n",
    "            total_words += len(words)\n",
    "            keyword_count += len(pattern.findall(text_str))\n",
    "        \n",
    "        # Calculate frequency per 1000 words\n",
    "        freq_per_1000 = (keyword_count / total_words * 1000) if total_words > 0 else 0\n",
    "        \n",
    "        freq_data[genre] = {\n",
    "            'count': keyword_count,\n",
    "            'total_words': total_words,\n",
    "            'freq_per_1000': round(freq_per_1000, 3)\n",
    "        }\n",
    "        \n",
    "        print(f\"{genre:8s}: {keyword_count:4d} occurrences | {freq_per_1000:6.3f} per 1000 words\")\n",
    "    \n",
    "    return freq_data\n",
    "\n",
    "print(\"âœ… Search functions created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65513ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Search for a legal term across all genres\n",
    "keyword = \"textual\"\n",
    "search_results = search_keyword_corpus(keyword, db_text, case_sensitive=False, show_context=True, context_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf234ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Frequency analysis across genres\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "freq_results = keyword_frequency_analysis(keyword, db_text, case_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82238f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_collocates(keyword, db_dict, window_size=5, min_freq=2, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Find words that frequently appear near the keyword (collocates)\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: Target word to find collocates for\n",
    "    - db_dict: Dictionary of DataFrames\n",
    "    - window_size: Number of words to look at on each side\n",
    "    - min_freq: Minimum frequency for a word to be considered a collocate\n",
    "    - case_sensitive: Whether to perform case-sensitive search\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”— Collocate Analysis for '{keyword}' (window: Â±{window_size} words)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    all_collocates = Counter()\n",
    "    genre_collocates = {}\n",
    "\n",
    "    for genre, df in db_dict.items():\n",
    "        print(f\"\\nðŸ“š {genre.upper()} Genre Collocates:\")\n",
    "        \n",
    "        # Create a fresh counter for each genre\n",
    "        genre_counter = Counter()\n",
    "        keyword_instances = 0\n",
    "        \n",
    "        for text in df['text']:\n",
    "            text_str = str(text).lower() if not case_sensitive else str(text)\n",
    "            words = nltk.word_tokenize(text_str)\n",
    "            \n",
    "            # Find all positions of the keyword\n",
    "            keyword_positions = []\n",
    "            for i, word in enumerate(words):\n",
    "                if (not case_sensitive and word.lower() == keyword.lower()) or (case_sensitive and word == keyword):\n",
    "                    keyword_positions.append(i)\n",
    "            \n",
    "            keyword_instances += len(keyword_positions)\n",
    "            \n",
    "            # Extract collocates around each keyword occurrence\n",
    "            for pos in keyword_positions:\n",
    "                start = max(0, pos - window_size)\n",
    "                end = min(len(words), pos + window_size + 1)\n",
    "                \n",
    "                # Get surrounding words (excluding the keyword itself)\n",
    "                context_words = words[start:pos] + words[pos+1:end]\n",
    "                \n",
    "                # Filter out punctuation and very short words\n",
    "                context_words = [w for w in context_words if w.isalpha() and len(w) > 2]\n",
    "                \n",
    "                genre_counter.update(context_words)\n",
    "                all_collocates.update(context_words)\n",
    "        \n",
    "        # Store the results for this genre\n",
    "        genre_collocates[genre] = genre_counter\n",
    "        \n",
    "        # Display top collocates for this genre\n",
    "        top_collocates = genre_counter.most_common(10)\n",
    "        if top_collocates:\n",
    "            print(f\"  Found {keyword_instances} instances of '{keyword}' in {genre}\")\n",
    "            # Show all results, but mark those below min_freq\n",
    "            for word, freq in top_collocates:\n",
    "                marker = \"  \" if freq >= min_freq else \"* \"\n",
    "                print(f\"{marker}{word:15s}: {freq:3d} times\")\n",
    "        else:\n",
    "            print(f\"  Found {keyword_instances} instances, but no significant collocates\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ TOP OVERALL COLLOCATES (min frequency: {min_freq}):\")\n",
    "    print(\"-\" * 40)\n",
    "    top_overall = all_collocates.most_common(20)\n",
    "    for word, freq in top_overall:\n",
    "        if freq >= min_freq:\n",
    "            print(f\"{word:15s}: {freq:3d} occurrences\")\n",
    "        \n",
    "    return {\n",
    "        'all_collocates': dict(all_collocates),\n",
    "        'by_genre': dict(genre_collocates),\n",
    "        #'top_overall': top_overall\n",
    "    }\n",
    "\n",
    "# Example 3: Find collocates for the keyword (CLEANED VERSION)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ”§ CLEANED COLLOCATE ANALYSIS:\")\n",
    "collocate_results = find_collocates('help', db_text, window_size=3, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74c346",
   "metadata": {},
   "source": [
    "### think about how it's helpful to have these various results displayed / consumed by the user\n",
    "\n",
    "The `got3` tool should have the following included steps for ease of access with working with COCA from BYU\n",
    "\n",
    "1. read the database files `got3.read_corpora(dir_of_text_files,corpora_name)`\n",
    "2. perform collocate analysis using `got3.find_collocates(keyword, db_dict, window_size, min_freq, case_sensitive)`\n",
    "3. perform keyword search using `got3.search_keyword_corpus(keyword, db_dict, case_sensitive, show_context, context_words)`\n",
    "4. perform keyword frequency analysis using `got3.keyword_frequency_analysis(keyword, db_dict, case_sensitive)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca4830",
   "metadata": {},
   "source": [
    "### Including BERT for Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7430da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2169e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model for embeddings\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "def bert_tokenize_corpus(corpus_data, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize corpus texts using BERT tokenizer\n",
    "    \n",
    "    Parameters:\n",
    "    - corpus_data: pandas Series or dict containing text data\n",
    "    - max_length: maximum sequence length for BERT\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with tokenized inputs\n",
    "    \"\"\"\n",
    "    tokenized_corpus = {}\n",
    "    \n",
    "    # Handle pandas Series\n",
    "    if hasattr(corpus_data, 'items'):\n",
    "        items = corpus_data.items()\n",
    "    else:\n",
    "        items = enumerate(corpus_data)\n",
    "    \n",
    "    print(f\"ðŸ¤— Tokenizing corpus with BERT (max_length={max_length})...\")\n",
    "    \n",
    "    for doc_id, text in items:\n",
    "        if pd.isna(text) or str(text).strip() == '':\n",
    "            continue\n",
    "            \n",
    "        text_str = str(text)\n",
    "        \n",
    "        # Tokenize with BERT\n",
    "        inputs = tokenizer(\n",
    "            text_str, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=max_length, \n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        tokenized_corpus[doc_id] = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'original_text': text_str[:200] + \"...\" if len(text_str) > 200 else text_str\n",
    "        }\n",
    "    \n",
    "    print(f\"âœ… Tokenized {len(tokenized_corpus)} documents\")\n",
    "    return tokenized_corpus\n",
    "\n",
    "# Tokenize the academic corpus\n",
    "print(\"ðŸŽ¯ Tokenizing Academic Corpus...\")\n",
    "bert_corpus = bert_tokenize_corpus(db_text['acad']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e73b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(tokenized_corpus, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for tokenized corpus\n",
    "    \n",
    "    Parameters:\n",
    "    - tokenized_corpus: Output from bert_tokenize_corpus\n",
    "    - batch_size: Number of documents to process at once\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with document embeddings\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    doc_ids = list(tokenized_corpus.keys())\n",
    "    \n",
    "    print(f\"ðŸ§  Generating BERT embeddings for {len(doc_ids)} documents...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(doc_ids), batch_size):\n",
    "            batch_ids = doc_ids[i:i+batch_size]\n",
    "            \n",
    "            # Prepare batch\n",
    "            input_ids_batch = torch.cat([tokenized_corpus[doc_id]['input_ids'] \n",
    "                                       for doc_id in batch_ids], dim=0)\n",
    "            attention_masks_batch = torch.cat([tokenized_corpus[doc_id]['attention_mask'] \n",
    "                                             for doc_id in batch_ids], dim=0)\n",
    "            \n",
    "            # Get BERT outputs\n",
    "            outputs = model(input_ids=input_ids_batch, attention_mask=attention_masks_batch)\n",
    "            \n",
    "            # Use [CLS] token embedding as document representation\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: [batch_size, 768]\n",
    "            \n",
    "            # Store embeddings\n",
    "            for j, doc_id in enumerate(batch_ids):\n",
    "                embeddings[doc_id] = {\n",
    "                    'embedding': cls_embeddings[j].numpy(),\n",
    "                    'original_text': tokenized_corpus[doc_id]['original_text']\n",
    "                }\n",
    "            \n",
    "            print(f\"  âœ… Processed batch {i//batch_size + 1}/{(len(doc_ids)-1)//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Generated embeddings for {len(embeddings)} documents\")\n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings for a sample of the corpus (first 10 documents for demo)\n",
    "sample_corpus = {k: v for k, v in list(bert_corpus.items())[:10]}\n",
    "bert_embeddings = generate_bert_embeddings(sample_corpus, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eff267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_documents(query_text, bert_embeddings, tokenizer, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Find documents most similar to a query text using BERT embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    - query_text: Text to find similar documents for\n",
    "    - bert_embeddings: Dictionary of document embeddings\n",
    "    - tokenizer: BERT tokenizer\n",
    "    - model: BERT model\n",
    "    - top_k: Number of most similar documents to return\n",
    "    \n",
    "    Returns:\n",
    "    - List of tuples (doc_id, similarity_score, text_preview)\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Finding documents similar to: '{query_text[:100]}...'\")\n",
    "    \n",
    "    # Generate embedding for query text\n",
    "    with torch.no_grad():\n",
    "        query_inputs = tokenizer(\n",
    "            query_text, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=512, \n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        query_outputs = model(input_ids=query_inputs['input_ids'], \n",
    "                            attention_mask=query_inputs['attention_mask'])\n",
    "        query_embedding = query_outputs.last_hidden_state[:, 0, :].numpy()[0]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for doc_id, doc_data in bert_embeddings.items():\n",
    "        doc_embedding = doc_data['embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = 1 - cosine(query_embedding, doc_embedding)\n",
    "        similarities.append((doc_id, similarity, doc_data['original_text']))\n",
    "    \n",
    "    # Sort by similarity and return top k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"ðŸ“Š Top {top_k} most similar documents:\")\n",
    "    for i, (doc_id, sim_score, text_preview) in enumerate(similarities[:top_k]):\n",
    "        print(f\"  {i+1}. Doc {doc_id} (similarity: {sim_score:.4f})\")\n",
    "        print(f\"     Preview: {text_preview[:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "def bert_based_keyword_context(keyword, bert_embeddings, tokenizer, model, context_window=50):\n",
    "    \"\"\"\n",
    "    Find documents containing a keyword and analyze their BERT-based semantic context\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: Keyword to search for\n",
    "    - bert_embeddings: Dictionary of document embeddings  \n",
    "    - tokenizer: BERT tokenizer\n",
    "    - model: BERT model\n",
    "    - context_window: Characters around keyword to show\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with keyword contexts and their embeddings\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”Ž BERT-based keyword analysis for: '{keyword}'\")\n",
    "    \n",
    "    keyword_contexts = {}\n",
    "    \n",
    "    for doc_id, doc_data in bert_embeddings.items():\n",
    "        text = doc_data['original_text']\n",
    "        \n",
    "        # Find keyword occurrences\n",
    "        text_lower = text.lower()\n",
    "        keyword_lower = keyword.lower()\n",
    "        \n",
    "        if keyword_lower in text_lower:\n",
    "            # Find all occurrences\n",
    "            start = 0\n",
    "            occurrences = []\n",
    "            \n",
    "            while True:\n",
    "                pos = text_lower.find(keyword_lower, start)\n",
    "                if pos == -1:\n",
    "                    break\n",
    "                \n",
    "                # Extract context\n",
    "                context_start = max(0, pos - context_window)\n",
    "                context_end = min(len(text), pos + len(keyword) + context_window)\n",
    "                context = text[context_start:context_end]\n",
    "                \n",
    "                occurrences.append({\n",
    "                    'position': pos,\n",
    "                    'context': context,\n",
    "                    'keyword_start': pos - context_start,\n",
    "                    'keyword_end': pos - context_start + len(keyword)\n",
    "                })\n",
    "                \n",
    "                start = pos + 1\n",
    "            \n",
    "            if occurrences:\n",
    "                keyword_contexts[doc_id] = {\n",
    "                    'embedding': doc_data['embedding'],\n",
    "                    'occurrences': occurrences,\n",
    "                    'full_text': text\n",
    "                }\n",
    "    \n",
    "    print(f\"ðŸ“ˆ Found '{keyword}' in {len(keyword_contexts)} documents\")\n",
    "    \n",
    "    # Show context examples\n",
    "    for doc_id, data in list(keyword_contexts.items())[:3]:  # Show first 3 examples\n",
    "        print(f\"\\nðŸ“ Document {doc_id}:\")\n",
    "        for i, occ in enumerate(data['occurrences'][:2]):  # Show first 2 occurrences per doc\n",
    "            context = occ['context']\n",
    "            start, end = occ['keyword_start'], occ['keyword_end']\n",
    "            highlighted = context[:start] + f\"**{context[start:end]}**\" + context[end:]\n",
    "            print(f\"   Context {i+1}: ...{highlighted}...\")\n",
    "    \n",
    "    return keyword_contexts\n",
    "\n",
    "# Example usage: Find documents similar to a legal query\n",
    "query = \"get out of text free card\"\n",
    "similar_docs = find_similar_documents(query, bert_embeddings, tokenizer, model, top_k=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example usage: BERT-based keyword analysis\n",
    "keyword_analysis = bert_based_keyword_context(\"help\", bert_embeddings, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def bert_document_clustering(bert_embeddings, n_clusters=3, visualize=True):\n",
    "    \"\"\"\n",
    "    Cluster documents based on BERT embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    - bert_embeddings: Dictionary of document embeddings\n",
    "    - n_clusters: Number of clusters to create\n",
    "    - visualize: Whether to create a visualization\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with cluster assignments and analysis\n",
    "    \"\"\"\n",
    "    print(f\"ðŸŽ¯ Clustering {len(bert_embeddings)} documents into {n_clusters} clusters using BERT embeddings...\")\n",
    "    \n",
    "    # Prepare embeddings matrix\n",
    "    doc_ids = list(bert_embeddings.keys())\n",
    "    embeddings_matrix = np.array([bert_embeddings[doc_id]['embedding'] for doc_id in doc_ids])\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings_matrix)\n",
    "    \n",
    "    # Organize results by cluster\n",
    "    clusters = {}\n",
    "    for i in range(n_clusters):\n",
    "        clusters[f'Cluster_{i}'] = []\n",
    "    \n",
    "    for doc_id, cluster_label in zip(doc_ids, cluster_labels):\n",
    "        cluster_name = f'Cluster_{cluster_label}'\n",
    "        clusters[cluster_name].append({\n",
    "            'doc_id': doc_id,\n",
    "            'text_preview': bert_embeddings[doc_id]['original_text'][:100] + \"...\"\n",
    "        })\n",
    "    \n",
    "    # Display cluster information\n",
    "    print(\"\\nðŸ“Š CLUSTER ANALYSIS RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for cluster_name, docs in clusters.items():\n",
    "        print(f\"\\nðŸ—‚ï¸  {cluster_name} ({len(docs)} documents):\")\n",
    "        for doc in docs[:3]:  # Show first 3 documents per cluster\n",
    "            print(f\"   â€¢ Doc {doc['doc_id']}: {doc['text_preview']}\")\n",
    "        if len(docs) > 3:\n",
    "            print(f\"   ... and {len(docs) - 3} more documents\")\n",
    "    \n",
    "    # Visualization\n",
    "    if visualize and len(bert_embeddings) > 2:\n",
    "        print(\"\\nðŸ“ˆ Creating visualization...\")\n",
    "        \n",
    "        # Reduce dimensionality for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings_matrix)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            cluster_mask = cluster_labels == i\n",
    "            plt.scatter(embeddings_2d[cluster_mask, 0], \n",
    "                       embeddings_2d[cluster_mask, 1], \n",
    "                       c=colors[i % len(colors)], \n",
    "                       label=f'Cluster {i}',\n",
    "                       alpha=0.7)\n",
    "        \n",
    "        plt.title('BERT-based Document Clustering (PCA Visualization)')\n",
    "        plt.xlabel(f'First Principal Component (explains {pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        plt.ylabel(f'Second Principal Component (explains {pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'clusters': clusters,\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'doc_ids': doc_ids,\n",
    "        'embeddings_matrix': embeddings_matrix,\n",
    "        'kmeans_model': kmeans\n",
    "    }\n",
    "\n",
    "def bert_semantic_search_legal_concepts(bert_embeddings, legal_concepts, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Search for documents related to specific legal concepts using BERT semantic similarity\n",
    "    \n",
    "    Parameters:\n",
    "    - bert_embeddings: Dictionary of document embeddings\n",
    "    - legal_concepts: List of legal concept queries\n",
    "    - tokenizer: BERT tokenizer\n",
    "    - model: BERT model\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping concepts to most relevant documents\n",
    "    \"\"\"\n",
    "    concept_matches = {}\n",
    "    \n",
    "    print(\"âš–ï¸  LEGAL CONCEPT SEMANTIC SEARCH\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for concept in legal_concepts:\n",
    "        print(f\"\\nðŸ” Searching for: '{concept}'\")\n",
    "        \n",
    "        # Find similar documents for this concept\n",
    "        similar_docs = find_similar_documents(\n",
    "            concept, bert_embeddings, tokenizer, model, top_k=3\n",
    "        )\n",
    "        \n",
    "        concept_matches[concept] = similar_docs\n",
    "    \n",
    "    return concept_matches\n",
    "\n",
    "# Example usage: Cluster the documents\n",
    "clustering_results = bert_document_clustering(bert_embeddings, n_clusters=3, visualize=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example usage: Semantic search for legal concepts\n",
    "legal_concepts = [\n",
    "    \"constitutional rights and amendments\",\n",
    "    \"judicial interpretation and precedent\", \n",
    "    \"legal procedure and court process\"\n",
    "]\n",
    "\n",
    "concept_search_results = bert_semantic_search_legal_concepts(\n",
    "    bert_embeddings, legal_concepts, tokenizer, model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec428d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integration with got3 toolkit for enhanced legal corpus analysis\n",
    "\n",
    "def create_bert_enhanced_corpus_analysis(corpus_data, keywords, legal_concepts):\n",
    "    \"\"\"\n",
    "    Combine traditional got3 analysis with BERT-based semantic analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - corpus_data: COCA corpus data from got3.read_corpora()\n",
    "    - keywords: List of keywords for traditional analysis\n",
    "    - legal_concepts: List of legal concepts for BERT semantic analysis\n",
    "    \n",
    "    Returns:\n",
    "    - Comprehensive analysis results combining both approaches\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'traditional_analysis': {},\n",
    "        'bert_analysis': {},\n",
    "        'combined_insights': {}\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸŽ¯ COMPREHENSIVE LEGAL CORPUS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Combining traditional got3 methods with BERT semantic analysis...\")\n",
    "    \n",
    "    # Traditional got3 analysis\n",
    "    print(\"\\nðŸ“š TRADITIONAL ANALYSIS (got3 methods):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        print(f\"\\nðŸ” Analyzing keyword: '{keyword}'\")\n",
    "        \n",
    "        # Traditional keyword search\n",
    "        search_results = search_keyword_corpus(keyword, corpus_data, show_context=False)\n",
    "        \n",
    "        # Frequency analysis  \n",
    "        freq_results = keyword_frequency_analysis(keyword, corpus_data)\n",
    "        \n",
    "        # Collocate analysis\n",
    "        collocate_results = find_collocates(keyword, corpus_data, window_size=3, min_freq=1)\n",
    "        \n",
    "        results['traditional_analysis'][keyword] = {\n",
    "            'search_results': search_results,\n",
    "            'frequency_results': freq_results,\n",
    "            'collocate_results': collocate_results\n",
    "        }\n",
    "    \n",
    "    # BERT-based analysis on a sample\n",
    "    print(f\"\\nðŸ§  BERT SEMANTIC ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Use academic corpus for BERT analysis (sample)\n",
    "    sample_texts = corpus_data\n",
    "    \n",
    "    # Tokenize and generate embeddings\n",
    "    bert_tokenized = bert_tokenize_corpus(sample_texts, max_length=256)\n",
    "    bert_embeds = generate_bert_embeddings(bert_tokenized, batch_size=4)\n",
    "    \n",
    "    # Semantic search for legal concepts\n",
    "    for concept in legal_concepts:\n",
    "        print(f\"\\nâš–ï¸  Semantic search for: '{concept}'\")\n",
    "        similar_docs = find_similar_documents(concept, bert_embeds, tokenizer, model, top_k=3)\n",
    "        results['bert_analysis'][concept] = similar_docs\n",
    "    \n",
    "    # Document clustering\n",
    "    print(f\"\\nðŸ—‚ï¸  Document clustering:\")\n",
    "    clustering = bert_document_clustering(bert_embeds, n_clusters=3, visualize=False)\n",
    "    results['bert_analysis']['clustering'] = clustering\n",
    "    \n",
    "    # Combined insights\n",
    "    print(f\"\\nðŸ’¡ COMBINED INSIGHTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Compare traditional keyword frequency with BERT semantic clusters\n",
    "    for keyword in keywords:\n",
    "        freq_data = results['traditional_analysis'][keyword]['frequency_results']\n",
    "        total_occurrences = sum(genre_data['count'] for genre_data in freq_data.values())\n",
    "        \n",
    "        if total_occurrences > 0:\n",
    "            insights.append(f\"'{keyword}' appears {total_occurrences} times across all genres\")\n",
    "        \n",
    "        # Top collocates\n",
    "        collocates = results['traditional_analysis'][keyword]['collocate_results']['all_collocates']\n",
    "        top_collocates = sorted(collocates.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        if top_collocates:\n",
    "            collocate_words = [word for word, count in top_collocates]\n",
    "            insights.append(f\"'{keyword}' most frequently appears with: {', '.join(collocate_words)}\")\n",
    "    \n",
    "    # BERT clustering insights\n",
    "    cluster_info = results['bert_analysis']['clustering']['clusters']\n",
    "    insights.append(f\"BERT analysis identified {len(cluster_info)} semantic clusters in the corpus\")\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(f\"  â€¢ {insight}\")\n",
    "    \n",
    "    results['combined_insights'] = insights\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Analysis complete! Traditional and BERT methods provide complementary insights.\")\n",
    "    return results\n",
    "\n",
    "# Example: Comprehensive analysis combining got3 and BERT\n",
    "keywords_to_analyze = ['legal', 'constitutional']\n",
    "legal_concepts_to_search = [\n",
    "    'constitutional interpretation and judicial review',\n",
    "    'legal precedent and case law'\n",
    "]\n",
    "\n",
    "comprehensive_results = create_bert_enhanced_corpus_analysis(\n",
    "    db_text, \n",
    "    keywords_to_analyze, \n",
    "    legal_concepts_to_search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b30bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
