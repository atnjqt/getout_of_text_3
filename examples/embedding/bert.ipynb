{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2569b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6ae23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'need', 'to', 'visit', 'the', 'financial', 'bank', 'tomorrow', ';', 'after', 'that', ',', 'we', \"'\", 'll', 'set', 'up', 'a', 'tent', 'by', 'the', 'river', 'bank', ',', 'just', 'across', 'from', 'the', 'bank', 'building', 'where', 'my', 'friend', 'works', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#The sentence\n",
    "#txt = \"I need to visit the bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank where my friend works.\"\n",
    "#txt = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "txt = \"I need to visit the financial bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank building where my friend works.\"\n",
    "\n",
    "#Add the special tokens\n",
    "wrangled_txt = '[CLS] ' + txt + ' [SEP]'\n",
    "\n",
    "#tokenization\n",
    "tokenized_txt = tokenizer.tokenize(wrangled_txt)\n",
    "\n",
    "print(tokenized_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1192bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]             101\n",
      "i               1,045\n",
      "need            2,342\n",
      "to              2,000\n",
      "visit           3,942\n",
      "the             1,996\n",
      "financial       3,361\n",
      "bank            2,924\n",
      "tomorrow        4,826\n",
      ";               1,025\n",
      "after           2,044\n",
      "that            2,008\n",
      ",               1,010\n",
      "we              2,057\n",
      "'               1,005\n",
      "ll              2,222\n",
      "set             2,275\n",
      "up              2,039\n",
      "a               1,037\n",
      "tent            9,311\n",
      "by              2,011\n",
      "the             1,996\n",
      "river           2,314\n",
      "bank            2,924\n",
      ",               1,010\n",
      "just            2,074\n",
      "across          2,408\n",
      "from            2,013\n",
      "the             1,996\n",
      "bank            2,924\n",
      "building        2,311\n",
      "where           2,073\n",
      "my              2,026\n",
      "friend          2,767\n",
      "works           2,573\n",
      ".               1,012\n",
      "[SEP]             102\n"
     ]
    }
   ],
   "source": [
    "#get the ids of the tokens\n",
    "ids_tokens = tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "\n",
    "#Display the tokens\n",
    "for t in zip(tokenized_txt, ids_tokens):\n",
    "    print('{:<12} {:>8,}'.format(t[0], t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c9cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_ids = [1] * len(tokenized_txt)\n",
    "#Convert the token IDs and segment IDs into tensors.\n",
    "\n",
    "token_tensor = torch.tensor([ids_tokens])\n",
    "segment_tensor = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07b174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model with the weights\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True, return_dict = False)\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8844b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/model_doc/bert#bertmodel\n",
    "#The input is of the shape (batch_size, sequence_length)\n",
    "#Compute the output\n",
    "with torch.no_grad():\n",
    "    outputs = model(token_tensor, segment_tensor)\n",
    "hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b3ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 1\n",
      "Number of tokens: 37\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "#The first one is initial embeddings\n",
    "print (\"Number of layers:\", len(hidden_states))\n",
    "layer_ptr = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_ptr]))\n",
    "batch_ptr = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_ptr][batch_ptr]))\n",
    "token_ptr = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_ptr][batch_ptr][token_ptr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7626f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 37, 768])\n"
     ]
    }
   ],
   "source": [
    "#Concatenate all the layers\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "#remove the batch dimension\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b2689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 13, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1 so that each word contains the 13 layer hidden states\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5868f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 37 x 768\n"
     ]
    }
   ],
   "source": [
    "#sum the last four layers\n",
    "token_vectors_sum = []\n",
    "\n",
    "# token_embeddings is a [35 x 13 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vector = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vectors_sum.append(sum_vector)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vectors_sum), len(token_vectors_sum[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b9a7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 i\n",
      "2 need\n",
      "3 to\n",
      "4 visit\n",
      "5 the\n",
      "6 financial\n",
      "7 bank\n",
      "8 tomorrow\n",
      "9 ;\n",
      "10 after\n",
      "11 that\n",
      "12 ,\n",
      "13 we\n",
      "14 '\n",
      "15 ll\n",
      "16 set\n",
      "17 up\n",
      "18 a\n",
      "19 tent\n",
      "20 by\n",
      "21 the\n",
      "22 river\n",
      "23 bank\n",
      "24 ,\n",
      "25 just\n",
      "26 across\n",
      "27 from\n",
      "28 the\n",
      "29 bank\n",
      "30 building\n",
      "31 where\n",
      "32 my\n",
      "33 friend\n",
      "34 works\n",
      "35 .\n",
      "36 [SEP]\n"
     ]
    }
   ],
   "source": [
    "#Display the token\n",
    "for i, t in enumerate(tokenized_txt):\n",
    "  print (i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f837710",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectors = torch.stack(token_vectors_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823ed9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.78\n",
      "Vector similarity for *different* meanings:  0.66\n",
      "Vector similarity for *different* meanings:  0.68\n"
     ]
    }
   ],
   "source": [
    "#compare the word bank in 7, 23, and 29\n",
    "#txt = \"I need to visit the financial bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank building where my friend works.\"\n",
    "\n",
    "same_bank_word = 1 - cosine(token_vectors[7], token_vectors[29])\n",
    "diff_bank_word1 = 1 - cosine(token_vectors[7], token_vectors[23])\n",
    "diff_bank_word2 = 1 - cosine(token_vectors[23], token_vectors[29])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank_word)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word1)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55b5f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_get_context_in_statue(txt, model):\n",
    "    wrangled_txt = '[CLS] ' + txt + ' [SEP]'\n",
    "    tokenized_txt = tokenizer.tokenize(wrangled_txt)\n",
    "    ids_tokens = tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "    segments_ids = [1] * len(tokenized_txt)\n",
    "    #Convert the token IDs and segment IDs into tensors.\n",
    "\n",
    "    token_tensor = torch.tensor([ids_tokens])\n",
    "    segment_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_tensor, segment_tensor)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    #Concatenate all the layers\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    #remove the batch dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    print(token_embeddings.shape)\n",
    "\n",
    "    # Swap dimensions 0 and 1 so that each word contains the 13 layer hidden states\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_embeddings.size()\n",
    "\n",
    "    #sum the last four layers\n",
    "    token_vectors_sum = []\n",
    "\n",
    "    # token_embeddings is a [35 x 13 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vector = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vectors_sum.append(sum_vector)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vectors_sum), len(token_vectors_sum[0])))\n",
    "\n",
    "    return(torch.stack(token_vectors_sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the word bank in 7, 23, and 29\n",
    "#txt = \"I need to visit the financial bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank building where my friend works.\"\n",
    "\n",
    "same_bank_word = 1 - cosine(token_vectors[7], token_vectors[29])\n",
    "diff_bank_word1 = 1 - cosine(token_vectors[7], token_vectors[23])\n",
    "diff_bank_word2 = 1 - cosine(token_vectors[23], token_vectors[29])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank_word)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word1)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9ff3c",
   "metadata": {},
   "source": [
    "## What about the language of the Clean Art Act (thinking back to Chevron)\n",
    "\n",
    "- https://www.law.cornell.edu/uscode/text/42/7411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "761be3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "42 U.S. Code § 7411 - Standards of performance for new stationary source\n",
      "(a)Definitions\n",
      "For purposes of this section:\n",
      "(1)The term standard of performance means a standard for emissions of air pollutants which reflects the degree of emission limitation achievable through the application of the best system of emission reduction which (taking into account the cost of achieving such reduction and any nonair quality health and environmental impact and energy requirements) the Administrator determines has been adequately demonstrated.\n",
      "(2)The term new source means any stationary source the construction or modification of which is commenced after the publication of regulations (or if earlier proposed regulations) prescribing a standard of performance under this section which will be applicable to such source.\n",
      "(3)The term stationary source means any building structure facility or installation which emits or may emit any air pollutant. Nothing in subchapter II of this chapter relating to nonroad engines shall be construed to apply to stationary internal combustion engines.\n",
      "(4)The term modification means any physical change in or change in the method of operation of a stationary source which increases the amount of any air pollutant emitted by such source or which results in the emission of any air pollutant not previously emitted.\n",
      "(5)The term owner or operator means any person who owns leases operates controls or supervises a stationary source.\n",
      "(6)The term existing source means any stationary source other than a new source.\n",
      "(7)The term technological system of continuous emission reduction means—\n",
      "(A)a technological process for production or operation by any source which is inherently low-polluting or nonpolluting or\n",
      "(B)a technological system for continuous reduction of the pollution generated by a source before such pollution is emitted into the ambient air including precombustion cleaning or treatment of fuels.\n",
      "(8)A conversion to coal (A) by reason of an order under section 2(a) of the Energy Supply and Environmental Coordination Act of 1974 [15 U.S.C. 792(a)] or any amendment thereto or any subsequent enactment which supersedes such Act [15 U.S.C. 791 et seq.] or (B) which qualifies under section 7413(d)(5)(A)(ii)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uscode_42_7411='''\n",
    "42 U.S. Code § 7411 - Standards of performance for new stationary sources\n",
    "(a)Definitions\n",
    "For purposes of this section:\n",
    "(1)The term “standard of performance” means a standard for emissions of air pollutants which reflects the degree of emission limitation achievable through the application of the best system of emission reduction which (taking into account the cost of achieving such reduction and any nonair quality health and environmental impact and energy requirements) the Administrator determines has been adequately demonstrated.\n",
    "(2)The term “new source” means any stationary source, the construction or modification of which is commenced after the publication of regulations (or, if earlier, proposed regulations) prescribing a standard of performance under this section which will be applicable to such source.\n",
    "(3)The term “stationary source” means any building, structure, facility, or installation which emits or may emit any air pollutant. Nothing in subchapter II of this chapter relating to nonroad engines shall be construed to apply to stationary internal combustion engines.\n",
    "(4)The term “modification” means any physical change in, or change in the method of operation of, a stationary source which increases the amount of any air pollutant emitted by such source or which results in the emission of any air pollutant not previously emitted.\n",
    "(5)The term “owner or operator” means any person who owns, leases, operates, controls, or supervises a stationary source.\n",
    "(6)The term “existing source” means any stationary source other than a new source.\n",
    "(7)The term “technological system of continuous emission reduction” means—\n",
    "(A)a technological process for production or operation by any source which is inherently low-polluting or nonpolluting, or\n",
    "(B)a technological system for continuous reduction of the pollution generated by a source before such pollution is emitted into the ambient air, including precombustion cleaning or treatment of fuels.\n",
    "(8)A conversion to coal (A) by reason of an order under section 2(a) of the Energy Supply and Environmental Coordination Act of 1974 [15 U.S.C. 792(a)] or any amendment thereto, or any subsequent enactment which supersedes such Act [15 U.S.C. 791 et seq.], or (B) which qualifies under section 7413(d)(5)(A)(ii)\n",
    "'''\n",
    "# clean up text to get more hits\n",
    "uscode_42_7411=uscode_42_7411.replace('“','')\n",
    "uscode_42_7411=uscode_42_7411.replace('”','')\n",
    "uscode_42_7411=uscode_42_7411.replace(',','')\n",
    "uscode_42_7411=uscode_42_7411.replace('sources','source')\n",
    "\n",
    "print(uscode_42_7411)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ee86c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 472, 768])\n",
      "Shape is: 472 x 768\n"
     ]
    }
   ],
   "source": [
    "cleanairact_token_vectors = bert_get_context_in_statue(uscode_42_7411, \n",
    "                                                       BertModel.from_pretrained('bert-base-uncased', \n",
    "                                                                                 output_hidden_states = True, \n",
    "                                                                                 return_dict = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "63f2d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_term='source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b3d031e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 81, 85, 122, 177, 189, 224, 228, 252, 272]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay I probably want to get the index of each occurrence of the ambiguous term\n",
    "ambiguous_term = \"source\"\n",
    "ambiguous_term_indexes = [i for i, token in enumerate(uscode_42_7411.split()) if token == ambiguous_term]\n",
    "ambiguous_term_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a903501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.34\n",
      "... (2)The term new source means any stationary ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.36\n",
      "... means any stationary source the construction or ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.33\n",
      "... (3)The term stationary source means any building ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.32\n",
      "... of a stationary source which increases the ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.20\n",
      "... emitted by such source or which results ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.40\n",
      "... (6)The term existing source means any stationary ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.30\n",
      "... means any stationary source other than a ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.27\n",
      "... operation by any source which is inherently ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.32\n",
      "... generated by a source before such pollution ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply cosine similarity to find similar meanings for the first index relative to each subsequent. i.e. the title to the statute compared to the language\n",
    "#[12, 81, 85, 122, 177, 189, 224, 228, 252, 272]\n",
    "\n",
    "for i in [12, 81, 85, 122, 177, 189, 224, 228, 252, 272]:\n",
    "    if i == 12:\n",
    "        continue\n",
    "    same_source_word = 1 - cosine(cleanairact_token_vectors[12], cleanairact_token_vectors[i])\n",
    "    print('Vector similarity for  *similar*  meanings:  %.2f' % same_source_word)\n",
    "    # print three tokens before and after each index for the term in a single line, starting and ending with ...\n",
    "    tokens = uscode_42_7411.split()\n",
    "    start = max(0, i - 3)\n",
    "    end = min(len(tokens), i + 4)\n",
    "    context = ' '.join(tokens[start:end])\n",
    "    print(f\"... {context} ...\")\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f22a8",
   "metadata": {},
   "source": [
    "## Using AWS Bedrock with DeepSeek\n",
    "\n",
    "- passing the prompt to the model for a similar AI LLM based task\n",
    "- this compares later to the bert pipeline for filling in the blanks\n",
    "- is really just exploratory on how embedding models work, and how they along with some AI techniques can be used for various NLP tasks in `getout-of-text3`\n",
    "\n",
    "### examples\n",
    "\n",
    "- In finding the ordinary meaning of words, namely the ambiguous text of importance in a statutory interpretation that is up for debate, there are various techniques we can employ to disambiguate the text and extract its intended meaning, including tradition KWIC (COCA), Embedding (LEGAL-BERT), and AI LLMs (DeepSeek on AWS Bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "83bfcd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name='atn-developer')\n",
    "\n",
    "bedrock = session.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "response = bedrock.invoke_model(\n",
    "    modelId=\"us.deepseek.r1-v1:0\",\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\",\n",
    "    body='{\"prompt\": \"Please analyze the masked sentence to fill the mask: \\\\\"To modify means we should [MASK] significant changes.\\\\\"\", \"max_tokens\": 256}',\n",
    ")\n",
    "\n",
    "deepseek = response['body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "835e78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bedrock_summary(ambiguous_term,statutory_text,bedrock_session):\n",
    "    prompt_text=\"You are an AI LLM assistant who is knowledgeable about legal statutes.\\\n",
    "         Your task is to provide a summary of the statutory text related to the ambiguous term.\\\n",
    "         Please ensure that your summary is clear and concise. \\\n",
    "         The goal of course is to identify clear and ordinary meaning of the term, in the statutory context, without any bias.\\\n",
    "         Subsequent analysis will perform NLP tasks to further extract the ordinary meaning, but this is an AI LLM assistant's interpretation for reference. \\\n",
    "         MAX_RESPONSE_TOKENS = 512\\\n",
    "         Term is: {}\\\n",
    "         Statutory text is: {}\".format(ambiguous_term, statutory_text)\n",
    "\n",
    "    print(prompt_text)\n",
    "\n",
    "    print('Passing to bedrock...')\n",
    "\n",
    "    # Convert the body dictionary to a JSON string and encode it\n",
    "    body = json.dumps({\n",
    "        \"prompt\": prompt_text,\n",
    "        \"max_tokens\": 512\n",
    "    }).encode(\"utf-8\")\n",
    "\n",
    "    response = bedrock_session.invoke_model(\n",
    "        modelId=\"us.deepseek.r1-v1:0\",\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=body\n",
    "    )\n",
    "\n",
    "    return response['body'].read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "964d7c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI LLM assistant who is knowledgeable about legal statutes.         Your task is to provide a summary of the statutory text related to the ambiguous term.         Please ensure that your summary is clear and concise.          The goal of course is to identify clear and ordinary meaning of the term, in the statutory context, without any bias.         Subsequent analysis will perform NLP tasks to further extract the ordinary meaning, but this is an AI LLM assistant's interpretation for reference.          MAX_RESPONSE_TOKENS = 512         Term is: source         Statutory text is: \n",
      "42 U.S. Code § 7411 - Standards of performance for new stationary source\n",
      "(a)Definitions\n",
      "For purposes of this section:\n",
      "(1)The term standard of performance means a standard for emissions of air pollutants which reflects the degree of emission limitation achievable through the application of the best system of emission reduction which (taking into account the cost of achieving such reduction and any nonair quality health and environmental impact and energy requirements) the Administrator determines has been adequately demonstrated.\n",
      "(2)The term new source means any stationary source the construction or modification of which is commenced after the publication of regulations (or if earlier proposed regulations) prescribing a standard of performance under this section which will be applicable to such source.\n",
      "(3)The term stationary source means any building structure facility or installation which emits or may emit any air pollutant. Nothing in subchapter II of this chapter relating to nonroad engines shall be construed to apply to stationary internal combustion engines.\n",
      "(4)The term modification means any physical change in or change in the method of operation of a stationary source which increases the amount of any air pollutant emitted by such source or which results in the emission of any air pollutant not previously emitted.\n",
      "(5)The term owner or operator means any person who owns leases operates controls or supervises a stationary source.\n",
      "(6)The term existing source means any stationary source other than a new source.\n",
      "(7)The term technological system of continuous emission reduction means—\n",
      "(A)a technological process for production or operation by any source which is inherently low-polluting or nonpolluting or\n",
      "(B)a technological system for continuous reduction of the pollution generated by a source before such pollution is emitted into the ambient air including precombustion cleaning or treatment of fuels.\n",
      "(8)A conversion to coal (A) by reason of an order under section 2(a) of the Energy Supply and Environmental Coordination Act of 1974 [15 U.S.C. 792(a)] or any amendment thereto or any subsequent enactment which supersedes such Act [15 U.S.C. 791 et seq.] or (B) which qualifies under section 7413(d)(5)(A)(ii)\n",
      "\n",
      "Passing to bedrock...\n"
     ]
    }
   ],
   "source": [
    "resp = get_bedrock_summary(ambiguous_term, uscode_42_7411,bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8e34cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</think>\n",
      "\n",
      "Okay, let's tackle this. The user wants a summary of the statutory text related to the ter\n",
      "m \"source\" in 42 U.S. Code § 7411. First, I need to find where \"source\" is defined in the statute. L\n",
      "ooking at the definitions in subsection (a), the term \"stationary source\" is defined in (3). \n",
      "\n",
      "The d\n",
      "efinition says a stationary source is any building, structure, facility, or installation that emits \n",
      "or may emit air pollutants. Also, there's a note that subchapter II about nonroad engines doesn't ap\n",
      "ply to stationary internal combustion engines. So, \"source\" here refers to fixed emitters like build\n",
      "ings or facilities.\n",
      "\n",
      "But wait, the user mentioned the term is \"source,\" but the statute uses \"statio\n",
      "nary source.\" I should check if \"source\" alone is used elsewhere. In (2), \"new source\" is defined as\n",
      " a stationary source constructed or modified after regulations are published. Similarly, \"existing s\n",
      "ource\" in (6) refers to stationary sources that aren't new. So, in this context, \"source\" is shortha\n",
      "nd for \"stationary source\" as defined in (3).\n",
      "\n",
      "The key elements are that a source is a fixed structu\n",
      "re or facility emitting pollutants. Modifications to a source that increase emissions are covered. T\n",
      "he summary should clarify that \"source\" here specifically means stationary sources, not mobile ones,\n",
      " and includes any such structure that emits air pollutants. Also, note exclusions like nonroad engin\n",
      "es not applying to stationary internal combustion engines.\n",
      "\n",
      "I need to make sure the summary is conci\n",
      "se, captures the statutory definition, and avoids bias. Highlight the components: building, structur\n",
      "e, facility, installation; emission of pollutants; exclusion of nonroad engines for stationary inter\n",
      "nal combustion ones. Also, link to how \"new source\" and \"existing source\" are defined based on when \n",
      "construction or modification occurred relative to regulations.\n",
      "\n",
      "Double-check that I'm not missing an\n",
      "y other mentions of \"source\" beyond the definitions. The term \"technological system of continuous em\n",
      "ission reduction\" in (7) relates to emission control but isn't part of the \"source\" definition. The \n",
      "summary should focus solely on the parts defining \"source\" as a stationary emitter.\n",
      "</think>\n",
      "\n",
      "**Summ\n",
      "ary of \"Source\" in 42 U.S.C. § 7411:**  \n",
      "The term **“source”** in this statute refers to a **station\n",
      "ary source**, defined as any *building, structure, facility, or installation* that **emits or may em\n",
      "it air pollutants**\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "deepseek_dict = json.loads(resp.decode())\n",
    "\n",
    "#deepseek_dict['choices'][0]['text']\n",
    "# print with line wrap\n",
    "# I want to print with a line break in the ['text']\n",
    "text = deepseek_dict['choices'][0]['text']\n",
    "for i in range(0, len(text), 100):\n",
    "    print(text[i:i+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b93b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel logic to \n",
    "329 U.S. 64\t\n",
    "\n",
    "# format to https://tile.loc.gov/storage-services/service/ll/usrep/usrep329/usrep32964/usrep32964.pdf\n",
    "https://tile.loc.gov/storage-services/service/ll/usrep/usrep{}/usrep{}{}/usrep{}{}.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8f4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "13b1c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please provide the answer in the format: [Answer] \"answer\".\n",
      "\n",
      "Okay, let's see. The sentence is \"To m\n",
      "odify means we should [MASK] significant changes.\" I need to find the right word to replace [MASK]. \n",
      "The verb here should fit with \"significant changes\" and make sense with \"modify.\" \n",
      "\n",
      "First, \"modify\" \n",
      "means to make partial or minor changes. So the sentence is explaining that modifying involves doing \n",
      "something to significant changes. Wait, that might not make sense. Maybe the sentence is saying that\n",
      " when we modify, we should do something with significant changes. Hmm. Maybe the intended meaning is\n",
      " that modifying requires making significant changes. But \"modify\" usually implies smaller changes. M\n",
      "aybe there's a contradiction here. Or perhaps the sentence is trying to say that to modify something\n",
      ", you need to implement or create significant changes. \n",
      "\n",
      "Let me think of possible verbs. Common coll\n",
      "ocations with \"changes\" include \"make,\" \"implement,\" \"introduce,\" \"create,\" \"bring about,\" \"effect,\"\n",
      " \"undergo.\" Which of these fits best? The structure is \"we should [verb] significant changes.\" \n",
      "\n",
      "\"Ma\n",
      "ke changes\" is very common. \"Implement changes\" is also used,\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "deepseek_dict = json.loads(deepseek.decode())\n",
    "#print(deepseek_dict)\n",
    "\n",
    "#deepseek_dict['choices'][0]['text']\n",
    "# print with line wrap\n",
    "# I want to print with a line break in the ['text']\n",
    "text = deepseek_dict['choices'][0]['text']\n",
    "for i in range(0, len(text), 100):\n",
    "    print(text[i:i+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e2da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d61a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.14109571278095245,\n",
       "  'token': 468,\n",
       "  'token_str': 'make',\n",
       "  'sequence': 'to modify means we should make significant changes'},\n",
       " {'score': 0.1251792460680008,\n",
       "  'token': 321,\n",
       "  'token_str': 'see',\n",
       "  'sequence': 'to modify means we should see significant changes'},\n",
       " {'score': 0.07972630113363266,\n",
       "  'token': 247,\n",
       "  'token_str': 'have',\n",
       "  'sequence': 'to modify means we should have significant changes'},\n",
       " {'score': 0.06908190250396729,\n",
       "  'token': 4908,\n",
       "  'token_str': 'expect',\n",
       "  'sequence': 'to modify means we should expect significant changes'},\n",
       " {'score': 0.058650076389312744,\n",
       "  'token': 594,\n",
       "  'token_str': 'report',\n",
       "  'sequence': 'to modify means we should report significant changes'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"To modify means we should [MASK] significant changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73eac980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1371982991695404,\n",
       "  'token': 4672,\n",
       "  'token_str': 'commodity',\n",
       "  'sequence': 'that bike is a commodity that is not permitted in the park.'},\n",
       " {'score': 0.08565253764390945,\n",
       "  'token': 4175,\n",
       "  'token_str': 'game',\n",
       "  'sequence': 'that bike is a game that is not permitted in the park.'},\n",
       " {'score': 0.07633555680513382,\n",
       "  'token': 424,\n",
       "  'token_str': 'product',\n",
       "  'sequence': 'that bike is a product that is not permitted in the park.'},\n",
       " {'score': 0.0717778131365776,\n",
       "  'token': 446,\n",
       "  'token_str': 'service',\n",
       "  'sequence': 'that bike is a service that is not permitted in the park.'},\n",
       " {'score': 0.062396444380283356,\n",
       "  'token': 1343,\n",
       "  'token_str': 'vehicle',\n",
       "  'sequence': 'that bike is a vehicle that is not permitted in the park.'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"That bike is a [MASK] that is not permitted in the park.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31277aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: \n",
      "Loss: nan\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# Tokenize the input sentence with the mask token\n",
    "inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Retrieve index of <mask>\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Predict the token for <mask>\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "print(\"Predicted token:\", tokenizer.decode(predicted_token_id))\n",
    "\n",
    "# Create labels tensor matching the input shape\n",
    "labels = inputs.input_ids.clone()\n",
    "labels[0, mask_token_index] = tokenizer(\"Paris\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "labels[labels != tokenizer.mask_token_id] = -100  # Mask all tokens except the label\n",
    "\n",
    "# Compute the loss\n",
    "outputs = model(**inputs, labels=labels)\n",
    "print(\"Loss:\", round(outputs.loss.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a320bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9971315860748291}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "result = classifier(\"I love using Hugging Face Transformers!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7998c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForMaskedLM were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.0021086677443236113, 'token': 20211, 'token_str': '##eyer', 'sequence': \"i love using a blueeyer when i'm on the tennis court!\"}, {'score': 0.0018039444694295526, 'token': 13936, 'token_str': '##uded', 'sequence': \"i love using a blueuded when i'm on the tennis court!\"}, {'score': 0.0016544635873287916, 'token': 24960, 'token_str': 'marrow', 'sequence': \"i love using a blue marrow when i'm on the tennis court!\"}, {'score': 0.0015833282377570868, 'token': 29114, 'token_str': 'accountants', 'sequence': \"i love using a blue accountants when i'm on the tennis court!\"}, {'score': 0.0015499064465984702, 'token': 16256, 'token_str': '##duction', 'sequence': \"i love using a blueduction when i'm on the tennis court!\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "result = classifier(\"I love using a blue [MASK] when I'm on the tennis court!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826ef020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5017262101173401,\n",
       "  'token': 468,\n",
       "  'token_str': 'make',\n",
       "  'sequence': 'to modify means we should make significant changes.'},\n",
       " {'score': 0.04991573467850685,\n",
       "  'token': 1262,\n",
       "  'token_str': 'consider',\n",
       "  'sequence': 'to modify means we should consider significant changes.'},\n",
       " {'score': 0.04759223759174347,\n",
       "  'token': 321,\n",
       "  'token_str': 'see',\n",
       "  'sequence': 'to modify means we should see significant changes.'},\n",
       " {'score': 0.044308967888355255,\n",
       "  'token': 4908,\n",
       "  'token_str': 'expect',\n",
       "  'sequence': 'to modify means we should expect significant changes.'},\n",
       " {'score': 0.037930168211460114,\n",
       "  'token': 2600,\n",
       "  'token_str': 'identify',\n",
       "  'sequence': 'to modify means we should identify significant changes.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"nlpaueb/legal-bert-base-uncased\")\n",
    "pipe(\"To modify means we should [MASK] significant changes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b1b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
