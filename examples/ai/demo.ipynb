{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8f6dd5",
   "metadata": {},
   "source": [
    "### TESTING NEW FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f85eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running getout_of_text3 version: 0.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import getout_of_text_3 as got3\n",
    "from getout_of_text_3 import ScotusAnalysisTool, ScotusFilteredAnalysisTool\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# print version of getout_of_text_3\n",
    "print('Running getout_of_text3 version:', got3.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cef1eab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown report written: ordinary_meaning_reasoning_first.md (length=3073 chars)\n"
     ]
    }
   ],
   "source": [
    "def export_markdown_reasoning_first(result, keyword: str, filename: str | None = None):\n",
    "    \"\"\"Export a markdown report with REASONING CONTENT first, then the rest.\n",
    "\n",
    "    Ordering Rules:\n",
    "    1. # reasoning content  (aggregated reasoning_content list or string)\n",
    "    2. Remaining known sections in this order if present: summary, occurrences_summary, limitations, total_contexts.\n",
    "    3. Any other string fields in the dict are appended at the end under a generic heading.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result : dict | str | list\n",
    "        Output from the SCOTUS analysis tool (JSON mode recommended for richer structure).\n",
    "    keyword : str\n",
    "        Used to build filename (sanitized) if filename not provided.\n",
    "    filename : str | None\n",
    "        Optional explicit filename. If None, auto-generated.\n",
    "    \"\"\"\n",
    "    import json, re, os\n",
    "\n",
    "    def _sanitize(name: str) -> str:\n",
    "        cleaned = ''.join(c if (c.isalnum() or c in ('-','_')) else '_' for c in name.strip())\n",
    "        return cleaned or 'analysis'\n",
    "\n",
    "    reasoning_block = ''\n",
    "    body_sections = []\n",
    "\n",
    "    if isinstance(result, dict):\n",
    "        rc = result.get('reasoning_content')\n",
    "        if isinstance(rc, list):\n",
    "            reasoning_block = '\\n'.join(str(x) for x in rc if str(x).strip())\n",
    "        elif isinstance(rc, str):\n",
    "            reasoning_block = rc.strip()\n",
    "        else:\n",
    "            # try nested style\n",
    "            if isinstance(rc, dict) and 'text' in rc:\n",
    "                reasoning_block = str(rc['text']).strip()\n",
    "        # Collect ordered sections\n",
    "        ordered_keys = ['summary', 'occurrences_summary', 'limitations', 'total_contexts']\n",
    "        used = set(['reasoning_content'])\n",
    "        for k in ordered_keys:\n",
    "            if k in result and k not in used:\n",
    "                val = result[k]\n",
    "                if isinstance(val, (str, int, float)):\n",
    "                    body_sections.append(f\"## {k}\\n\\n{val}\\n\")\n",
    "                else:\n",
    "                    body_sections.append(f\"## {k}\\n\\n{json.dumps(val, indent=2)}\\n\")\n",
    "                used.add(k)\n",
    "        # Append any other simple string fields not yet used\n",
    "        for k, v in result.items():\n",
    "            if k in used:\n",
    "                continue\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                body_sections.append(f\"## {k}\\n\\n{v.strip()}\\n\")\n",
    "            elif isinstance(v, (int, float)):\n",
    "                body_sections.append(f\"## {k}\\n\\n{v}\\n\")\n",
    "    elif isinstance(result, list):\n",
    "        # Try to extract reasoning blocks and remainder\n",
    "        reasoning_parts = []\n",
    "        other_parts = []\n",
    "        for block in result:\n",
    "            if isinstance(block, dict):\n",
    "                rc = block.get('reasoning_content')\n",
    "                if isinstance(rc, list):\n",
    "                    reasoning_parts.extend(str(x) for x in rc if str(x).strip())\n",
    "                elif isinstance(rc, str) and rc.strip():\n",
    "                    reasoning_parts.append(rc.strip())\n",
    "                if 'text' in block and isinstance(block['text'], str):\n",
    "                    other_parts.append(block['text'])\n",
    "            elif isinstance(block, str):\n",
    "                other_parts.append(block)\n",
    "        reasoning_block = '\\n'.join(reasoning_parts)\n",
    "        if other_parts:\n",
    "            body_sections.append(\"## response\\n\\n\" + '\\n\\n'.join(other_parts))\n",
    "    else:  # plain string\n",
    "        reasoning_block = ''\n",
    "        body_sections.append(f\"## response\\n\\n{str(result)}\")\n",
    "\n",
    "    # Fallback if no reasoning extracted\n",
    "    if not reasoning_block:\n",
    "        reasoning_block = '(No explicit reasoning_content found in result)'\n",
    "\n",
    "    report = [\"# reasoning content\\n\", \"```text\\n\", reasoning_block, \"\\n```\\n\\n\"]\n",
    "    report.extend(body_sections)\n",
    "\n",
    "    safe_keyword = _sanitize(keyword)\n",
    "    outname = filename or f\"{safe_keyword}_reasoning_first.md\"\n",
    "    with open(outname, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    print(f\"Markdown report written: {outname} (length={sum(len(x) for x in report)} chars)\")\n",
    "    return outname\n",
    "\n",
    "# Example export for the filtered JSON analysis (only if 'analysis' exists)\n",
    "try:\n",
    "    if 'analysis' in globals():\n",
    "        export_markdown_reasoning_first(analysis, 'ordinary_meaning')\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "\n",
    "# Utility: export a LangChain / Bedrock style streamed block list (reasoning + text) to markdown\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def export_blocks_reasoning_first(blocks, keyword: str, filename: str | None = None):\n",
    "    \"\"\"Export a list of model blocks (each a dict with 'type' and maybe 'reasoning_content' or 'text')\n",
    "    to a markdown report where the reasoning content appears FIRST.\n",
    "\n",
    "    Expected block shapes (any others are stringified):\n",
    "      {'type': 'reasoning_content', 'reasoning_content': {'text': '...'}}\n",
    "      {'type': 'text', 'text': '...'}\n",
    "\n",
    "    Ordering:\n",
    "      1. # reasoning content (aggregate all reasoning text blocks in order)\n",
    "      2. ## response (concatenate all text blocks)\n",
    "\n",
    "    Each section fenced appropriately for readability. Returns output filepath.\n",
    "    \"\"\"\n",
    "    def _sanitize(name: str) -> str:\n",
    "        return ''.join(c if (c.isalnum() or c in ('-','_')) else '_' for c in name.strip()) or 'analysis'\n",
    "\n",
    "    reasoning_parts = []\n",
    "    text_parts = []\n",
    "    for b in blocks:\n",
    "        if not isinstance(b, dict):\n",
    "            text_parts.append(str(b))\n",
    "            continue\n",
    "        b_type = b.get('type')\n",
    "        if b_type == 'reasoning_content':\n",
    "            rc = b.get('reasoning_content')\n",
    "            if isinstance(rc, dict) and 'text' in rc and rc['text']:\n",
    "                reasoning_parts.append(str(rc['text']))\n",
    "            elif isinstance(rc, str):\n",
    "                reasoning_parts.append(rc)\n",
    "        elif b_type == 'text':\n",
    "            t = b.get('text')\n",
    "            if isinstance(t, str):\n",
    "                text_parts.append(t)\n",
    "        else:  # fallback\n",
    "            # include unknown block types in response section for transparency\n",
    "            text_parts.append(str(b))\n",
    "\n",
    "    reasoning_block = '\\n\\n'.join(p.strip() for p in reasoning_parts if p and p.strip())\n",
    "    if not reasoning_block:\n",
    "        reasoning_block = '(No reasoning_content blocks found)'\n",
    "    response_block = '\\n\\n'.join(p.strip() for p in text_parts if p and p.strip()) or '(No text blocks found)'\n",
    "\n",
    "    safe_keyword = _sanitize(keyword)\n",
    "    outname = filename or f\"{safe_keyword}_reasoning_first_stream.md\"\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"# reasoning content\\n\")\n",
    "    lines.append(\"```text\\n\")\n",
    "    lines.append(reasoning_block)\n",
    "    lines.append(\"\\n```\\n\\n\")\n",
    "    lines.append(\"## response\\n\\n\")\n",
    "    lines.append(response_block)\n",
    "    lines.append(\"\\n\\n---\\n\")\n",
    "    lines.append(f\"_generated: {datetime.utcnow().isoformat()}Z | keyword: {keyword}_\")\n",
    "\n",
    "    with open(outname, 'w', encoding='utf-8') as f:\n",
    "        f.write(''.join(lines))\n",
    "    print(f\"Markdown (reasoning first) written: {outname} | length={sum(len(x) for x in lines)} chars\")\n",
    "    return outname\n",
    "\n",
    "def normalize_and_export_filtered_analysis(filtered_result: dict, keyword: str, filename: str | None = None):\n",
    "    \"\"\"\n",
    "    Specialized function to handle ScotusFilteredAnalysisTool output that often contains\n",
    "    embedded JSON within string fields (especially 'summary').\n",
    "    \n",
    "    This function:\n",
    "    1. Detects and extracts embedded JSON from string fields\n",
    "    2. Merges the extracted structured data with top-level fields\n",
    "    3. Exports to markdown with reasoning content first\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filtered_result : dict\n",
    "        Raw output from ScotusFilteredAnalysisTool._run()\n",
    "    keyword : str\n",
    "        Keyword used for filename generation\n",
    "    filename : str | None\n",
    "        Optional explicit filename\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to the generated markdown file\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def _find_embedded_json(text: str) -> str | None:\n",
    "        \"\"\"Find JSON object in text using multiple patterns.\"\"\"\n",
    "        # Look for various JSON starting patterns\n",
    "        patterns = [\n",
    "            r'\\{\\s*\"keyword\":\\s*\"[^\"]*\"',  # {\"keyword\": \"...\"}\n",
    "            r'\\{\\n\\s*\"keyword\":\\s*\"[^\"]*\"',  # multiline variant\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                # Find matching closing brace\n",
    "                depth = 0\n",
    "                in_string = False\n",
    "                escape = False\n",
    "                \n",
    "                for i, char in enumerate(text[start_pos:], start=start_pos):\n",
    "                    if in_string:\n",
    "                        if escape:\n",
    "                            escape = False\n",
    "                        elif char == '\\\\':\n",
    "                            escape = True\n",
    "                        elif char == '\"':\n",
    "                            in_string = False\n",
    "                    else:\n",
    "                        if char == '\"':\n",
    "                            in_string = True\n",
    "                        elif char == '{':\n",
    "                            depth += 1\n",
    "                        elif char == '}':\n",
    "                            depth -= 1\n",
    "                            if depth == 0:\n",
    "                                return text[start_pos:i+1]\n",
    "        return None\n",
    "    \n",
    "    def _sanitize_filename(name: str) -> str:\n",
    "        \"\"\"Create safe filename from keyword.\"\"\"\n",
    "        return ''.join(c if (c.isalnum() or c in ('-','_')) else '_' for c in name.strip()) or 'analysis'\n",
    "    \n",
    "    # Step 1: Create working copy\n",
    "    normalized = dict(filtered_result)\n",
    "    extraction_notes = []\n",
    "    \n",
    "    # Step 2: Process each string field for embedded JSON\n",
    "    for field_name, field_value in list(filtered_result.items()):\n",
    "        if not isinstance(field_value, str) or '\"keyword\"' not in field_value:\n",
    "            continue\n",
    "            \n",
    "        # Try to extract JSON\n",
    "        json_text = _find_embedded_json(field_value)\n",
    "        if not json_text:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            extracted_data = json.loads(json_text)\n",
    "            extraction_notes.append(f\"Extracted JSON from '{field_name}'\")\n",
    "            \n",
    "            # Merge extracted data into normalized result\n",
    "            for key, value in extracted_data.items():\n",
    "                if key == 'reasoning_content':\n",
    "                    # Special handling for reasoning_content - merge lists\n",
    "                    existing = normalized.get('reasoning_content', [])\n",
    "                    if isinstance(existing, str):\n",
    "                        existing = [existing] if existing.strip() else []\n",
    "                    elif not isinstance(existing, list):\n",
    "                        existing = []\n",
    "                    \n",
    "                    new_items = []\n",
    "                    if isinstance(value, list):\n",
    "                        new_items = [str(item).strip() for item in value if str(item).strip()]\n",
    "                    elif isinstance(value, str) and value.strip():\n",
    "                        new_items = [value.strip()]\n",
    "                    \n",
    "                    # Combine and deduplicate while preserving order\n",
    "                    combined = existing + new_items\n",
    "                    seen = set()\n",
    "                    deduplicated = []\n",
    "                    for item in combined:\n",
    "                        if item not in seen:\n",
    "                            seen.add(item)\n",
    "                            deduplicated.append(item)\n",
    "                    \n",
    "                    normalized['reasoning_content'] = deduplicated\n",
    "                else:\n",
    "                    # For other fields, use extracted value\n",
    "                    normalized[key] = value\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            extraction_notes.append(f\"Failed to parse JSON in '{field_name}': {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    # Step 3: Add processing metadata\n",
    "    if extraction_notes:\n",
    "        normalized['_processing_notes'] = '; '.join(extraction_notes)\n",
    "        normalized['_processed_at'] = datetime.utcnow().isoformat() + 'Z'\n",
    "    \n",
    "    # Step 4: Build markdown content\n",
    "    def build_markdown_sections():\n",
    "        sections = []\n",
    "        \n",
    "        # Reasoning content first\n",
    "        reasoning = normalized.get('reasoning_content', [])\n",
    "        if isinstance(reasoning, list):\n",
    "            reasoning_text = '\\n'.join(str(item) for item in reasoning if str(item).strip())\n",
    "        elif isinstance(reasoning, str):\n",
    "            reasoning_text = reasoning.strip()\n",
    "        else:\n",
    "            reasoning_text = '(No reasoning content found)'\n",
    "        \n",
    "        sections.append(\"# reasoning content\\n\")\n",
    "        sections.append(\"```text\\n\")\n",
    "        sections.append(reasoning_text)\n",
    "        sections.append(\"\\n```\\n\\n\")\n",
    "        \n",
    "        # Structured sections in priority order\n",
    "        priority_fields = ['summary', 'occurrences_summary', 'limitations', 'total_contexts']\n",
    "        processed_fields = {'reasoning_content', '_processing_notes', '_processed_at'}\n",
    "        \n",
    "        for field in priority_fields:\n",
    "            if field in normalized and field not in processed_fields:\n",
    "                value = normalized[field]\n",
    "                sections.append(f\"## {field}\\n\\n\")\n",
    "                if isinstance(value, (str, int, float)):\n",
    "                    sections.append(f\"{value}\\n\\n\")\n",
    "                else:\n",
    "                    sections.append(f\"{json.dumps(value, indent=2)}\\n\\n\")\n",
    "                processed_fields.add(field)\n",
    "        \n",
    "        # Any remaining fields\n",
    "        for field, value in normalized.items():\n",
    "            if field in processed_fields:\n",
    "                continue\n",
    "            if isinstance(value, str) and value.strip():\n",
    "                sections.append(f\"## {field}\\n\\n{value.strip()}\\n\\n\")\n",
    "            elif isinstance(value, (int, float)):\n",
    "                sections.append(f\"## {field}\\n\\n{value}\\n\\n\")\n",
    "        \n",
    "        # Processing metadata footer\n",
    "        if extraction_notes:\n",
    "            sections.append(\"---\\n\\n\")\n",
    "            sections.append(\"### Processing Notes\\n\\n\")\n",
    "            for note in extraction_notes:\n",
    "                sections.append(f\"- {note}\\n\")\n",
    "            sections.append(f\"\\n_Generated: {datetime.utcnow().isoformat()}Z_\\n\")\n",
    "        \n",
    "        return ''.join(sections)\n",
    "    \n",
    "    # Step 5: Write markdown file\n",
    "    safe_keyword = _sanitize_filename(keyword)\n",
    "    output_filename = filename or f\"{safe_keyword}_filtered_normalized.md\"\n",
    "    \n",
    "    markdown_content = build_markdown_sections()\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    \n",
    "    # Step 6: Report results\n",
    "    chars_written = len(markdown_content)\n",
    "    extracted_count = len(extraction_notes)\n",
    "    \n",
    "    print(f\"âœ… Filtered analysis exported: {output_filename}\")\n",
    "    print(f\"   ğŸ“„ Content: {chars_written} characters\")\n",
    "    print(f\"   ğŸ”§ Extractions: {extracted_count} JSON blocks processed\")\n",
    "    if extracted_count > 0:\n",
    "        print(f\"   ğŸ“‹ Notes: {'; '.join(extraction_notes)}\")\n",
    "    \n",
    "    return output_filename\n",
    "\n",
    "# Example using the provided structure (assign to variable `blocks_example` before calling if not already)\n",
    "try:\n",
    "    if 'blocks_example' in globals():\n",
    "        export_blocks_reasoning_first(blocks_example, 'textualism')\n",
    "except Exception as e:\n",
    "    print('Export failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a8594",
   "metadata": {},
   "source": [
    "### Step 1. Read SCOTUS DIY Corpus\n",
    "\n",
    "- saved as `loc_gov.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedf4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pdf scotus files\n",
    "df = pd.read_json(\"../loc_gov.json\", lines=True)\n",
    "\n",
    "df['key'] = df['filename'].apply(lambda x: x.split('usrep')[1][:3])\n",
    "df['subkey'] = df['filename'].apply(lambda x: x.split('usrep')[1].split('.pdf')[0])\n",
    "\n",
    "# Create a dictionary to hold the DataFrame contents\n",
    "df_dict = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if row['key'] not in df_dict:\n",
    "        df_dict[row['key']] = {}\n",
    "    df_dict[row['key']][row['subkey']] = row['content']\n",
    "\n",
    "# format scotus data for getout_of_text_3, similar to COCA keyword results\n",
    "db_dict_formatted = {}\n",
    "for volume, cases in df_dict.items():\n",
    "    # Create a DataFrame for each volume with case text\n",
    "    case_data = []\n",
    "    for case_id, case_text in cases.items():\n",
    "        case_data.append({'case_id': case_id, 'text': case_text})\n",
    "    db_dict_formatted[volume] = pd.DataFrame(case_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a9cf8",
   "metadata": {},
   "source": [
    "### Step 2. Load Langchain AWS Bedrock Model\n",
    "\n",
    "Here we are using AWS Bedrock model `openai.gpt-oss-120b-1:0` as the max token count of 128,000 allows for a large context window at a cost-effective price structure. Notably as well we always aim for open models to promote transparent and responsible AI. For more info, see:\n",
    "- https://openai.com/open-models/\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-openai.html\n",
    "\n",
    "#### Tools\n",
    "\n",
    "- `search_tool` takes a keyword and analysis type, which allows for a quick summary of a keyword across SCOTUS\n",
    "- `filtered_tool` takes a filtered corpus dict, allowing for the researcher to control exactly what is passed to AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc3637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    \"openai.gpt-oss-120b-1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    "    credentials_profile_name=\"atn-developer\",\n",
    "    max_tokens=128000\n",
    ")\n",
    "# Assume you built db_dict_formatted (volume -> DataFrame with columns ['case_id','text'])\n",
    "search_tool = ScotusAnalysisTool(model=model, db_dict_formatted=db_dict_formatted)\n",
    "filtered_tool = ScotusFilteredAnalysisTool(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ffa27",
   "metadata": {},
   "source": [
    "### Step 3. Using the `search_tool` for a keyword of interest, namely `bank`\n",
    "\n",
    "- this is a classic NLP keyword to benchmark results (i.e. financial bank vs riverbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb1c5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword=\"Chevron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3116d5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'reasoning_content',\n",
       "  'reasoning_content': {'text': 'We need to produce insights on temporal evolution, contextual variation, notable intra-dataset patterns, interpretive themes relevant to ordinary meaning. Use only provided data. No external knowledge. Summarize occurrences by volume/year perhaps? The data includes references to cases spanning 1960s to 2000s. Need to analyze.\\n\\nWe have many entries with \"Chevron Oil Co.\", \"Chevron U.S.A. Inc.\", \"Chevron Corp.\", etc. Patterns: early references (1970s) about Chevron Oil Co. v. Huson (1971) and retroactivity; later references (1980s-1990s) about Chevron U.S.A. Inc. v. NRDC (1984); later 2000s about Chevron deference and agency interpretation.\\n\\nTemporal evolution: early 1960s-70s references relate to Chevron Oil Co. (oil industry) and retroactive application; mid-1980s adoption of Chevron doctrine for agency deference; 1990s-2000s proliferation across diverse statutory contexts (EPA, EEOC, labor, tax, securities, etc.) showing expansion.\\n\\nContextual variation: used in property disputes, retroactivity analysis, agency deference, statutory interpretation, preemption, etc.\\n\\nIntra-dataset patterns: many citations within same volume of U.S. Reports: e.g., 404 U.S. 97 (Huson) appears repeatedly; 467 U.S. 837 (NRDC) appears repeatedly; citations often paired with other cases to illustrate principles; sometimes \"Chevron\" appears in lower court opinion excerpts highlighting deference; also appears in amicus briefs.\\n\\nInterpretive themes: ordinary meaning focus on \"Chevron\" as shorthand for legal doctrine; the text shows that \"Chevron\" is invoked to invoke reasonableness of agency interpretation; also used to signal retroactive application tests; reflects ordinary meaning of \"Chevron\" as corporate entity but also as doctrinal shorthand.\\n\\nNow craft answer.',\n",
       "   'signature': ''}},\n",
       " {'type': 'text',\n",
       "  'text': '**Forensic Linguistic Brief â€“ â€œChevronâ€ in the SCOTUSâ€search corpus**  \\n*(256 citations drawn from 353\\u202fâ€“\\u202f570,\\u202fU.S. Reports; no external material consulted)*  \\n\\n---\\n\\n## 1. Temporal evolution â€“ how the â€œChevronâ€ signifier changes over time  \\n\\n| Approx. period (vol.\\u202f&\\u202fyear) | Dominant â€œChevronâ€ formulation | Main legal problem attached to the signifier |\\n|------------------------------|--------------------------------|---------------------------------------------|\\n| **1965â€‘1973** (vols.\\u202f353â€‘417) | **Chevron\\u202fOil\\u202fCo.** (e.g., *Chevron Oil Co. v. Huson*, 404\\u202fU.S.\\u202f97\\u202f(1971)) |\\u202fRetroactivity & â€œfirstâ€‘impressionâ€ analysis â€“ the Court asks whether a new rule must apply prospectively. Citations focus on â€œthe decision to be applied nonâ€‘retroactively must establish a new principle of lawâ€ (see 404\\u202fU.S.\\u202f97\\u202fÂ¶\\u202f106â€‘107). |\\n| **1978â€‘1985** (vols.\\u202f440â€‘470) | **Chevron\\u202fU.S.A.\\u202fInc.** (e.g., *Chevron\\u202fU.S.A.\\u202fInc. v. Nat. Res. Def.\\u202fCounc.*, 467\\u202fU.S.\\u202f837\\u202f(1984)) |\\u202fBirth of the â€œChevron deferenceâ€ doctrine. The corpus repeatedly pairs the case with language such as â€œreasonable interpretation of the statuteâ€ or â€œagencyâ€™s construction must be permissibleâ€. The shift is from a *retroactivity* focus to a *deference* focus. |\\n| **1986â€‘1995** (vols.\\u202f471â€‘500) | Mixed â€“ **Chevron\\u202fOil\\u202fCo.** (retroâ€‘act) + **Chevron\\u202fU.S.A.\\u202fInc.** (deference) + **Chevron\\u202fCorp.** (as amicus) |\\u202fThe signifier becomes a â€œtemplateâ€ for a wide range of agencyâ€‘interpretation discussions: environmental statutes, labor regulations, tax provisions, ERISA, EEOC rules, etc. The same judicial language (â€œreasonable constructionâ€, â€œunambiguous intent of Congressâ€) is reâ€‘used in very different substantive contexts. |\\n| **1996â€‘2005** (vols.\\u202f501â€‘570) | **Chevron** (lowerâ€‘case, doctrinal shorthand) appears in footnotes, dissents, and amicus briefs. |\\u202fThe term is now a *conceptual shorthand* rather than a party name. It is invoked to justify or reject deference, to signal â€œthe Chevron stepâ€‘two testâ€, or to argue that a rule â€œdoes not rise to Chevron statusâ€. The word surfaces in nonâ€‘U.S.â€‘Supremeâ€‘Court materials (e.g., amicus briefs, districtâ€‘court opinions) that cite the Supreme Court precedent. |\\n\\n**Key trajectory** â€“ *Chevron* starts as a corporate litigant (oilâ€‘company) in the earlyâ€‘70s, becomes the name of a landmark deference holding in the midâ€‘80s, and finally matures into a generic doctrinal label used across the entire administrativeâ€‘law landscape by the lateâ€‘90s and earlyâ€‘00s.\\n\\n---\\n\\n## 2. Contextual variation â€“ where â€œChevronâ€ lives in the text  \\n\\n| Contextual cluster (as visible in the snippets) | Representative quotation | How â€œChevronâ€ functions in the clause |\\n|--------------------------------------------------|--------------------------|--------------------------------------|\\n| **Retroactivity / firstâ€‘impression** | â€œâ€¦the decision to be applied nonâ€‘retroactively must establish a new principle of law.â€ (404\\u202fU.S.\\u202f97\\u202fÂ¶\\u202f106) | *Chevron* is the *source* of the retroactivity test; the case is cited to *limit* the reach of new rules. |\\n| **Agency deference (stepâ€‘two reasonableness)** | â€œâ€¦if the agencyâ€™s construction is a reasonable interpretation of the statute, see **Chevron**\\u202fU.S.A.\\u202fInc.\\u202fv.\\u202fNRDCâ€¦â€ (multiple volumes) | *Chevron* supplies the *standard* (reasonable, not â€œarbitrary, capriciousâ€) on which courts judge agency interpretations. |\\n| **Preemption & â€œexpresslyâ€‘unambiguousâ€ language** | â€œâ€¦the statute is ambiguous, see **Chevron**\\u202fU.S.A.\\u202fInc.\\u202fv.\\u202fNRDC, 467\\u202fU.S.\\u202f837\\u202fÂ¶\\u202f843â€‘844.â€ | *Chevron* is invoked to *justify* that agency reading can fill gaps where Congress is silent. |\\n| **Amicus briefs & party identification** | â€œBrief for **Chevron**\\u202fCorporation as Amicus Curiae.â€ (vol.\\u202f504) | The word is used as a *party label* in procedural filings, showing that the corporation still appears in litigation unrelated to the doctrinal holding. |\\n| **Dissenting or limiting language** | â€œThe majority must hide behind **Chevronâ€™s** vocabulary, despite its explicit holding that **Chevron** does not apply.â€ (vol.\\u202f540) | *Chevron* becomes a *target* of criticism, a shorthand for the doctrinal approach the dissent seeks to avoid. |\\n| **Procedural â€œreview of judgments, not opinionsâ€** | â€œThe Court reviews judgments, not opinions. **Chevron**\\u202fU.S.A.\\u202fInc.\\u202fv.\\u202fNRDCâ€¦â€ (vol.\\u202f500) | The case is referenced to **clarify** the scope of appellate review, not the substance of the agency decision. |\\n\\n**Pattern:** the signifier moves fluidly between *substantive* (the doctrine itself) and *procedural* (the parties, the record) roles, always anchored to a single legal proposition: *when a statute is ambiguous, an agencyâ€™s reasonable construction may be given deference*.\\n\\n---\\n\\n## 3. Notable intraâ€‘dataset patterns (internal regularities)\\n\\n1. **Citation clustering around two anchor cases**  \\n   - *Chevron Oil Co. v. Huson* (404\\u202fU.S.\\u202f97) appears in **34** separate entries (vols.\\u202f404â€‘517).  \\n   - *Chevron U.S.A. Inc. v. NRDC* (467\\u202fU.S.\\u202f837) appears in **61** entries (vols.\\u202f458â€‘570).  \\n   - No other case in the set reaches more than **7** citations, indicating a *binary focal point* around which the entire corpus organizes.\\n\\n2. **Repeating â€œtwoâ€‘stepâ€ language**  \\n   - Phrases such as â€œthe first question is whether Congress has directly spoken to the precise question at issueâ€ and â€œthe second step is whether the agencyâ€™s construction is reasonableâ€ recur verbatim in **â‰¥\\u202f18** excerpts.  \\n   - The exact wording, including the comma placement and the phrase â€œreasonable constructionâ€, is *identical* across citations, showing that the search engine captured the *standardized judicial template*.\\n\\n3. **Coâ€‘citation with other landmark cases**  \\n   - Each *Chevron* citation is frequently paired with another precedent (e.g., *Linkletter v. Walker*, *Brown v. Louisiana*, *Mead v. United States*). This pattern points to *bundling* of doctrinal pillars: retroactivity (Linkletter), deference (Mead), or the â€œclearâ€‘statementâ€ rule (Brown).  \\n\\n4. **Shift from uppercase â€œCHEVRONâ€ to title case**  \\n   - Early entries (vols.\\u202f404â€‘417) show **CHEVRON** in all caps (court headings). Later entries adopt *Chevron* (title case) or even lowerâ€‘case â€œchevronâ€ in dissent footnotes. The orthographic change aligns with the evolution from *party name* (capitalized for emphasis) to *doctrinal term* (treated like any other legal concept).\\n\\n5. **Geographic / industry spread**  \\n   - The term appears in contexts involving **oil & gas** (Chevron Oil Co., â€œChevron\\u202fU.S.A.\\u202fInc.â€), **environmental regulation** (EPA, Clean Air Act), **labor** (NLRB, ERISA), **tax** (IRS), **communications** (FCC), **intellectual property** (patent briefs), and **international arbitration** (Chevron Corp. in Nigeria). The only constant is the *reference to the same two Supreme Court holdings*; the surrounding substantive field is highly variable.\\n\\n---\\n\\n## 4. Interpretive themes relevant to â€œordinary meaningâ€\\n\\n1. **â€œChevronâ€ as a *generic doctrinal cue***  \\n   - In the ordinary reading of the excerpts, a lawyer encountering â€œChevronâ€ instantly knows the discussion will touch on **agency deference** or **retroactivity**. The word functions like a *keyword* that triggers a specific analytical framework, independent of the underlying factual dispute.\\n\\n2. **Semantic shift from *entity* to *principle***  \\n   - Early citations treat â€œChevronâ€ as a *corporate actor* (e.g., â€œChevron Oil Co. v. Husonâ€). By the midâ€‘80s the ordinary meaning is *the twoâ€‘step test* itself. The transition is captured entirely within the corpus: the same orthographic token carries two distinct semantic loads at different times.\\n\\n3. **Reliance on *plainâ€‘language* construction**  \\n   - The majority of snippets embed the phrase â€œthe plain meaning of the statuteâ€ directly after a *Chevron* citation, underscoring that the doctrine is **rooted in an ordinaryâ€‘meaning approach** to statutory text. The Court repeatedly tells the reader that â€œif Congress has spoken directly, that is the end of the matterâ€ â€“ a literal, ordinaryâ€‘meaning principle.\\n\\n4. **Presumption of *reasonableness* as the ordinary yardstick**  \\n   - The term â€œreasonableâ€ recurs verbatim after *Chevron* references. In ordinary discourse this signals that the agencyâ€™s interpretation must be **reasonable**, not merely *correct* or *perfect*. The ordinary reader grasps that the threshold is deliberately low, reflecting a pragmatic, everyday standard of rationality.\\n\\n5. **Deference as *ordinary institutional respect***  \\n   - Several dissenting passages frame *Chevron* as an â€œoverâ€‘reachâ€ of the Courtâ€™s ordinary respect for agency expertise. The ordinary meaning of â€œdeferenceâ€ is therefore tightly bound to *Chevron*â€”the doctrine is routinely portrayed as the judicial expression of ordinary institutional trust in specialized agencies.\\n\\n---\\n\\n### Summary\\n\\n- **Temporal trajectory:** From a corporate litigant (early 1970s) â†’ a landmark deference holding (midâ€‘1980s) â†’ a generic doctrinal label used across the administrativeâ€‘law spectrum (1990sâ€‘2000s).  \\n- **Contextual spread:** The signifier appears in retroactivity tests, agencyâ€‘interpretation deference, preemption analyses, procedural citations, and amicusâ€‘brief party listings.  \\n- **Internal regularities:** Two anchor cases dominate; a fixed â€œtwoâ€‘stepâ€ phrasing recurs; coâ€‘citation with other doctrinal landmarks is common; orthographic shifts mirror semantic shift.  \\n- **Ordinaryâ€‘meaning lens:** For any reader of these excerpts, â€œChevronâ€ instantly signals an analysis grounded in plainâ€‘text interpretation, reasonableness, and institutional deferenceâ€”its everyday lexical load is the *doctrine* rather than the *company*.\\n\\nThese insights, extracted solely from the supplied search results, map the vivid lifeâ€‘cycle of the â€œChevronâ€ token within the Supreme Courtâ€™s published opinions and related legal filings.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_result = search_tool._run(keyword=keyword, \n",
    "                               analysis_focus=\"general\")\n",
    "text_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d8b4fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown (reasoning first) written: Chevron_reasoning_first_stream.md | length=11676 chars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chevron_reasoning_first_stream.md'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_blocks_reasoning_first(text_result, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013fce5b",
   "metadata": {},
   "source": [
    "### Step 4. Using the `filtered_tool` for a keyword of interest, namely `dictionary`\n",
    "\n",
    "- to see how SCOTUS references dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3fcc28b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] raw_chars=192574 extracted_chars=186980 reduction_ratio=0.971 rawâ‰ˆ48144tok extractedâ‰ˆ46745tok strategy=all limit=None\n"
     ]
    }
   ],
   "source": [
    "# After you have filtered JSON:\n",
    "filtered_json = got3.search_keyword_corpus(keyword, \n",
    "                                           db_dict_formatted, \n",
    "                                           output=\"json\",\n",
    "                                           context_words=60,\n",
    "                                           parallel=True)\n",
    "\n",
    "analysis = filtered_tool._run(\n",
    "    keyword=keyword,\n",
    "    results_json=filtered_json,\n",
    "    extraction_strategy=\"all\",\n",
    "    #max_contexts=60, # to filter results\n",
    "    return_json=True,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c182fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keyword': 'Chevron',\n",
       " 'total_contexts': 256,\n",
       " 'occurrences_summary': '256 context snippet(s) across 256 case(s)',\n",
       " 'reasoning_content': ['Model did not return valid JSON; wrapped raw text.',\n",
       "  'Multiple contexts allow limited comparative analysis.'],\n",
       " 'summary': '{\\'type\\': \\'reasoning_content\\', \\'reasoning_content\\': {\\'text\\': \\'We need produce JSON with fields. Use keyword \"Chevron\". total_contexts = number of contexts which is total cases =256? Actually there are 256 contexts. So total_contexts =256. occurrences_summary: from prompt: \"Occurrences Per Case (sample): 353:11=1; 390:17=1; ...\" that\\\\\\'s many but sample. We need a summary string describing frequency. Probably note that \"Chevron appears in all 256 contexts, with at least one occurrence per case.\" Use that. reasoning_content: 3-6 steps of analysis: e.g., identify contexts, extract uses, categorize, assess patterns, summarize. summary: overall overview of usage patterns: Chevron mainly appears in case titles referencing Chevron Oil Co., etc., used to illustrate Chevron deference doctrine, also as a company name in maps, etc. limitations: only provided snippets, no full opinions, cannot assess full context, limited to extracted snippets, no external info.\\\\n\\\\nReturn JSON only.\\\\n\\\\n\\', \\'signature\\': \\'\\'}}\\n{\\n  \"keyword\": \"Chevron\",\\n  \"total_contexts\": 256,\\n  \"occurrences_summary\": \"The term â€œChevronâ€ appears at least once in each of the 256 sampled cases, yielding a minimum of 256 occurrences. Sample counts show many cases with a single citation (e.g., 353:11=1, 390:17=1), indicating a broad but shallow distribution across the corpus rather than intensive clustering in a few decisions.\",\\n  \"reasoning_content\": [\\n    \"Identify every snippet containing the word â€œChevronâ€.\",\\n    \"Classify each occurrence by its function (e.g., case title, map label, statutory citation, doctrinal reference).\",\\n    \"Tabulate the frequency of each functional category across the 256 contexts.\",\\n    \"Detect recurring legal themes (e.g., Chevron deference, Chevron Oil Co. v. Huson, agencyâ€‘interpretation cases).\",\\n    \"Synthesize the patterns into an overall usage overview.\"\\n  ],\\n  \"summary\": \"Across the sampled volumes, â€œChevronâ€ is most commonly encountered in Supreme Court and appellate opinions where it serves as shorthand for Chevron Oil Co. and the landmark Chevron deference doctrine. It also appears in ancillary material such as maps, product references, and corporate mentions. The dominant semantic range is legalâ€”either as a party in litigation or as a citation to the 1984 Chevron decisionâ€”showing that the corpus heavily references the agencyâ€‘interpretation principle. Secondary uses (e.g., geographic maps, corporate equipment) are isolated and do not affect the primary legal pattern.\",\\n  \"limitations\": \"Analysis is confined to the extracted 256 context snippets; full opinions or surrounding text may contain additional nuances that are not captured. The sample provides only one occurrence per case in many instances, limiting the ability to assess intraâ€‘case variation. No external legal databases or doctrinal commentary were consulted.\"\\n}',\n",
       " 'limitations': 'Auto-wrapped due to invalid JSON from model.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d69ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Filtered analysis exported: Chevron_filtered_normalized.md\n",
      "   ğŸ“„ Content: 2007 characters\n",
      "   ğŸ”§ Extractions: 1 JSON blocks processed\n",
      "   ğŸ“‹ Notes: Extracted JSON from 'summary'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chevron_filtered_normalized.md'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export to report\n",
    "normalize_and_export_filtered_analysis(analysis, keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
